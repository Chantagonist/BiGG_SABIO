{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Firefox driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated completion of scraping the XLS data for test_Ecoli-2: 2022-01-28 19:16:24.663706, in 4.75 hours\n",
      "< Phosphofructokinase > was either already scraped, or is duplicated in the model.\n",
      "< Pyruvate formate lyase > was either already scraped, or is duplicated in the model.\n",
      "< Glucose-6-phosphate isomerase > was either already scraped, or is duplicated in the model.\n",
      "< Phosphoglycerate kinase > was either already scraped, or is duplicated in the model.\n",
      "< 6-phosphogluconolactonase > was either already scraped, or is duplicated in the model.\n",
      "< Acetaldehyde dehydrogenase (acetylating) > was either already scraped, or is duplicated in the model.\n",
      "< 2 oxoglutarate reversible transport via symport > was either already scraped, or is duplicated in the model.\n",
      "< Phosphoglycerate mutase > was either already scraped, or is duplicated in the model.\n",
      "< Phosphate reversible transport via symport > was either already scraped, or is duplicated in the model.\n",
      "< Alcohol dehydrogenase (ethanol) > was either already scraped, or is duplicated in the model.\n",
      "< Acetaldehyde reversible transport > was either already scraped, or is duplicated in the model.\n",
      "< Acetate kinase > was either already scraped, or is duplicated in the model.\n",
      "< Phosphoenolpyruvate carboxylase > was either already scraped, or is duplicated in the model.\n",
      "< Aconitase (half-reaction A, Citrate hydro-lyase) > was either already scraped, or is duplicated in the model.\n",
      "< Aconitase (half-reaction B, Isocitrate hydro-lyase) > was either already scraped, or is duplicated in the model.\n",
      "< ATP maintenance requirement > was either already scraped, or is duplicated in the model.\n",
      "< Phosphoenolpyruvate carboxykinase > was either already scraped, or is duplicated in the model.\n",
      "< Acetate reversible transport via proton symport > was either already scraped, or is duplicated in the model.\n",
      "< Phosphoenolpyruvate synthase > was either already scraped, or is duplicated in the model.\n",
      "< Adenylate kinase > was either already scraped, or is duplicated in the model.\n",
      "< 2-Oxogluterate dehydrogenase > was either already scraped, or is duplicated in the model.\n",
      "< ATP synthase (four protons for one ATP) > was either already scraped, or is duplicated in the model.\n",
      "< Phosphotransacetylase > was either already scraped, or is duplicated in the model.\n",
      "< Pyruvate kinase > was either already scraped, or is duplicated in the model.\n",
      "< Biomass Objective Function with GAM > was either already scraped, or is duplicated in the model.\n",
      "< Pyruvate transport in via proton symport > was either already scraped, or is duplicated in the model.\n",
      "< CO2 transporter via diffusion > was either already scraped, or is duplicated in the model.\n",
      "< Ribulose 5-phosphate 3-epimerase > was either already scraped, or is duplicated in the model.\n",
      "< Citrate synthase > was either already scraped, or is duplicated in the model.\n",
      "< Ribose-5-phosphate isomerase > was either already scraped, or is duplicated in the model.\n",
      "< Succinate transport via proton symport (2 H) > was either already scraped, or is duplicated in the model.\n",
      "< Cytochrome oxidase bd (ubiquinol-8: 2 protons) > was either already scraped, or is duplicated in the model.\n",
      "< D lactate transport via proton symport > was either already scraped, or is duplicated in the model.\n",
      "< Enolase > was either already scraped, or is duplicated in the model.\n",
      "< Succinate transport out via proton antiport > was either already scraped, or is duplicated in the model.\n",
      "< Ethanol reversible transport via proton symport > was either already scraped, or is duplicated in the model.\n",
      "< Succinate dehydrogenase (irreversible) > was either already scraped, or is duplicated in the model.\n",
      "< Succinyl-CoA synthetase (ADP-forming) > was either already scraped, or is duplicated in the model.\n",
      "< Transaldolase > was either already scraped, or is duplicated in the model.\n",
      "< NAD(P) transhydrogenase > was either already scraped, or is duplicated in the model.\n",
      "< Transketolase > was either already scraped, or is duplicated in the model.\n",
      "< Transketolase > was either already scraped, or is duplicated in the model.\n",
      "< Triose-phosphate isomerase > was either already scraped, or is duplicated in the model.\n",
      "< Acetate exchange > was either already scraped, or is duplicated in the model.\n",
      "< Acetaldehyde exchange > was either already scraped, or is duplicated in the model.\n",
      "< 2-Oxoglutarate exchange > was either already scraped, or is duplicated in the model.\n",
      "< CO2 exchange > was either already scraped, or is duplicated in the model.\n",
      "< Ethanol exchange > was either already scraped, or is duplicated in the model.\n",
      "< Formate exchange > was either already scraped, or is duplicated in the model.\n",
      "< D-Fructose exchange > was either already scraped, or is duplicated in the model.\n",
      "< Fumarate exchange > was either already scraped, or is duplicated in the model.\n",
      "< D-Glucose exchange > was either already scraped, or is duplicated in the model.\n",
      "< L-Glutamine exchange > was either already scraped, or is duplicated in the model.\n",
      "< L-Glutamate exchange > was either already scraped, or is duplicated in the model.\n",
      "< H+ exchange > was either already scraped, or is duplicated in the model.\n",
      "< H2O exchange > was either already scraped, or is duplicated in the model.\n",
      "< D-lactate exchange > was either already scraped, or is duplicated in the model.\n",
      "< L-Malate exchange > was either already scraped, or is duplicated in the model.\n",
      "< Ammonia exchange > was either already scraped, or is duplicated in the model.\n",
      "< O2 exchange > was either already scraped, or is duplicated in the model.\n",
      "< Phosphate exchange > was either already scraped, or is duplicated in the model.\n",
      "< Pyruvate exchange > was either already scraped, or is duplicated in the model.\n",
      "< Succinate exchange > was either already scraped, or is duplicated in the model.\n",
      "< Fructose-bisphosphate aldolase > was either already scraped, or is duplicated in the model.\n",
      "< Fructose-bisphosphatase > was either already scraped, or is duplicated in the model.\n",
      "< Formate transport in via proton symport > was either already scraped, or is duplicated in the model.\n",
      "< Formate transport via diffusion > was either already scraped, or is duplicated in the model.\n",
      "< Fumarate reductase > was either already scraped, or is duplicated in the model.\n",
      "< Fructose transport via PEP:Pyr PTS (f6p generating) > was either already scraped, or is duplicated in the model.\n",
      "< Fumarase > was either already scraped, or is duplicated in the model.\n",
      "< Fumarate transport via proton symport (2 H) > was either already scraped, or is duplicated in the model.\n",
      "< Glucose 6-phosphate dehydrogenase > was either already scraped, or is duplicated in the model.\n",
      "< Glyceraldehyde-3-phosphate dehydrogenase > was either already scraped, or is duplicated in the model.\n",
      "< D-glucose transport via PEP:Pyr PTS > was either already scraped, or is duplicated in the model.\n",
      "< Glutamine synthetase > was either already scraped, or is duplicated in the model.\n",
      "< L-glutamine transport via ABC system > was either already scraped, or is duplicated in the model.\n",
      "< Glutamate dehydrogenase (NADP) > was either already scraped, or is duplicated in the model.\n",
      "< Glutaminase > was either already scraped, or is duplicated in the model.\n",
      "< Glutamate synthase (NADPH) > was either already scraped, or is duplicated in the model.\n",
      "< L glutamate transport via proton symport  reversible > was either already scraped, or is duplicated in the model.\n",
      "< Phosphogluconate dehydrogenase > was either already scraped, or is duplicated in the model.\n",
      "< H2O transport via diffusion > was either already scraped, or is duplicated in the model.\n",
      "< Isocitrate dehydrogenase (NADP) > was either already scraped, or is duplicated in the model.\n",
      "< Isocitrate lyase > was either already scraped, or is duplicated in the model.\n",
      "< D-lactate dehydrogenase > was either already scraped, or is duplicated in the model.\n",
      "< Malate synthase > was either already scraped, or is duplicated in the model.\n",
      "< Malate transport via proton symport (2 H) > was either already scraped, or is duplicated in the model.\n",
      "< Malate dehydrogenase > was either already scraped, or is duplicated in the model.\n",
      "< Malic enzyme (NAD) > was either already scraped, or is duplicated in the model.\n",
      "< Malic enzyme (NADP) > was either already scraped, or is duplicated in the model.\n",
      "< NADH dehydrogenase (ubiquinone-8 & 3 protons) > was either already scraped, or is duplicated in the model.\n",
      "< NAD transhydrogenase > was either already scraped, or is duplicated in the model.\n",
      "< Ammonia reversible transport > was either already scraped, or is duplicated in the model.\n",
      "< O2 transport  diffusion  > was either already scraped, or is duplicated in the model.\n",
      "< Pyruvate dehydrogenase > was either already scraped, or is duplicated in the model.\n",
      "SABIO XLS data has been scraped.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The XLS data has been concatenated.\n",
      "Estimated completion of scraping the XLS data for test_Ecoli-2: 2022-01-31 00:55:27.766591, in 58.4 hours\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3326: DtypeWarning: Columns (27) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped entryID: 12525\n",
      "Scraped entryID: 12526\n",
      "Scraped entryID: 15281\n",
      "Scraped entryID: 15282\n",
      "Scraped entryID: 15283\n",
      "Scraped entryID: 15291\n",
      "Scraped entryID: 15292\n",
      "Scraped entryID: 15293\n",
      "Scraped entryID: 16042\n",
      "Scraped entryID: 16043\n",
      "24528 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 24528\n",
      "Scraped entryID: 28149\n",
      "4749 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 4749\n",
      "4750 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 4750\n",
      "Scraped entryID: 30252\n",
      "Scraped entryID: 30253\n",
      "Scraped entryID: 30254\n",
      "Scraped entryID: 30255\n",
      "Scraped entryID: 30256\n",
      "Scraped entryID: 30257\n",
      "Scraped entryID: 69377\n",
      "Scraped entryID: 69378\n",
      "Scraped entryID: 69379\n",
      "Scraped entryID: 69380\n",
      "12752 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 12752\n",
      "12753 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 12753\n",
      "Scraped entryID: 12754\n",
      "Scraped entryID: 12755\n",
      "Scraped entryID: 24017\n",
      "Scraped entryID: 24018\n",
      "Scraped entryID: 24019\n",
      "Scraped entryID: 24020\n",
      "Scraped entryID: 24021\n",
      "Scraped entryID: 24022\n",
      "Scraped entryID: 24023\n",
      "Scraped entryID: 24024\n",
      "Scraped entryID: 24025\n",
      "Scraped entryID: 24026\n",
      "Scraped entryID: 24027\n",
      "Scraped entryID: 24028\n",
      "Scraped entryID: 24029\n",
      "Scraped entryID: 24030\n",
      "Scraped entryID: 24031\n",
      "Scraped entryID: 24032\n",
      "Scraped entryID: 27649\n",
      "Scraped entryID: 27984\n",
      "30131 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 30131\n",
      "30132 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 30132\n",
      "30133 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 30133\n",
      "Scraped entryID: 30200\n",
      "Scraped entryID: 30201\n",
      "Scraped entryID: 30202\n",
      "Scraped entryID: 30203\n",
      "Scraped entryID: 31500\n",
      "Scraped entryID: 31501\n",
      "Scraped entryID: 31502\n",
      "Scraped entryID: 31503\n",
      "Scraped entryID: 31561\n",
      "Scraped entryID: 31562\n",
      "Scraped entryID: 31563\n",
      "Scraped entryID: 31564\n",
      "Scraped entryID: 31565\n",
      "Scraped entryID: 31566\n",
      "Scraped entryID: 31567\n",
      "Scraped entryID: 31568\n",
      "Scraped entryID: 31569\n",
      "Scraped entryID: 31570\n",
      "Scraped entryID: 31571\n",
      "Scraped entryID: 31572\n",
      "Scraped entryID: 32650\n",
      "Scraped entryID: 32651\n",
      "Scraped entryID: 32652\n",
      "Scraped entryID: 32653\n",
      "Scraped entryID: 32654\n",
      "Scraped entryID: 32655\n",
      "Scraped entryID: 32656\n",
      "Scraped entryID: 32657\n",
      "Scraped entryID: 32658\n",
      "Scraped entryID: 32659\n",
      "Scraped entryID: 32660\n",
      "Scraped entryID: 32661\n",
      "Scraped entryID: 32662\n",
      "Scraped entryID: 32663\n",
      "Scraped entryID: 32664\n",
      "Scraped entryID: 34657\n",
      "Scraped entryID: 34658\n",
      "Scraped entryID: 34659\n",
      "Scraped entryID: 34660\n",
      "Scraped entryID: 34661\n",
      "Scraped entryID: 34662\n",
      "Scraped entryID: 34663\n",
      "Scraped entryID: 34664\n",
      "Scraped entryID: 54458\n",
      "Scraped entryID: 54459\n",
      "67528 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 67528\n",
      "Scraped entryID: 67529\n",
      "Scraped entryID: 39275\n",
      "Scraped entryID: 39276\n",
      "Scraped entryID: 39277\n",
      "Scraped entryID: 39278\n",
      "Scraped entryID: 39279\n",
      "Scraped entryID: 41260\n",
      "Scraped entryID: 41309\n",
      "Scraped entryID: 73853\n",
      "Scraped entryID: 73854\n",
      "53751 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 53751\n",
      "Scraped entryID: 53752\n",
      "Scraped entryID: 53753\n",
      "53758 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 53758\n",
      "Scraped entryID: 41280\n",
      "Scraped entryID: 41308\n",
      "Scraped entryID: 49358\n",
      "Scraped entryID: 2870\n",
      "Scraped entryID: 24770\n",
      "Scraped entryID: 24771\n",
      "Scraped entryID: 45328\n",
      "Scraped entryID: 49359\n",
      "Scraped entryID: 53713\n",
      "54906 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'volume',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 54906\n",
      "Scraped entryID: 56577\n",
      "Scraped entryID: 59281\n",
      "Scraped entryID: 59282\n",
      "Scraped entryID: 59283\n",
      "Scraped entryID: 59284\n",
      "Scraped entryID: 59285\n",
      "Scraped entryID: 59286\n",
      "Scraped entryID: 59287\n",
      "Scraped entryID: 59288\n",
      "Scraped entryID: 59289\n",
      "Scraped entryID: 59290\n",
      "Scraped entryID: 60402\n",
      "Scraped entryID: 29381\n",
      "Scraped entryID: 29382\n",
      "Scraped entryID: 29383\n",
      "Scraped entryID: 32390\n",
      "Scraped entryID: 32391\n",
      "Scraped entryID: 32392\n",
      "Scraped entryID: 56819\n",
      "Scraped entryID: 56820\n",
      "Scraped entryID: 56821\n",
      "Scraped entryID: 56822\n",
      "Scraped entryID: 56823\n",
      "Scraped entryID: 56824\n",
      "Scraped entryID: 56825\n",
      "Scraped entryID: 56826\n",
      "Scraped entryID: 56827\n",
      "Scraped entryID: 56828\n",
      "Scraped entryID: 56829\n",
      "Scraped entryID: 56843\n",
      "56847 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '97.24',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 56847\n",
      "Scraped entryID: 58324\n",
      "Scraped entryID: 58326\n",
      "Scraped entryID: 59479\n",
      "Scraped entryID: 59480\n",
      "Scraped entryID: 59481\n",
      "Scraped entryID: 59482\n",
      "Scraped entryID: 59483\n",
      "Scraped entryID: 59484\n",
      "Scraped entryID: 59485\n",
      "Scraped entryID: 59486\n",
      "Scraped entryID: 59487\n",
      "Scraped entryID: 59488\n",
      "Scraped entryID: 59489\n",
      "Scraped entryID: 59490\n",
      "Scraped entryID: 59491\n",
      "Scraped entryID: 49400\n",
      "49800 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 49800\n",
      "50751 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 50751\n",
      "Scraped entryID: 54197\n",
      "57917 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 57917\n",
      "Scraped entryID: 60019\n",
      "Scraped entryID: 60021\n",
      "Scraped entryID: 60022\n",
      "Scraped entryID: 60023\n",
      "Scraped entryID: 60024\n",
      "Scraped entryID: 60025\n",
      "Scraped entryID: 60026\n",
      "Scraped entryID: 60027\n",
      "Scraped entryID: 39849\n",
      "53757 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 53757\n",
      "Scraped entryID: 59685\n",
      "Scraped entryID: 59686\n",
      "Scraped entryID: 59687\n",
      "Scraped entryID: 59688\n",
      "Scraped entryID: 41268\n",
      "Scraped entryID: 41317\n",
      "53748 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 53748\n",
      "Scraped entryID: 53749\n",
      "Scraped entryID: 53750\n",
      "53759 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 53759\n",
      "Scraped entryID: 53760\n",
      "Scraped entryID: 53761\n",
      "Scraped entryID: 53762\n",
      "34636 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 34636\n",
      "Scraped entryID: 40937\n",
      "Scraped entryID: 40938\n",
      "40948 missing_unit\n",
      "{'comment': '5.3 +/- 1.8 nmol*min^(-1)*unit A600^(-1)',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 40948\n",
      "40949 missing_unit\n",
      "{'comment': '5.6 +/- 1.2 nmol*min^(-1)*unit A600^(-1)',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 40949\n",
      "40950 missing_unit\n",
      "{'comment': '2.7 +/- 1.0 nmol*min^(-1)*unit A600^(-1)',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 40950\n",
      "40951 missing_unit\n",
      "{'comment': '0.3 +/- 0.2 nmol*min^(-1)*unit A600^(-1)',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 40951\n",
      "40952 missing_unit\n",
      "{'comment': '0.5 +/- 0.3 nmol*min^(-1)*unit A600^(-1)',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 40952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40953 missing_unit\n",
      "{'comment': '1.9 +/- 0.5 nmol*min^(-1)*unit A600^(-1)',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 40953\n",
      "42715 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 42715\n",
      "42869 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 42869\n",
      "42870 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 42870\n",
      "42871 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 42871\n",
      "42872 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 42872\n",
      "27658 missing_unit\n",
      "{'comment': '46 nmol/min/ mg of cells (dry weight)',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 27658\n",
      "27659 missing_unit\n",
      "{'comment': '6.6 nmol/min/ mg of cells (dry weight)',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 27659\n",
      "27660 missing_unit\n",
      "{'comment': '6.6 nmol/min/ mg of cells (dry weight)',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 27660\n",
      "28525 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 28525\n",
      "28526 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 28526\n",
      "28527 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 28527\n",
      "28528 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 28528\n",
      "28529 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 28529\n",
      "28530 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 28530\n",
      "28531 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 28531\n",
      "Scraped entryID: 52576\n",
      "Scraped entryID: 52577\n",
      "Scraped entryID: 52578\n",
      "Scraped entryID: 52579\n",
      "Scraped entryID: 55680\n",
      "Scraped entryID: 55681\n",
      "Scraped entryID: 55682\n",
      "Scraped entryID: 55683\n",
      "Scraped entryID: 55684\n",
      "Scraped entryID: 55685\n",
      "Scraped entryID: 59681\n",
      "Scraped entryID: 59682\n",
      "39044 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 39044\n",
      "39045 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 39045\n",
      "Scraped entryID: 41266\n",
      "Scraped entryID: 41315\n",
      "41342 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 41342\n",
      "54916 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'volume',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 54916\n",
      "Scraped entryID: 73851\n",
      "Scraped entryID: 4232\n",
      "Scraped entryID: 4233\n",
      "29444 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 29444\n",
      "Scraped entryID: 29447\n",
      "Scraped entryID: 4754\n",
      "Scraped entryID: 4755\n",
      "Scraped entryID: 4756\n",
      "Scraped entryID: 4757\n",
      "Scraped entryID: 4758\n",
      "Scraped entryID: 4759\n",
      "Scraped entryID: 4760\n",
      "Scraped entryID: 4761\n",
      "Scraped entryID: 4762\n",
      "Scraped entryID: 4763\n",
      "Scraped entryID: 4764\n",
      "Scraped entryID: 4765\n",
      "Scraped entryID: 4766\n",
      "Scraped entryID: 4767\n",
      "Scraped entryID: 4768\n",
      "Scraped entryID: 4769\n",
      "Scraped entryID: 4770\n",
      "Scraped entryID: 4771\n",
      "Scraped entryID: 27939\n",
      "Scraped entryID: 27940\n",
      "Scraped entryID: 27941\n",
      "Scraped entryID: 27942\n",
      "Scraped entryID: 27943\n",
      "Scraped entryID: 27944\n",
      "Scraped entryID: 27945\n",
      "Scraped entryID: 27946\n",
      "Scraped entryID: 27947\n",
      "Scraped entryID: 27948\n",
      "Scraped entryID: 27949\n",
      "Scraped entryID: 27950\n",
      "Scraped entryID: 27951\n",
      "Scraped entryID: 32612\n",
      "Scraped entryID: 32613\n",
      "Scraped entryID: 32614\n",
      "Scraped entryID: 32615\n",
      "Scraped entryID: 32616\n",
      "Scraped entryID: 35121\n",
      "Scraped entryID: 35122\n",
      "Scraped entryID: 35123\n",
      "Scraped entryID: 35124\n",
      "Scraped entryID: 35125\n",
      "Scraped entryID: 35126\n",
      "Scraped entryID: 36028\n",
      "Scraped entryID: 36029\n",
      "Scraped entryID: 36030\n",
      "Scraped entryID: 36031\n",
      "Scraped entryID: 36032\n",
      "Scraped entryID: 36033\n",
      "Scraped entryID: 36034\n",
      "Scraped entryID: 36035\n",
      "Scraped entryID: 36036\n",
      "Scraped entryID: 36037\n",
      "Scraped entryID: 36038\n",
      "Scraped entryID: 36039\n",
      "Scraped entryID: 36040\n",
      "Scraped entryID: 36041\n",
      "Scraped entryID: 36042\n",
      "Scraped entryID: 36043\n",
      "Scraped entryID: 36044\n",
      "Scraped entryID: 36045\n",
      "Scraped entryID: 54446\n",
      "Scraped entryID: 54447\n",
      "Scraped entryID: 54448\n",
      "Scraped entryID: 54449\n",
      "56728 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '0.28',\n",
      " 'type': 'Keq',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 56728\n",
      "Scraped entryID: 59108\n",
      "Scraped entryID: 59109\n",
      "Scraped entryID: 59110\n",
      "Scraped entryID: 59111\n",
      "Scraped entryID: 60196\n",
      "Scraped entryID: 60197\n",
      "Scraped entryID: 60198\n",
      "Scraped entryID: 60199\n",
      "Scraped entryID: 60200\n",
      "Scraped entryID: 60201\n",
      "Scraped entryID: 63698\n",
      "Scraped entryID: 63699\n",
      "Scraped entryID: 63700\n",
      "Scraped entryID: 63701\n",
      "Scraped entryID: 63702\n",
      "Scraped entryID: 63703\n",
      "Scraped entryID: 63704\n",
      "Scraped entryID: 64369\n",
      "Scraped entryID: 64370\n",
      "Scraped entryID: 64371\n",
      "Scraped entryID: 64372\n",
      "Scraped entryID: 65343\n",
      "Scraped entryID: 65347\n",
      "Scraped entryID: 69385\n",
      "Scraped entryID: 69386\n",
      "Scraped entryID: 69387\n",
      "Scraped entryID: 69388\n",
      "Scraped entryID: 26732\n",
      "Scraped entryID: 26737\n",
      "Scraped entryID: 31934\n",
      "Scraped entryID: 44036\n",
      "Scraped entryID: 44045\n",
      "Scraped entryID: 58965\n",
      "Scraped entryID: 58967\n",
      "Scraped entryID: 58969\n",
      "Scraped entryID: 58971\n",
      "Scraped entryID: 58973\n",
      "Scraped entryID: 3463\n",
      "8030 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 8030\n",
      "8042 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 8042\n",
      "Scraped entryID: 8133\n",
      "Scraped entryID: 8134\n",
      "Scraped entryID: 8135\n",
      "Scraped entryID: 8136\n",
      "Scraped entryID: 8137\n",
      "Scraped entryID: 16520\n",
      "Scraped entryID: 16523\n",
      "Scraped entryID: 16530\n",
      "Scraped entryID: 26526\n",
      "Scraped entryID: 26527\n",
      "Scraped entryID: 26528\n",
      "Scraped entryID: 27904\n",
      "Scraped entryID: 27905\n",
      "Scraped entryID: 27906\n",
      "Scraped entryID: 27907\n",
      "Scraped entryID: 27919\n",
      "Scraped entryID: 27920\n",
      "Scraped entryID: 27921\n",
      "Scraped entryID: 27922\n",
      "Scraped entryID: 29416\n",
      "Scraped entryID: 29417\n",
      "Scraped entryID: 37906\n",
      "Scraped entryID: 37954\n",
      "Scraped entryID: 37957\n",
      "Scraped entryID: 38914\n",
      "Scraped entryID: 39926\n",
      "Scraped entryID: 39929\n",
      "Scraped entryID: 42814\n",
      "Scraped entryID: 42815\n",
      "Scraped entryID: 42816\n",
      "Scraped entryID: 45762\n",
      "Scraped entryID: 45763\n",
      "Scraped entryID: 45764\n",
      "Scraped entryID: 49030\n",
      "Scraped entryID: 49031\n",
      "Scraped entryID: 49312\n",
      "Scraped entryID: 49315\n",
      "Scraped entryID: 58088\n",
      "Scraped entryID: 70401\n",
      "Scraped entryID: 70402\n",
      "Scraped entryID: 70403\n",
      "Scraped entryID: 70405\n",
      "Scraped entryID: 70406\n",
      "Scraped entryID: 70407\n",
      "Scraped entryID: 62261\n",
      "Scraped entryID: 62262\n",
      "Scraped entryID: 62263\n",
      "Scraped entryID: 35472\n",
      "Scraped entryID: 35473\n",
      "Scraped entryID: 35474\n",
      "Scraped entryID: 35475\n",
      "Scraped entryID: 35476\n",
      "Scraped entryID: 35477\n",
      "Scraped entryID: 35478\n",
      "Scraped entryID: 35479\n",
      "Scraped entryID: 35480\n",
      "Scraped entryID: 35481\n",
      "Scraped entryID: 43580\n",
      "Scraped entryID: 43581\n",
      "Scraped entryID: 48906\n",
      "Scraped entryID: 48907\n",
      "Scraped entryID: 48908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped entryID: 48909\n",
      "Scraped entryID: 48910\n",
      "Scraped entryID: 48911\n",
      "Scraped entryID: 1644\n",
      "Scraped entryID: 8230\n",
      "Scraped entryID: 8231\n",
      "Scraped entryID: 8232\n",
      "Scraped entryID: 8233\n",
      "Scraped entryID: 8234\n",
      "Scraped entryID: 13539\n",
      "Scraped entryID: 13540\n",
      "Scraped entryID: 13541\n",
      "Scraped entryID: 16690\n",
      "Scraped entryID: 16691\n",
      "Scraped entryID: 16692\n",
      "Scraped entryID: 16693\n",
      "Scraped entryID: 16694\n",
      "Scraped entryID: 16695\n",
      "Scraped entryID: 16696\n",
      "Scraped entryID: 16697\n",
      "Scraped entryID: 16698\n",
      "Scraped entryID: 16699\n",
      "Scraped entryID: 16700\n",
      "Scraped entryID: 16701\n",
      "Scraped entryID: 16706\n",
      "Scraped entryID: 16707\n",
      "Scraped entryID: 16708\n",
      "Scraped entryID: 16709\n",
      "Scraped entryID: 16710\n",
      "Scraped entryID: 16711\n",
      "Scraped entryID: 16712\n",
      "Scraped entryID: 16713\n",
      "Scraped entryID: 16714\n",
      "Scraped entryID: 16715\n",
      "Scraped entryID: 16716\n",
      "Scraped entryID: 16717\n",
      "Scraped entryID: 16718\n",
      "Scraped entryID: 16719\n",
      "Scraped entryID: 16720\n",
      "Scraped entryID: 16721\n",
      "Scraped entryID: 16722\n",
      "Scraped entryID: 16723\n",
      "Scraped entryID: 16724\n",
      "16725 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 16725\n",
      "16726 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 16726\n",
      "16727 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 16727\n",
      "16728 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 16728\n",
      "Scraped entryID: 32338\n",
      "32339 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 32339\n",
      "32342 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 32342\n",
      "32343 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 32343\n",
      "32409 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 32409\n",
      "32410 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 32410\n",
      "32411 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 32411\n",
      "Scraped entryID: 32414\n",
      "Scraped entryID: 32415\n",
      "Scraped entryID: 32429\n",
      "Scraped entryID: 32431\n",
      "Scraped entryID: 32433\n",
      "Scraped entryID: 32435\n",
      "Scraped entryID: 32697\n",
      "Scraped entryID: 32698\n",
      "Scraped entryID: 32699\n",
      "Scraped entryID: 32700\n",
      "Scraped entryID: 32701\n",
      "Scraped entryID: 32702\n",
      "Scraped entryID: 32703\n",
      "Scraped entryID: 33568\n",
      "Scraped entryID: 33569\n",
      "59770 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 59770\n",
      "59771 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 59771\n",
      "59772 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 59772\n",
      "59773 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 59773\n",
      "59774 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 59774\n",
      "Scraped entryID: 59775\n",
      "Scraped entryID: 59776\n",
      "Scraped entryID: 70314\n",
      "Scraped entryID: 70315\n",
      "Scraped entryID: 70316\n",
      "Scraped entryID: 70317\n",
      "Scraped entryID: 70318\n",
      "Scraped entryID: 70319\n",
      "Scraped entryID: 70320\n",
      "Scraped entryID: 73961\n",
      "Scraped entryID: 73962\n",
      "Scraped entryID: 73963\n",
      "Scraped entryID: 73964\n",
      "Scraped entryID: 73965\n",
      "73966 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 73966\n",
      "73967 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 73967\n",
      "73968 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 73968\n",
      "73969 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 73969\n",
      "Scraped entryID: 73994\n",
      "Scraped entryID: 52315\n",
      "52316 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': 'NADPH',\n",
      " 'start val.': '-',\n",
      " 'type': 'Km',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 52316\n",
      "52317 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': 'NADPH',\n",
      " 'start val.': '-',\n",
      " 'type': 'Km',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 52317\n",
      "52318 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': 'NADPH',\n",
      " 'start val.': '-',\n",
      " 'type': 'Km',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 52318\n",
      "52319 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': 'NADPH',\n",
      " 'start val.': '-',\n",
      " 'type': 'Km',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 52319\n",
      "52320 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': 'NADPH',\n",
      " 'start val.': '-',\n",
      " 'type': 'Km',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 52320\n",
      "Scraped entryID: 52321\n",
      "Scraped entryID: 52322\n",
      "53404 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 53404\n",
      "53405 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 53405\n",
      "53406 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 53406\n",
      "53407 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 53407\n",
      "53408 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 53408\n",
      "53409 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 53409\n",
      "Scraped entryID: 53410\n",
      "Scraped entryID: 53411\n",
      "Scraped entryID: 53412\n",
      "Scraped entryID: 53419\n",
      "Scraped entryID: 53420\n",
      "Scraped entryID: 53421\n",
      "Scraped entryID: 53422\n",
      "59777 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 59777\n",
      "59778 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 59778\n",
      "59779 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 59779\n",
      "Scraped entryID: 59780\n",
      "Scraped entryID: 16702\n",
      "Scraped entryID: 16703\n",
      "Scraped entryID: 16704\n",
      "Scraped entryID: 16705\n",
      "Scraped entryID: 23880\n",
      "Scraped entryID: 23881\n",
      "Scraped entryID: 23882\n",
      "Scraped entryID: 23883\n",
      "Scraped entryID: 23884\n",
      "Scraped entryID: 23885\n",
      "Scraped entryID: 23886\n",
      "Scraped entryID: 23887\n",
      "Scraped entryID: 23888\n",
      "Scraped entryID: 23889\n",
      "Scraped entryID: 23890\n",
      "Scraped entryID: 23891\n",
      "Scraped entryID: 23892\n",
      "Scraped entryID: 23893\n",
      "Scraped entryID: 23894\n",
      "Scraped entryID: 23895\n",
      "Scraped entryID: 23896\n",
      "Scraped entryID: 23897\n",
      "Scraped entryID: 23916\n",
      "Scraped entryID: 23917\n",
      "Scraped entryID: 23918\n",
      "Scraped entryID: 23919\n",
      "Scraped entryID: 30204\n",
      "Scraped entryID: 30205\n",
      "Scraped entryID: 30206\n",
      "Scraped entryID: 66332\n",
      "Scraped entryID: 66333\n",
      "Scraped entryID: 66334\n",
      "Scraped entryID: 66335\n",
      "Scraped entryID: 13070\n",
      "Scraped entryID: 13071\n",
      "Scraped entryID: 13072\n",
      "Scraped entryID: 13073\n",
      "Scraped entryID: 13074\n",
      "Scraped entryID: 13075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped entryID: 13076\n",
      "Scraped entryID: 13077\n",
      "Scraped entryID: 13078\n",
      "Scraped entryID: 13079\n",
      "Scraped entryID: 13080\n",
      "Scraped entryID: 13081\n",
      "Scraped entryID: 13082\n",
      "Scraped entryID: 15329\n",
      "Scraped entryID: 15330\n",
      "Scraped entryID: 15331\n",
      "Scraped entryID: 15332\n",
      "Scraped entryID: 15333\n",
      "Scraped entryID: 15334\n",
      "Scraped entryID: 26504\n",
      "Scraped entryID: 26505\n",
      "Scraped entryID: 26506\n",
      "26507 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 26507\n",
      "Scraped entryID: 26508\n",
      "Scraped entryID: 26509\n",
      "Scraped entryID: 26510\n",
      "Scraped entryID: 26511\n",
      "Scraped entryID: 26512\n",
      "Scraped entryID: 26513\n",
      "Scraped entryID: 26514\n",
      "Scraped entryID: 26515\n",
      "Scraped entryID: 26516\n",
      "Scraped entryID: 26517\n",
      "Scraped entryID: 26518\n",
      "Scraped entryID: 26519\n",
      "Scraped entryID: 26520\n",
      "Scraped entryID: 26521\n",
      "Scraped entryID: 26522\n",
      "Scraped entryID: 26523\n",
      "Scraped entryID: 59615\n",
      "Scraped entryID: 63402\n",
      "Scraped entryID: 63403\n",
      "Scraped entryID: 63404\n",
      "Scraped entryID: 63405\n",
      "Scraped entryID: 63406\n",
      "Scraped entryID: 63407\n",
      "Scraped entryID: 63408\n",
      "Scraped entryID: 63969\n",
      "Scraped entryID: 63970\n",
      "Scraped entryID: 63971\n",
      "Scraped entryID: 63972\n",
      "Scraped entryID: 63973\n",
      "Scraped entryID: 63974\n",
      "Scraped entryID: 63975\n",
      "Scraped entryID: 63976\n",
      "Scraped entryID: 63977\n",
      "Scraped entryID: 63978\n",
      "Scraped entryID: 63980\n",
      "Scraped entryID: 63981\n",
      "Scraped entryID: 63982\n",
      "Scraped entryID: 63983\n",
      "Scraped entryID: 66249\n",
      "Scraped entryID: 66715\n",
      "Scraped entryID: 66726\n",
      "Scraped entryID: 70761\n",
      "Scraped entryID: 70762\n",
      "Scraped entryID: 70763\n",
      "Scraped entryID: 70764\n",
      "Scraped entryID: 70765\n",
      "Scraped entryID: 70766\n",
      "43769 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 43769\n",
      "43770 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 43770\n",
      "Scraped entryID: 44585\n",
      "Scraped entryID: 44586\n",
      "Scraped entryID: 31935\n",
      "Scraped entryID: 31936\n",
      "Scraped entryID: 31937\n",
      "Scraped entryID: 31938\n",
      "Scraped entryID: 31939\n",
      "Scraped entryID: 31940\n",
      "Scraped entryID: 31941\n",
      "Scraped entryID: 31942\n",
      "Scraped entryID: 31943\n",
      "Scraped entryID: 44037\n",
      "Scraped entryID: 44038\n",
      "Scraped entryID: 44039\n",
      "Scraped entryID: 44046\n",
      "Scraped entryID: 45187\n",
      "54502 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 54502\n",
      "54503 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 54503\n",
      "54504 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 54504\n",
      "Scraped entryID: 59260\n",
      "Scraped entryID: 59261\n",
      "Scraped entryID: 60063\n",
      "Scraped entryID: 60064\n",
      "Scraped entryID: 60065\n",
      "45390 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 45390\n",
      "45391 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 45391\n",
      "45392 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 45392\n",
      "Scraped entryID: 61828\n",
      "Scraped entryID: 72842\n",
      "Scraped entryID: 72843\n",
      "Scraped entryID: 72844\n",
      "Scraped entryID: 72845\n",
      "Scraped entryID: 72846\n",
      "Scraped entryID: 72847\n",
      "Scraped entryID: 72848\n",
      "Scraped entryID: 72849\n",
      "Scraped entryID: 72850\n",
      "Scraped entryID: 72851\n",
      "Scraped entryID: 72852\n",
      "Scraped entryID: 72853\n",
      "Scraped entryID: 72854\n",
      "Scraped entryID: 72855\n",
      "Scraped entryID: 72856\n",
      "Scraped entryID: 72857\n",
      "Scraped entryID: 72858\n",
      "59553 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 59553\n",
      "59554 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 59554\n",
      "59555 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 59555\n",
      "Scraped entryID: 59556\n",
      "Scraped entryID: 59557\n",
      "Scraped entryID: 59558\n",
      "Scraped entryID: 65346\n",
      "Scraped entryID: 62345\n",
      "Scraped entryID: 62346\n",
      "Scraped entryID: 62347\n",
      "Scraped entryID: 62348\n",
      "Scraped entryID: 62349\n",
      "Scraped entryID: 62354\n",
      "Scraped entryID: 1918\n",
      "Scraped entryID: 7228\n",
      "Scraped entryID: 7229\n",
      "Scraped entryID: 7230\n",
      "Scraped entryID: 7231\n",
      "Scraped entryID: 7232\n",
      "Scraped entryID: 7233\n",
      "Scraped entryID: 7234\n",
      "Scraped entryID: 7235\n",
      "Scraped entryID: 7236\n",
      "Scraped entryID: 7237\n",
      "Scraped entryID: 7238\n",
      "Scraped entryID: 7239\n",
      "Scraped entryID: 7240\n",
      "Scraped entryID: 7241\n",
      "Scraped entryID: 7242\n",
      "Scraped entryID: 7243\n",
      "Scraped entryID: 7244\n",
      "Scraped entryID: 7245\n",
      "Scraped entryID: 7246\n",
      "Scraped entryID: 7247\n",
      "Scraped entryID: 7248\n",
      "Scraped entryID: 7249\n",
      "Scraped entryID: 7250\n",
      "Scraped entryID: 7251\n",
      "Scraped entryID: 7252\n",
      "Scraped entryID: 7253\n",
      "Scraped entryID: 7254\n",
      "Scraped entryID: 7255\n",
      "Scraped entryID: 7256\n",
      "Scraped entryID: 7257\n",
      "Scraped entryID: 7258\n",
      "Scraped entryID: 7259\n",
      "Scraped entryID: 7260\n",
      "Scraped entryID: 7261\n",
      "Scraped entryID: 7262\n",
      "Scraped entryID: 7263\n",
      "Scraped entryID: 7264\n",
      "Scraped entryID: 7265\n",
      "Scraped entryID: 7266\n",
      "Scraped entryID: 7267\n",
      "Scraped entryID: 7268\n",
      "Scraped entryID: 7269\n",
      "Scraped entryID: 7270\n",
      "Scraped entryID: 7271\n",
      "Scraped entryID: 40476\n",
      "Scraped entryID: 54986\n",
      "Scraped entryID: 54987\n",
      "55314 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 55314\n",
      "55315 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 55315\n",
      "Scraped entryID: 56971\n",
      "Scraped entryID: 56972\n",
      "Scraped entryID: 56973\n",
      "Scraped entryID: 56974\n",
      "Scraped entryID: 56975\n",
      "Scraped entryID: 56976\n",
      "Scraped entryID: 56977\n",
      "Scraped entryID: 56978\n",
      "Scraped entryID: 56979\n",
      "Scraped entryID: 56980\n",
      "Scraped entryID: 56981\n",
      "Scraped entryID: 56982\n",
      "Scraped entryID: 64049\n",
      "Scraped entryID: 64050\n",
      "Scraped entryID: 64082\n",
      "Scraped entryID: 64083\n",
      "Scraped entryID: 64084\n",
      "Scraped entryID: 64085\n",
      "64494 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 64494\n",
      "64495 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 64495\n",
      "64496 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 64496\n",
      "64497 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 64497\n",
      "64498 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 64498\n",
      "64499 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 64499\n",
      "64500 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 64500\n",
      "64501 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 64501\n",
      "Scraped entryID: 64502\n",
      "Scraped entryID: 64503\n",
      "Scraped entryID: 73970\n",
      "Scraped entryID: 73971\n",
      "Scraped entryID: 73972\n",
      "Scraped entryID: 4571\n",
      "Scraped entryID: 4572\n",
      "Scraped entryID: 4573\n",
      "Scraped entryID: 4574\n",
      "Scraped entryID: 4575\n",
      "Scraped entryID: 4576\n",
      "Scraped entryID: 11899\n",
      "Scraped entryID: 13272\n",
      "13275 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 13275\n",
      "Scraped entryID: 15223\n",
      "Scraped entryID: 15227\n",
      "Scraped entryID: 19593\n",
      "Scraped entryID: 44271\n",
      "Scraped entryID: 44294\n",
      "Scraped entryID: 44327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55317 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 55317\n",
      "55318 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 55318\n",
      "Scraped entryID: 58233\n",
      "Scraped entryID: 58752\n",
      "Scraped entryID: 59291\n",
      "Scraped entryID: 59292\n",
      "Scraped entryID: 59293\n",
      "Scraped entryID: 59294\n",
      "Scraped entryID: 64564\n",
      "64566 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 64566\n",
      "64567 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 64567\n",
      "64568 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 64568\n",
      "Scraped entryID: 64569\n",
      "Scraped entryID: 64570\n",
      "Scraped entryID: 65824\n",
      "Scraped entryID: 65825\n",
      "Scraped entryID: 65826\n",
      "66654 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 66654\n",
      "66655 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 66655\n",
      "66656 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 66656\n",
      "66657 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 66657\n",
      "66660 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 66660\n",
      "66661 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 66661\n",
      "66662 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 66662\n",
      "66664 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 66664\n",
      "66665 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 66665\n",
      "Scraped entryID: 66667\n",
      "Scraped entryID: 68118\n",
      "Scraped entryID: 68119\n",
      "Scraped entryID: 68120\n",
      "Scraped entryID: 68121\n",
      "Scraped entryID: 68122\n",
      "Scraped entryID: 68123\n",
      "Scraped entryID: 68133\n",
      "Scraped entryID: 69361\n",
      "Scraped entryID: 69363\n",
      "Scraped entryID: 69364\n",
      "Scraped entryID: 69365\n",
      "71695 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 71695\n",
      "Scraped entryID: 73422\n",
      "Scraped entryID: 73423\n",
      "Scraped entryID: 25771\n",
      "Scraped entryID: 25772\n",
      "Scraped entryID: 25773\n",
      "Scraped entryID: 25774\n",
      "Scraped entryID: 25775\n",
      "Scraped entryID: 25776\n",
      "34177 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 34177\n",
      "34178 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 34178\n",
      "34179 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 34179\n",
      "34180 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 34180\n",
      "Scraped entryID: 34181\n",
      "Scraped entryID: 34182\n",
      "Scraped entryID: 34183\n",
      "Scraped entryID: 13273\n",
      "49498 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 49498\n",
      "Scraped entryID: 6180\n",
      "Scraped entryID: 6896\n",
      "Scraped entryID: 20086\n",
      "Scraped entryID: 21567\n",
      "Scraped entryID: 21568\n",
      "Scraped entryID: 21569\n",
      "Scraped entryID: 21570\n",
      "Scraped entryID: 22653\n",
      "Scraped entryID: 22654\n",
      "Scraped entryID: 22655\n",
      "Scraped entryID: 22656\n",
      "Scraped entryID: 22657\n",
      "Scraped entryID: 22658\n",
      "Scraped entryID: 22659\n",
      "Scraped entryID: 22660\n",
      "Scraped entryID: 22661\n",
      "Scraped entryID: 22662\n",
      "Scraped entryID: 22663\n",
      "Scraped entryID: 22664\n",
      "Scraped entryID: 22665\n",
      "Scraped entryID: 22666\n",
      "Scraped entryID: 25746\n",
      "Scraped entryID: 26858\n",
      "Scraped entryID: 27643\n",
      "Scraped entryID: 27644\n",
      "Scraped entryID: 27645\n",
      "Scraped entryID: 27646\n",
      "Scraped entryID: 27647\n",
      "Scraped entryID: 27648\n",
      "28276 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 28276\n",
      "28277 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 28277\n",
      "Scraped entryID: 32567\n",
      "32568 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 32568\n",
      "Scraped entryID: 34168\n",
      "Scraped entryID: 34169\n",
      "Scraped entryID: 34170\n",
      "39174 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 39174\n",
      "Scraped entryID: 45556\n",
      "Scraped entryID: 45557\n",
      "Scraped entryID: 45558\n",
      "Scraped entryID: 45559\n",
      "Scraped entryID: 45560\n",
      "54914 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'volume',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 54914\n",
      "Scraped entryID: 55200\n",
      "Scraped entryID: 55201\n",
      "Scraped entryID: 56539\n",
      "Scraped entryID: 56572\n",
      "56838 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'volume',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 56838\n",
      "56855 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'volume',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 56855\n",
      "Scraped entryID: 57217\n",
      "Scraped entryID: 57218\n",
      "Scraped entryID: 57219\n",
      "Scraped entryID: 57220\n",
      "Scraped entryID: 57778\n",
      "Scraped entryID: 58370\n",
      "Scraped entryID: 59941\n",
      "Scraped entryID: 59942\n",
      "Scraped entryID: 59943\n",
      "Scraped entryID: 59944\n",
      "Scraped entryID: 59945\n",
      "Scraped entryID: 59946\n",
      "Scraped entryID: 59947\n",
      "Scraped entryID: 59948\n",
      "59949 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 59949\n",
      "Scraped entryID: 35769\n",
      "Scraped entryID: 35770\n",
      "Scraped entryID: 35797\n",
      "Scraped entryID: 35798\n",
      "Scraped entryID: 4505\n",
      "Scraped entryID: 4506\n",
      "Scraped entryID: 4507\n",
      "Scraped entryID: 4508\n",
      "Scraped entryID: 4509\n",
      "Scraped entryID: 4510\n",
      "Scraped entryID: 4511\n",
      "Scraped entryID: 4512\n",
      "Scraped entryID: 4513\n",
      "Scraped entryID: 4514\n",
      "Scraped entryID: 4515\n",
      "Scraped entryID: 4516\n",
      "Scraped entryID: 4517\n",
      "Scraped entryID: 4518\n",
      "Scraped entryID: 4519\n",
      "Scraped entryID: 4520\n",
      "Scraped entryID: 6441\n",
      "Scraped entryID: 6453\n",
      "Scraped entryID: 6456\n",
      "Scraped entryID: 6461\n",
      "Scraped entryID: 6462\n",
      "Scraped entryID: 6465\n",
      "Scraped entryID: 6466\n",
      "Scraped entryID: 6467\n",
      "Scraped entryID: 6482\n",
      "Scraped entryID: 6921\n",
      "Scraped entryID: 11020\n",
      "Scraped entryID: 11022\n",
      "Scraped entryID: 11023\n",
      "Scraped entryID: 11031\n",
      "Scraped entryID: 11269\n",
      "Scraped entryID: 11270\n",
      "Scraped entryID: 11271\n",
      "Scraped entryID: 11297\n",
      "Scraped entryID: 11298\n",
      "Scraped entryID: 11299\n",
      "Scraped entryID: 11300\n",
      "Scraped entryID: 11301\n",
      "Scraped entryID: 11302\n",
      "Scraped entryID: 11303\n",
      "Scraped entryID: 11305\n",
      "Scraped entryID: 11307\n",
      "Scraped entryID: 11309\n",
      "Scraped entryID: 11341\n",
      "Scraped entryID: 11528\n",
      "Scraped entryID: 11529\n",
      "Scraped entryID: 11530\n",
      "Scraped entryID: 11532\n",
      "Scraped entryID: 11533\n",
      "Scraped entryID: 11787\n",
      "Scraped entryID: 11790\n",
      "Scraped entryID: 11792\n",
      "Scraped entryID: 11795\n",
      "Scraped entryID: 11796\n",
      "Scraped entryID: 11797\n",
      "Scraped entryID: 11799\n",
      "Scraped entryID: 11800\n",
      "Scraped entryID: 11802\n",
      "Scraped entryID: 11803\n",
      "Scraped entryID: 12094\n",
      "Scraped entryID: 12111\n",
      "Scraped entryID: 12112\n",
      "Scraped entryID: 12119\n",
      "Scraped entryID: 12120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped entryID: 12121\n",
      "Scraped entryID: 12122\n",
      "Scraped entryID: 12387\n",
      "Scraped entryID: 12830\n",
      "Scraped entryID: 12831\n",
      "Scraped entryID: 12832\n",
      "Scraped entryID: 12861\n",
      "Scraped entryID: 13163\n",
      "Scraped entryID: 13164\n",
      "Scraped entryID: 13201\n",
      "Scraped entryID: 13202\n",
      "Scraped entryID: 13203\n",
      "Scraped entryID: 13204\n",
      "Scraped entryID: 13205\n",
      "Scraped entryID: 13206\n",
      "Scraped entryID: 15616\n",
      "Scraped entryID: 15617\n",
      "Scraped entryID: 15621\n",
      "Scraped entryID: 15622\n",
      "Scraped entryID: 16053\n",
      "Scraped entryID: 16054\n",
      "Scraped entryID: 16055\n",
      "Scraped entryID: 16056\n",
      "Scraped entryID: 16057\n",
      "Scraped entryID: 16058\n",
      "Scraped entryID: 16604\n",
      "Scraped entryID: 16884\n",
      "Scraped entryID: 16885\n",
      "Scraped entryID: 16886\n",
      "Scraped entryID: 16887\n",
      "Scraped entryID: 16888\n",
      "Scraped entryID: 16889\n",
      "Scraped entryID: 16913\n",
      "Scraped entryID: 16914\n",
      "Scraped entryID: 16915\n",
      "Scraped entryID: 16916\n",
      "Scraped entryID: 16917\n",
      "Scraped entryID: 16918\n",
      "Scraped entryID: 16919\n",
      "Scraped entryID: 16920\n",
      "Scraped entryID: 16921\n",
      "Scraped entryID: 16922\n",
      "Scraped entryID: 16923\n",
      "Scraped entryID: 16924\n",
      "Scraped entryID: 16925\n",
      "19554 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': 'Ethanol',\n",
      " 'start val.': '-',\n",
      " 'type': 'Km',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 19554\n",
      "19555 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': 'NAD+',\n",
      " 'start val.': '-',\n",
      " 'type': 'Km',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 19555\n",
      "19558 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': 'Ethanol',\n",
      " 'start val.': '-',\n",
      " 'type': 'Km',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 19558\n",
      "19559 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': 'Ethanol',\n",
      " 'start val.': '-',\n",
      " 'type': 'Km',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 19559\n",
      "19560 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': 'Ethanol',\n",
      " 'start val.': '-',\n",
      " 'type': 'Km',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 19560\n",
      "Scraped entryID: 22350\n",
      "Scraped entryID: 22354\n",
      "Scraped entryID: 22370\n",
      "Scraped entryID: 22374\n",
      "Scraped entryID: 22376\n",
      "Scraped entryID: 22380\n",
      "Scraped entryID: 22382\n",
      "Scraped entryID: 22383\n",
      "Scraped entryID: 22384\n",
      "Scraped entryID: 22385\n",
      "Scraped entryID: 23420\n",
      "Scraped entryID: 23421\n",
      "Scraped entryID: 23422\n",
      "Scraped entryID: 23423\n",
      "Scraped entryID: 23519\n",
      "Scraped entryID: 23520\n",
      "Scraped entryID: 23521\n",
      "Scraped entryID: 23522\n",
      "Scraped entryID: 23523\n",
      "Scraped entryID: 23524\n",
      "Scraped entryID: 23525\n",
      "Scraped entryID: 23526\n",
      "Scraped entryID: 23527\n",
      "Scraped entryID: 23528\n",
      "Scraped entryID: 23529\n",
      "Scraped entryID: 23530\n",
      "Scraped entryID: 23531\n",
      "Scraped entryID: 23532\n",
      "Scraped entryID: 23533\n",
      "Scraped entryID: 23534\n",
      "Scraped entryID: 23535\n",
      "Scraped entryID: 23536\n",
      "Scraped entryID: 23537\n",
      "Scraped entryID: 23538\n",
      "Scraped entryID: 23539\n",
      "Scraped entryID: 23540\n",
      "Scraped entryID: 23541\n",
      "Scraped entryID: 23542\n",
      "Scraped entryID: 23824\n",
      "Scraped entryID: 23825\n",
      "Scraped entryID: 23936\n",
      "Scraped entryID: 26072\n",
      "Scraped entryID: 26108\n",
      "Scraped entryID: 26111\n",
      "Scraped entryID: 26112\n",
      "Scraped entryID: 26113\n",
      "Scraped entryID: 26115\n",
      "Scraped entryID: 26116\n",
      "Scraped entryID: 27125\n",
      "Scraped entryID: 27129\n",
      "Scraped entryID: 27133\n",
      "Scraped entryID: 33115\n",
      "Scraped entryID: 33116\n",
      "Scraped entryID: 33117\n",
      "Scraped entryID: 33118\n",
      "Scraped entryID: 33119\n",
      "Scraped entryID: 33120\n",
      "Scraped entryID: 33121\n",
      "Scraped entryID: 33122\n",
      "Scraped entryID: 33123\n",
      "Scraped entryID: 33124\n",
      "Scraped entryID: 33125\n",
      "Scraped entryID: 33126\n",
      "Scraped entryID: 33923\n",
      "Scraped entryID: 33939\n",
      "Scraped entryID: 33945\n",
      "Scraped entryID: 33946\n",
      "Scraped entryID: 33947\n",
      "Scraped entryID: 33948\n",
      "Scraped entryID: 33949\n",
      "Scraped entryID: 33950\n",
      "Scraped entryID: 33951\n",
      "Scraped entryID: 33952\n",
      "Scraped entryID: 33953\n",
      "Scraped entryID: 33954\n",
      "Scraped entryID: 33955\n",
      "Scraped entryID: 33956\n",
      "Scraped entryID: 33957\n",
      "Scraped entryID: 33958\n",
      "Scraped entryID: 34459\n",
      "Scraped entryID: 34467\n",
      "Scraped entryID: 34475\n",
      "Scraped entryID: 34485\n",
      "Scraped entryID: 36718\n",
      "Scraped entryID: 36719\n",
      "Scraped entryID: 36720\n",
      "Scraped entryID: 36721\n",
      "Scraped entryID: 36722\n",
      "Scraped entryID: 36723\n",
      "Scraped entryID: 36724\n",
      "Scraped entryID: 36725\n",
      "Scraped entryID: 36813\n",
      "Scraped entryID: 36815\n",
      "Scraped entryID: 42643\n",
      "Scraped entryID: 42645\n",
      "Scraped entryID: 46081\n",
      "Scraped entryID: 46082\n",
      "Scraped entryID: 51860\n",
      "Scraped entryID: 51865\n",
      "Scraped entryID: 54491\n",
      "54665 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 54665\n",
      "54666 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 54666\n",
      "54667 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 54667\n",
      "54668 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 54668\n",
      "Scraped entryID: 55928\n",
      "Scraped entryID: 55958\n",
      "Scraped entryID: 55959\n",
      "Scraped entryID: 55960\n",
      "Scraped entryID: 56319\n",
      "Scraped entryID: 56320\n",
      "Scraped entryID: 56323\n",
      "Scraped entryID: 56535\n",
      "Scraped entryID: 56564\n",
      "Scraped entryID: 56565\n",
      "56844 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'volume',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 56844\n",
      "56859 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'volume',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 56859\n",
      "Scraped entryID: 57777\n",
      "Scraped entryID: 57779\n",
      "Scraped entryID: 64442\n",
      "Scraped entryID: 64443\n",
      "Scraped entryID: 64450\n",
      "Scraped entryID: 64451\n",
      "Scraped entryID: 64456\n",
      "Scraped entryID: 64457\n",
      "Scraped entryID: 64462\n",
      "Scraped entryID: 64463\n",
      "Scraped entryID: 5895\n",
      "Scraped entryID: 5896\n",
      "5899 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 5899\n",
      "5900 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 5900\n",
      "5901 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 5901\n",
      "5902 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 5902\n",
      "Scraped entryID: 11959\n",
      "Scraped entryID: 11960\n",
      "Scraped entryID: 15030\n",
      "Scraped entryID: 15031\n",
      "Scraped entryID: 15032\n",
      "Scraped entryID: 15033\n",
      "Scraped entryID: 15034\n",
      "Scraped entryID: 15035\n",
      "Scraped entryID: 15036\n",
      "Scraped entryID: 15037\n",
      "Scraped entryID: 15038\n",
      "Scraped entryID: 15039\n",
      "Scraped entryID: 15040\n",
      "Scraped entryID: 15041\n",
      "Scraped entryID: 15042\n",
      "Scraped entryID: 15043\n",
      "Scraped entryID: 15044\n",
      "Scraped entryID: 15045\n",
      "Scraped entryID: 15046\n",
      "Scraped entryID: 15047\n",
      "Scraped entryID: 15048\n",
      "Scraped entryID: 15049\n",
      "Scraped entryID: 15050\n",
      "Scraped entryID: 15051\n",
      "Scraped entryID: 15052\n",
      "Scraped entryID: 15053\n",
      "Scraped entryID: 15054\n",
      "Scraped entryID: 15055\n",
      "Scraped entryID: 15056\n",
      "Scraped entryID: 15057\n",
      "Scraped entryID: 16065\n",
      "Scraped entryID: 16066\n",
      "Scraped entryID: 16067\n",
      "Scraped entryID: 16068\n",
      "Scraped entryID: 16069\n",
      "Scraped entryID: 16070\n",
      "21429 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 21429\n",
      "21430 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 21430\n",
      "Scraped entryID: 21991\n",
      "22598 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 22598\n",
      "22599 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 22599\n",
      "22602 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 22602\n",
      "22603 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 22603\n",
      "22604 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 22604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22605 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 22605\n",
      "Scraped entryID: 25465\n",
      "Scraped entryID: 25466\n",
      "Scraped entryID: 25467\n",
      "Scraped entryID: 25468\n",
      "Scraped entryID: 25469\n",
      "Scraped entryID: 25470\n",
      "Scraped entryID: 25471\n",
      "Scraped entryID: 25472\n",
      "Scraped entryID: 25473\n",
      "Scraped entryID: 25474\n",
      "Scraped entryID: 25475\n",
      "Scraped entryID: 25476\n",
      "Scraped entryID: 25477\n",
      "Scraped entryID: 25478\n",
      "Scraped entryID: 25479\n",
      "Scraped entryID: 25480\n",
      "Scraped entryID: 25481\n",
      "Scraped entryID: 25482\n",
      "Scraped entryID: 25483\n",
      "Scraped entryID: 25484\n",
      "Scraped entryID: 25485\n",
      "Scraped entryID: 25486\n",
      "Scraped entryID: 25487\n",
      "Scraped entryID: 25488\n",
      "Scraped entryID: 25489\n",
      "Scraped entryID: 25490\n",
      "Scraped entryID: 25491\n",
      "Scraped entryID: 68679\n",
      "68677 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 68677\n",
      "Scraped entryID: 68678\n",
      "Scraped entryID: 5897\n",
      "Scraped entryID: 5898\n",
      "Scraped entryID: 41805\n",
      "Scraped entryID: 41807\n",
      "Scraped entryID: 41808\n",
      "Scraped entryID: 41809\n",
      "Scraped entryID: 41810\n",
      "Scraped entryID: 41811\n",
      "Scraped entryID: 41812\n",
      "Scraped entryID: 58080\n",
      "Scraped entryID: 58081\n",
      "Scraped entryID: 58084\n",
      "Scraped entryID: 58085\n",
      "Scraped entryID: 64247\n",
      "Scraped entryID: 64248\n",
      "Scraped entryID: 64249\n",
      "Scraped entryID: 65340\n",
      "Scraped entryID: 67038\n",
      "68099 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 68099\n",
      "68100 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 68100\n",
      "Scraped entryID: 72255\n",
      "Scraped entryID: 72256\n",
      "Scraped entryID: 41806\n",
      "Scraped entryID: 58082\n",
      "Scraped entryID: 58083\n",
      "Scraped entryID: 58086\n",
      "Scraped entryID: 58087\n",
      "Scraped entryID: 64384\n",
      "Scraped entryID: 64385\n",
      "Scraped entryID: 66536\n",
      "Scraped entryID: 66537\n",
      "Scraped entryID: 67036\n",
      "Scraped entryID: 67037\n",
      "68097 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 68097\n",
      "68098 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 68098\n",
      "Scraped entryID: 69369\n",
      "Scraped entryID: 69370\n",
      "Scraped entryID: 69371\n",
      "Scraped entryID: 72257\n",
      "Scraped entryID: 72258\n",
      "Scraped entryID: 41782\n",
      "Scraped entryID: 41783\n",
      "Scraped entryID: 45028\n",
      "Scraped entryID: 46948\n",
      "Scraped entryID: 46955\n",
      "Scraped entryID: 73050\n",
      "Scraped entryID: 73051\n",
      "Scraped entryID: 73052\n",
      "Scraped entryID: 73053\n",
      "Scraped entryID: 73054\n",
      "73261 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 73261\n",
      "73262 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 73262\n",
      "Scraped entryID: 64858\n",
      "Scraped entryID: 64859\n",
      "Scraped entryID: 64860\n",
      "Scraped entryID: 64861\n",
      "Scraped entryID: 64862\n",
      "Scraped entryID: 64863\n",
      "Scraped entryID: 64864\n",
      "Scraped entryID: 64865\n",
      "Scraped entryID: 64866\n",
      "Scraped entryID: 64867\n",
      "Scraped entryID: 64868\n",
      "Scraped entryID: 64869\n",
      "Scraped entryID: 64870\n",
      "Scraped entryID: 64871\n",
      "Scraped entryID: 64872\n",
      "Scraped entryID: 64873\n",
      "Scraped entryID: 64874\n",
      "Scraped entryID: 64875\n",
      "Scraped entryID: 64876\n",
      "Scraped entryID: 64877\n",
      "Scraped entryID: 64878\n",
      "Scraped entryID: 64879\n",
      "Scraped entryID: 64880\n",
      "Scraped entryID: 64881\n",
      "Scraped entryID: 64882\n",
      "Scraped entryID: 64883\n",
      "Scraped entryID: 64884\n",
      "Scraped entryID: 64885\n",
      "Scraped entryID: 64886\n",
      "Scraped entryID: 40879\n",
      "Scraped entryID: 40880\n",
      "Scraped entryID: 40881\n",
      "Scraped entryID: 40882\n",
      "Scraped entryID: 40883\n",
      "Scraped entryID: 40884\n",
      "Scraped entryID: 40885\n",
      "Scraped entryID: 40886\n",
      "Scraped entryID: 40887\n",
      "Scraped entryID: 40888\n",
      "40889 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 40889\n",
      "40890 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 40890\n",
      "40891 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 40891\n",
      "40892 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 40892\n",
      "40893 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 40893\n",
      "40894 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 40894\n",
      "40895 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 40895\n",
      "40896 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 40896\n",
      "40897 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 40897\n",
      "40898 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 40898\n",
      "40899 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 40899\n",
      "40900 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 40900\n",
      "40901 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 40901\n",
      "40902 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 40902\n",
      "40903 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 40903\n",
      "Scraped entryID: 47237\n",
      "Scraped entryID: 47238\n",
      "Scraped entryID: 47239\n",
      "Scraped entryID: 47240\n",
      "Scraped entryID: 47241\n",
      "Scraped entryID: 47242\n",
      "47572 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': 'ATP',\n",
      " 'start val.': '-',\n",
      " 'type': 'Km',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 47572\n",
      "47573 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': 'ATP',\n",
      " 'start val.': '-',\n",
      " 'type': 'Km',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 47573\n",
      "Scraped entryID: 47574\n",
      "Scraped entryID: 47575\n",
      "Scraped entryID: 47576\n",
      "Scraped entryID: 47577\n",
      "Scraped entryID: 47578\n",
      "48820 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 48820\n",
      "Scraped entryID: 48821\n",
      "Scraped entryID: 48823\n",
      "Scraped entryID: 48824\n",
      "Scraped entryID: 48825\n",
      "Scraped entryID: 48826\n",
      "Scraped entryID: 48871\n",
      "Scraped entryID: 48872\n",
      "Scraped entryID: 48873\n",
      "Scraped entryID: 48874\n",
      "48875 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 48875\n",
      "48876 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 48876\n",
      "Scraped entryID: 48877\n",
      "Scraped entryID: 48878\n",
      "Scraped entryID: 48879\n",
      "Scraped entryID: 48884\n",
      "Scraped entryID: 48885\n",
      "Scraped entryID: 48886\n",
      "Scraped entryID: 12522\n",
      "Scraped entryID: 12523\n",
      "Scraped entryID: 12524\n",
      "Scraped entryID: 50295\n",
      "Scraped entryID: 50296\n",
      "Scraped entryID: 54101\n",
      "Scraped entryID: 54809\n",
      "Scraped entryID: 54810\n",
      "Scraped entryID: 54811\n",
      "54812 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 54812\n",
      "Scraped entryID: 54813\n",
      "54814 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 54814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped entryID: 54815\n",
      "Scraped entryID: 54816\n",
      "Scraped entryID: 55584\n",
      "Scraped entryID: 55585\n",
      "55586 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 55586\n",
      "55587 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 55587\n",
      "55588 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 55588\n",
      "Scraped entryID: 56567\n",
      "Scraped entryID: 56791\n",
      "Scraped entryID: 56792\n",
      "56846 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'volume',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 56846\n",
      "Scraped entryID: 58124\n",
      "Scraped entryID: 58125\n",
      "Scraped entryID: 58346\n",
      "Scraped entryID: 58347\n",
      "Scraped entryID: 58348\n",
      "Scraped entryID: 58349\n",
      "58869 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 58869\n",
      "58870 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 58870\n",
      "Scraped entryID: 58871\n",
      "Scraped entryID: 58872\n",
      "Scraped entryID: 66076\n",
      "Scraped entryID: 5128\n",
      "Scraped entryID: 5129\n",
      "5130 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': 'ATP',\n",
      " 'start val.': '-',\n",
      " 'type': 'Km',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 5130\n",
      "5131 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': 'UTP',\n",
      " 'start val.': '-',\n",
      " 'type': 'Km',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 5131\n",
      "Scraped entryID: 5132\n",
      "Scraped entryID: 5133\n",
      "Scraped entryID: 5455\n",
      "Scraped entryID: 5456\n",
      "Scraped entryID: 5457\n",
      "Scraped entryID: 5458\n",
      "Scraped entryID: 5459\n",
      "Scraped entryID: 5460\n",
      "Scraped entryID: 5461\n",
      "Scraped entryID: 5462\n",
      "Scraped entryID: 40464\n",
      "Scraped entryID: 40465\n",
      "Scraped entryID: 40466\n",
      "Scraped entryID: 40467\n",
      "Scraped entryID: 40468\n",
      "Scraped entryID: 40469\n",
      "Scraped entryID: 73258\n",
      "Scraped entryID: 73259\n",
      "Scraped entryID: 73260\n",
      "Scraped entryID: 4820\n",
      "Scraped entryID: 4821\n",
      "Scraped entryID: 4822\n",
      "Scraped entryID: 4823\n",
      "Scraped entryID: 4824\n",
      "Scraped entryID: 4825\n",
      "Scraped entryID: 4826\n",
      "Scraped entryID: 4827\n",
      "Scraped entryID: 4828\n",
      "Scraped entryID: 9311\n",
      "Scraped entryID: 9316\n",
      "Scraped entryID: 9321\n",
      "Scraped entryID: 9323\n",
      "43647 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 43647\n",
      "43648 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 43648\n",
      "43649 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 43649\n",
      "Scraped entryID: 43701\n",
      "Scraped entryID: 43702\n",
      "43703 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 43703\n",
      "Scraped entryID: 43704\n",
      "Scraped entryID: 43705\n",
      "Scraped entryID: 43706\n",
      "Scraped entryID: 45467\n",
      "Scraped entryID: 45468\n",
      "Scraped entryID: 45469\n",
      "Scraped entryID: 45470\n",
      "Scraped entryID: 45471\n",
      "Scraped entryID: 45472\n",
      "Scraped entryID: 45473\n",
      "Scraped entryID: 45474\n",
      "Scraped entryID: 45475\n",
      "68039 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 68039\n",
      "68040 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 68040\n",
      "68041 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 68041\n",
      "68042 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 68042\n",
      "68043 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 68043\n",
      "68044 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 68044\n",
      "68045 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 68045\n",
      "68046 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 68046\n",
      "68047 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 68047\n",
      "68048 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 68048\n",
      "Scraped entryID: 68049\n",
      "68050 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 68050\n",
      "68051 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 68051\n",
      "68052 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 68052\n",
      "68053 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 68053\n",
      "Scraped entryID: 68055\n",
      "Scraped entryID: 9312\n",
      "Scraped entryID: 9313\n",
      "Scraped entryID: 9314\n",
      "Scraped entryID: 9315\n",
      "Scraped entryID: 9317\n",
      "Scraped entryID: 9318\n",
      "Scraped entryID: 9319\n",
      "Scraped entryID: 9320\n",
      "Scraped entryID: 9322\n",
      "Scraped entryID: 9324\n",
      "Scraped entryID: 68054\n",
      "Scraped entryID: 8429\n",
      "Scraped entryID: 61572\n",
      "Scraped entryID: 61573\n",
      "Scraped entryID: 61574\n",
      "Scraped entryID: 61575\n",
      "Scraped entryID: 61576\n",
      "Scraped entryID: 61577\n",
      "Scraped entryID: 61578\n",
      "Scraped entryID: 61579\n",
      "Scraped entryID: 61580\n",
      "Scraped entryID: 61581\n",
      "Scraped entryID: 61582\n",
      "Scraped entryID: 61583\n",
      "Scraped entryID: 61584\n",
      "Scraped entryID: 61585\n",
      "Scraped entryID: 61586\n",
      "Scraped entryID: 61587\n",
      "Scraped entryID: 61588\n",
      "Scraped entryID: 61589\n",
      "Scraped entryID: 61590\n",
      "Scraped entryID: 61591\n",
      "Scraped entryID: 61592\n",
      "Scraped entryID: 61593\n",
      "Scraped entryID: 61594\n",
      "Scraped entryID: 61595\n",
      "Scraped entryID: 68290\n",
      "Scraped entryID: 68291\n",
      "Scraped entryID: 68292\n",
      "Scraped entryID: 68293\n",
      "Scraped entryID: 68294\n",
      "71358 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 71358\n",
      "Scraped entryID: 71359\n",
      "Scraped entryID: 71360\n",
      "71361 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 71361\n",
      "71362 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 71362\n",
      "Scraped entryID: 71363\n",
      "Scraped entryID: 73763\n",
      "Scraped entryID: 73764\n",
      "Scraped entryID: 73765\n",
      "73766 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '1.07',\n",
      " 'type': 'Keq',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 73766\n",
      "Scraped entryID: 71364\n",
      "Scraped entryID: 71365\n",
      "Scraped entryID: 16059\n",
      "Scraped entryID: 16060\n",
      "Scraped entryID: 16061\n",
      "Scraped entryID: 16062\n",
      "Scraped entryID: 16063\n",
      "Scraped entryID: 16064\n",
      "Scraped entryID: 25412\n",
      "Scraped entryID: 25413\n",
      "Scraped entryID: 25414\n",
      "Scraped entryID: 25415\n",
      "Scraped entryID: 25416\n",
      "Scraped entryID: 25417\n",
      "Scraped entryID: 25418\n",
      "Scraped entryID: 25419\n",
      "Scraped entryID: 25420\n",
      "Scraped entryID: 25421\n",
      "Scraped entryID: 25422\n",
      "Scraped entryID: 25423\n",
      "Scraped entryID: 25424\n",
      "Scraped entryID: 25425\n",
      "Scraped entryID: 25426\n",
      "Scraped entryID: 25427\n",
      "Scraped entryID: 25428\n",
      "Scraped entryID: 25429\n",
      "Scraped entryID: 25430\n",
      "Scraped entryID: 25431\n",
      "Scraped entryID: 25432\n",
      "Scraped entryID: 25433\n",
      "Scraped entryID: 25434\n",
      "Scraped entryID: 25435\n",
      "Scraped entryID: 25436\n",
      "Scraped entryID: 25437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped entryID: 25438\n",
      "Scraped entryID: 25439\n",
      "Scraped entryID: 34388\n",
      "Scraped entryID: 73168\n",
      "Scraped entryID: 73169\n",
      "Scraped entryID: 73170\n",
      "Scraped entryID: 73171\n",
      "Scraped entryID: 73172\n",
      "Scraped entryID: 73173\n",
      "Scraped entryID: 73174\n",
      "Scraped entryID: 73175\n",
      "Scraped entryID: 73176\n",
      "Scraped entryID: 73177\n",
      "Scraped entryID: 73178\n",
      "Scraped entryID: 73179\n",
      "Scraped entryID: 73180\n",
      "Scraped entryID: 73181\n",
      "Scraped entryID: 73182\n",
      "Scraped entryID: 73183\n",
      "Scraped entryID: 25110\n",
      "Scraped entryID: 25111\n",
      "Scraped entryID: 25112\n",
      "Scraped entryID: 25113\n",
      "Scraped entryID: 25114\n",
      "Scraped entryID: 25115\n",
      "Scraped entryID: 25116\n",
      "Scraped entryID: 28897\n",
      "Scraped entryID: 28898\n",
      "Scraped entryID: 28899\n",
      "Scraped entryID: 28900\n",
      "Scraped entryID: 28901\n",
      "Scraped entryID: 28902\n",
      "Scraped entryID: 28906\n",
      "Scraped entryID: 28907\n",
      "Scraped entryID: 28908\n",
      "Scraped entryID: 54460\n",
      "Scraped entryID: 66470\n",
      "Scraped entryID: 66471\n",
      "Scraped entryID: 53052\n",
      "Scraped entryID: 53053\n",
      "Scraped entryID: 54294\n",
      "Scraped entryID: 54295\n",
      "Scraped entryID: 54296\n",
      "Scraped entryID: 54297\n",
      "Scraped entryID: 54298\n",
      "Scraped entryID: 54299\n",
      "Scraped entryID: 54300\n",
      "Scraped entryID: 54301\n",
      "Scraped entryID: 54302\n",
      "Scraped entryID: 60004\n",
      "Scraped entryID: 60163\n",
      "Scraped entryID: 60164\n",
      "Scraped entryID: 60165\n",
      "Scraped entryID: 60166\n",
      "Scraped entryID: 60167\n",
      "Scraped entryID: 60168\n",
      "Scraped entryID: 60169\n",
      "Scraped entryID: 60170\n",
      "Scraped entryID: 60171\n",
      "Scraped entryID: 60172\n",
      "Scraped entryID: 60173\n",
      "Scraped entryID: 60174\n",
      "Scraped entryID: 60175\n",
      "Scraped entryID: 60176\n",
      "Scraped entryID: 60177\n",
      "Scraped entryID: 60178\n",
      "Scraped entryID: 65338\n",
      "Scraped entryID: 65339\n",
      "68094 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 68094\n",
      "68095 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 68095\n",
      "Scraped entryID: 69362\n",
      "Scraped entryID: 69366\n",
      "Scraped entryID: 69367\n",
      "Scraped entryID: 69368\n",
      "Scraped entryID: 9178\n",
      "Scraped entryID: 9179\n",
      "Scraped entryID: 9180\n",
      "Scraped entryID: 9181\n",
      "Scraped entryID: 9182\n",
      "Scraped entryID: 9183\n",
      "Scraped entryID: 9184\n",
      "Scraped entryID: 9185\n",
      "Scraped entryID: 9186\n",
      "Scraped entryID: 9187\n",
      "Scraped entryID: 9188\n",
      "Scraped entryID: 9189\n",
      "Scraped entryID: 9190\n",
      "Scraped entryID: 9191\n",
      "Scraped entryID: 9192\n",
      "Scraped entryID: 9193\n",
      "Scraped entryID: 18564\n",
      "Scraped entryID: 18565\n",
      "Scraped entryID: 18566\n",
      "Scraped entryID: 18567\n",
      "Scraped entryID: 18568\n",
      "Scraped entryID: 18569\n",
      "Scraped entryID: 18570\n",
      "Scraped entryID: 18571\n",
      "Scraped entryID: 18572\n",
      "Scraped entryID: 18573\n",
      "Scraped entryID: 18574\n",
      "Scraped entryID: 43919\n",
      "Scraped entryID: 43920\n",
      "Scraped entryID: 63638\n",
      "Scraped entryID: 63639\n",
      "Scraped entryID: 63640\n",
      "Scraped entryID: 63641\n",
      "Scraped entryID: 63642\n",
      "Scraped entryID: 63643\n",
      "Scraped entryID: 63644\n",
      "Scraped entryID: 63645\n",
      "Scraped entryID: 63678\n",
      "Scraped entryID: 63679\n",
      "Scraped entryID: 63680\n",
      "Scraped entryID: 63681\n",
      "Scraped entryID: 63682\n",
      "Scraped entryID: 63683\n",
      "Scraped entryID: 63684\n",
      "Scraped entryID: 63685\n",
      "Scraped entryID: 63686\n",
      "Scraped entryID: 63687\n",
      "Scraped entryID: 63688\n",
      "Scraped entryID: 63689\n",
      "Scraped entryID: 63690\n",
      "Scraped entryID: 63691\n",
      "Scraped entryID: 63692\n",
      "Scraped entryID: 63693\n",
      "Scraped entryID: 63694\n",
      "Scraped entryID: 63695\n",
      "Scraped entryID: 63696\n",
      "63697 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': 'ATP',\n",
      " 'start val.': '2.5',\n",
      " 'type': 'Hill coefficient',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 63697\n",
      "Scraped entryID: 66534\n",
      "Scraped entryID: 66535\n",
      "Scraped entryID: 68005\n",
      "Scraped entryID: 68006\n",
      "Scraped entryID: 68007\n",
      "Scraped entryID: 68093\n",
      "Scraped entryID: 68096\n",
      "Scraped entryID: 72259\n",
      "Scraped entryID: 72260\n",
      "Scraped entryID: 72261\n",
      "Scraped entryID: 72262\n",
      "Scraped entryID: 72263\n",
      "Scraped entryID: 72343\n",
      "Scraped entryID: 72344\n",
      "Scraped entryID: 72345\n",
      "Scraped entryID: 72360\n",
      "Scraped entryID: 72361\n",
      "Scraped entryID: 72362\n",
      "Scraped entryID: 72363\n",
      "Scraped entryID: 72364\n",
      "Scraped entryID: 72365\n",
      "Scraped entryID: 68022\n",
      "Scraped entryID: 68023\n",
      "Scraped entryID: 68024\n",
      "Scraped entryID: 68558\n",
      "Scraped entryID: 68559\n",
      "Scraped entryID: 19375\n",
      "Scraped entryID: 19376\n",
      "Scraped entryID: 24910\n",
      "Scraped entryID: 24911\n",
      "Scraped entryID: 24912\n",
      "Scraped entryID: 24913\n",
      "Scraped entryID: 54890\n",
      "Scraped entryID: 54891\n",
      "Scraped entryID: 54892\n",
      "Scraped entryID: 54893\n",
      "Scraped entryID: 54894\n",
      "54895 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': '-',\n",
      " 'start val.': '-',\n",
      " 'type': 'Vmax',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 54895\n",
      "Scraped entryID: 54896\n",
      "Scraped entryID: 54897\n",
      "Scraped entryID: 54898\n",
      "Scraped entryID: 54899\n",
      "54900 missing_unit\n",
      "{'comment': '-',\n",
      " 'deviat.': '-',\n",
      " 'end val.': '-',\n",
      " 'species': 'D-Ribose 5-phosphate',\n",
      " 'start val.': '-',\n",
      " 'type': 'Km',\n",
      " 'unit': '-'}\n",
      "Scraped entryID: 54900\n",
      "Scraped entryID: 55472\n",
      "Scraped entryID: 55473\n",
      "Scraped entryID: 55474\n",
      "Scraped entryID: 55475\n",
      "Scraped entryID: 55476\n",
      "Scraped entryID: 55477\n",
      "Scraped entryID: 55478\n",
      "Scraped entryID: 59997\n",
      "Scraped entryID: 59998\n",
      "Scraped entryID: 59999\n",
      "Scraped entryID: 60000\n",
      "Scraped entryID: 60001\n",
      "Scraped entryID: 60002\n",
      "Scraped entryID: 60003\n",
      "Scraped entryID: 63845\n",
      "Scraped entryID: 63846\n",
      "Scraped entryID: 63847\n",
      "Scraped entryID: 63848\n",
      "Scraped entryID: 63849\n",
      "Scraped entryID: 63850\n",
      "Scraped entryID: 63851\n",
      "Scraped entryID: 73701\n",
      "Scraped entryID: 73702\n",
      "The parameter specifications have been scraped.\n",
      "reaction Formate + Acetyl-CoA = Coenzyme A + Pyruvate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3326: DtypeWarning: Columns (27) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'formate C-acetyltransferase'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-41302618bc34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'run'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'../bigg_sabio/scrapper.py'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mscraping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSABIO_scraping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mscraping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../bigg_models/Ecoli core, BiGG, indented.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test_Ecoli-2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Dropbox\\My PC (DESKTOP-M302P50)\\Documents\\UVic Civil Engineering\\dFBA\\BiGG_SABIO\\bigg_sabio\\scrapper.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(self, bigg_model_path, bigg_model_name)\u001b[0m\n\u001b[0;32m    900\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m         \u001b[1;31m# update the step counter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 902\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'The dFBA data file have been generated.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    903\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_progress_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_number\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Dropbox\\My PC (DESKTOP-M302P50)\\Documents\\UVic Civil Engineering\\dFBA\\BiGG_SABIO\\bigg_sabio\\scrapper.py\u001b[0m in \u001b[0;36mto_fba\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameter_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m                 \u001b[0mstart_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparameter_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 766\u001b[1;33m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    767\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mend\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"end val.\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"end value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameter_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'formate C-acetyltransferase'"
     ]
    }
   ],
   "source": [
    "# %run \"../../../../biofouling_models/Web scraping and data/SABIO/complete_sabio.py\"\n",
    "%run ../bigg_sabio/scrapper.py\n",
    "scraping = SABIO_scraping()\n",
    "scraping.main('../bigg_models/Ecoli core, BiGG, indented.json', 'test_Ecoli-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing Step 4\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Formate c-acetyltransferase'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-9da6b68fa8dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'run'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'../bigg_sabio/scrapper.py'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbgsb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSABIO_scraping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../bigg_models/Ecoli core, BiGG, indented.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test_Ecoli-2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mbgsb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Dropbox\\My PC (DESKTOP-M302P50)\\Documents\\UVic Civil Engineering\\dFBA\\BiGG_SABIO\\bigg_sabio\\scrapper.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    293\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscrape_entryids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_number\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_fba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Execution complete.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Dropbox\\My PC (DESKTOP-M302P50)\\Documents\\UVic Civil Engineering\\dFBA\\BiGG_SABIO\\bigg_sabio\\scrapper.py\u001b[0m in \u001b[0;36mto_fba\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0menzyme\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbigg_enzyme_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_contents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m                 \u001b[0mincorrect_enzymes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbigg_enzyme_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 809\u001b[1;33m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_contents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbigg_enzyme_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'annotations'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    810\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'unidentified enzyme name {bigg_enzyme_name}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Formate c-acetyltransferase'"
     ]
    }
   ],
   "source": [
    "%run ../bigg_sabio/scrapper.py\n",
    "bgsb = SABIO_scraping('../bigg_models/Ecoli core, BiGG, indented.json', 'test_Ecoli-2')\n",
    "bgsb.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated completion of scraping the XLS data for test_Ecoli-3: 2022-01-30 01:05:48.741807, in 4.75 hours\n"
     ]
    }
   ],
   "source": [
    "%run ../bigg_sabio/scrapper.py\n",
    "bgsb = SABIO_scraping('../bigg_models/Ecoli core, BiGG, indented.json', 'test_Ecoli-3')\n",
    "%time bgsb.scrape_bigg_xls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Phosphofructokinase',\n",
      " 'Pyruvate formate lyase',\n",
      " 'Glucose-6-phosphate isomerase',\n",
      " 'Phosphoglycerate kinase',\n",
      " '6-phosphogluconolactonase',\n",
      " 'Acetaldehyde dehydrogenase (acetylating)',\n",
      " '2 oxoglutarate reversible transport via symport',\n",
      " 'Phosphoglycerate mutase',\n",
      " 'Phosphate reversible transport via symport',\n",
      " 'Alcohol dehydrogenase (ethanol)',\n",
      " 'Acetaldehyde reversible transport',\n",
      " 'Acetate kinase',\n",
      " 'Phosphoenolpyruvate carboxylase',\n",
      " 'Aconitase (half-reaction A, Citrate hydro-lyase)',\n",
      " 'Aconitase (half-reaction B, Isocitrate hydro-lyase)',\n",
      " 'ATP maintenance requirement',\n",
      " 'Phosphoenolpyruvate carboxykinase',\n",
      " 'Acetate reversible transport via proton symport',\n",
      " 'Phosphoenolpyruvate synthase',\n",
      " 'Adenylate kinase',\n",
      " '2-Oxogluterate dehydrogenase',\n",
      " 'ATP synthase (four protons for one ATP)',\n",
      " 'Phosphotransacetylase',\n",
      " 'Pyruvate kinase',\n",
      " 'Biomass Objective Function with GAM',\n",
      " 'Pyruvate transport in via proton symport',\n",
      " 'CO2 transporter via diffusion',\n",
      " 'Ribulose 5-phosphate 3-epimerase',\n",
      " 'Citrate synthase',\n",
      " 'Ribose-5-phosphate isomerase',\n",
      " 'Succinate transport via proton symport (2 H)',\n",
      " 'Cytochrome oxidase bd (ubiquinol-8: 2 protons)',\n",
      " 'D lactate transport via proton symport',\n",
      " 'Enolase',\n",
      " 'Succinate transport out via proton antiport',\n",
      " 'Ethanol reversible transport via proton symport',\n",
      " 'Succinate dehydrogenase (irreversible)',\n",
      " 'Succinyl-CoA synthetase (ADP-forming)',\n",
      " 'Transaldolase',\n",
      " 'NAD(P) transhydrogenase',\n",
      " 'Transketolase',\n",
      " 'Triose-phosphate isomerase',\n",
      " 'Acetate exchange',\n",
      " 'Acetaldehyde exchange',\n",
      " '2-Oxoglutarate exchange',\n",
      " 'CO2 exchange',\n",
      " 'Ethanol exchange',\n",
      " 'Formate exchange',\n",
      " 'D-Fructose exchange',\n",
      " 'Fumarate exchange',\n",
      " 'D-Glucose exchange',\n",
      " 'L-Glutamine exchange',\n",
      " 'L-Glutamate exchange',\n",
      " 'H+ exchange',\n",
      " 'H2O exchange',\n",
      " 'D-lactate exchange',\n",
      " 'L-Malate exchange',\n",
      " 'Ammonia exchange',\n",
      " 'O2 exchange',\n",
      " 'Phosphate exchange',\n",
      " 'Pyruvate exchange',\n",
      " 'Succinate exchange',\n",
      " 'Fructose-bisphosphate aldolase',\n",
      " 'Fructose-bisphosphatase',\n",
      " 'Formate transport in via proton symport',\n",
      " 'Formate transport via diffusion',\n",
      " 'Fumarate reductase',\n",
      " 'Fructose transport via PEP:Pyr PTS (f6p generating)',\n",
      " 'Fumarase',\n",
      " 'Fumarate transport via proton symport (2 H)',\n",
      " 'Glucose 6-phosphate dehydrogenase',\n",
      " 'Glyceraldehyde-3-phosphate dehydrogenase',\n",
      " 'D-glucose transport via PEP:Pyr PTS',\n",
      " 'Glutamine synthetase',\n",
      " 'L-glutamine transport via ABC system',\n",
      " 'Glutamate dehydrogenase (NADP)',\n",
      " 'Glutaminase',\n",
      " 'Glutamate synthase (NADPH)',\n",
      " 'L glutamate transport via proton symport  reversible',\n",
      " 'Phosphogluconate dehydrogenase',\n",
      " 'H2O transport via diffusion',\n",
      " 'Isocitrate dehydrogenase (NADP)',\n",
      " 'Isocitrate lyase',\n",
      " 'D-lactate dehydrogenase',\n",
      " 'Malate synthase',\n",
      " 'Malate transport via proton symport (2 H)',\n",
      " 'Malate dehydrogenase',\n",
      " 'Malic enzyme (NAD)',\n",
      " 'Malic enzyme (NADP)',\n",
      " 'NADH dehydrogenase (ubiquinone-8 & 3 protons)',\n",
      " 'NAD transhydrogenase',\n",
      " 'Ammonia reversible transport',\n",
      " 'O2 transport  diffusion ',\n",
      " 'Pyruvate dehydrogenase']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(list(scraping.model_contents.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'In': ['',\n",
      "        'from pprint import pprint\\n'\n",
      "        'pprint(locals())\\n'\n",
      "        'values = {\\n'\n",
      "        \"    'a': 120,\\n\"\n",
      "        \"    'b': 10\\n\"\n",
      "        '}\\n'\n",
      "        'locals().update(values)\\n'\n",
      "        'pprint(locals())\\n'\n",
      "        \"print(eval('a*b'))\"],\n",
      " 'Out': {},\n",
      " '_': '',\n",
      " '__': '',\n",
      " '___': '',\n",
      " '__builtin__': <module 'builtins' (built-in)>,\n",
      " '__builtins__': <module 'builtins' (built-in)>,\n",
      " '__doc__': 'Automatically created module for IPython interactive environment',\n",
      " '__loader__': None,\n",
      " '__name__': '__main__',\n",
      " '__package__': None,\n",
      " '__spec__': None,\n",
      " '_dh': ['C:\\\\Users\\\\Andrew Freiburger\\\\Dropbox\\\\My PC '\n",
      "         '(DESKTOP-M302P50)\\\\Documents\\\\UVic Civil '\n",
      "         'Engineering\\\\dFBA\\\\BiGG_SABIO\\\\examples'],\n",
      " '_i': '',\n",
      " '_i1': 'from pprint import pprint\\n'\n",
      "        'pprint(locals())\\n'\n",
      "        'values = {\\n'\n",
      "        \"    'a': 120,\\n\"\n",
      "        \"    'b': 10\\n\"\n",
      "        '}\\n'\n",
      "        'locals().update(values)\\n'\n",
      "        'pprint(locals())\\n'\n",
      "        \"print(eval('a*b'))\",\n",
      " '_ih': ['',\n",
      "         'from pprint import pprint\\n'\n",
      "         'pprint(locals())\\n'\n",
      "         'values = {\\n'\n",
      "         \"    'a': 120,\\n\"\n",
      "         \"    'b': 10\\n\"\n",
      "         '}\\n'\n",
      "         'locals().update(values)\\n'\n",
      "         'pprint(locals())\\n'\n",
      "         \"print(eval('a*b'))\"],\n",
      " '_ii': '',\n",
      " '_iii': '',\n",
      " '_oh': {},\n",
      " 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x0000026DA7C7F988>,\n",
      " 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x0000026DA7BCED08>>,\n",
      " 'pprint': <function pprint at 0x0000026DA5CE8B88>,\n",
      " 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x0000026DA7C7F988>}\n",
      "{'In': ['',\n",
      "        'from pprint import pprint\\n'\n",
      "        'pprint(locals())\\n'\n",
      "        'values = {\\n'\n",
      "        \"    'a': 120,\\n\"\n",
      "        \"    'b': 10\\n\"\n",
      "        '}\\n'\n",
      "        'locals().update(values)\\n'\n",
      "        'pprint(locals())\\n'\n",
      "        \"print(eval('a*b'))\"],\n",
      " 'Out': {},\n",
      " '_': '',\n",
      " '__': '',\n",
      " '___': '',\n",
      " '__builtin__': <module 'builtins' (built-in)>,\n",
      " '__builtins__': <module 'builtins' (built-in)>,\n",
      " '__doc__': 'Automatically created module for IPython interactive environment',\n",
      " '__loader__': None,\n",
      " '__name__': '__main__',\n",
      " '__package__': None,\n",
      " '__spec__': None,\n",
      " '_dh': ['C:\\\\Users\\\\Andrew Freiburger\\\\Dropbox\\\\My PC '\n",
      "         '(DESKTOP-M302P50)\\\\Documents\\\\UVic Civil '\n",
      "         'Engineering\\\\dFBA\\\\BiGG_SABIO\\\\examples'],\n",
      " '_i': '',\n",
      " '_i1': 'from pprint import pprint\\n'\n",
      "        'pprint(locals())\\n'\n",
      "        'values = {\\n'\n",
      "        \"    'a': 120,\\n\"\n",
      "        \"    'b': 10\\n\"\n",
      "        '}\\n'\n",
      "        'locals().update(values)\\n'\n",
      "        'pprint(locals())\\n'\n",
      "        \"print(eval('a*b'))\",\n",
      " '_ih': ['',\n",
      "         'from pprint import pprint\\n'\n",
      "         'pprint(locals())\\n'\n",
      "         'values = {\\n'\n",
      "         \"    'a': 120,\\n\"\n",
      "         \"    'b': 10\\n\"\n",
      "         '}\\n'\n",
      "         'locals().update(values)\\n'\n",
      "         'pprint(locals())\\n'\n",
      "         \"print(eval('a*b'))\"],\n",
      " '_ii': '',\n",
      " '_iii': '',\n",
      " '_oh': {},\n",
      " 'a': 120,\n",
      " 'b': 10,\n",
      " 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x0000026DA7C7F988>,\n",
      " 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x0000026DA7BCED08>>,\n",
      " 'pprint': <function pprint at 0x0000026DA5CE8B88>,\n",
      " 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x0000026DA7C7F988>,\n",
      " 'values': {'a': 120, 'b': 10}}\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(locals())\n",
    "values = {\n",
    "    'a': 120,\n",
    "    'b': 10\n",
    "}\n",
    "locals().update(values)\n",
    "pprint(locals())\n",
    "print(eval('a*b'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-f743a63a004c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-f743a63a004c>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    for x: str in range(10):\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for x: str in range(10):\n",
    "    print(x)\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r",
      "1\r",
      "2\r",
      "3\r",
      "4\r",
      "5\r",
      "6\r",
      "7\r",
      "8\r",
      "9\r"
     ]
    }
   ],
   "source": [
    "for x in range(10):\n",
    "    print(x, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = []\n",
    "products.append(6)\n",
    "products[-1] = 4\n",
    "print(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  4  5\n",
       "1  7  8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "df = pandas.DataFrame(data = [[4,5], [7,8]])\n",
    "\n",
    "display(df)\n",
    "\n",
    "print(df[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@authors: Ethan Chan, Matthew Freiburger\n",
    "\"\"\"\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import time\n",
    "import json\n",
    "from itertools import islice\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "class SABIO_scraping():\n",
    "#     __slots__ = (str(x) for x in [progress_file_prefix, xls_download_prefix, scraped_xls_prefix, scraped_entryids_prefix, sel_xls_download_path, processed_xls, entry_json, scraped_model, bigg_model_name_suffix, sub_directory_path, progress_file_path, xls_download_path, scraped_xls_file_path, scraped_entryids_file_path, xls_csv_file_path, entryids_json_file_path, scraped_model_json_file_path, bigg_model, step_number, cwd])\n",
    "    \n",
    "    def __init__(self,source='sabio'):\n",
    "\n",
    "        self.source = 'sabio'\n",
    "            \n",
    "        self.count = 0\n",
    "            \n",
    "        self.parameters = {}\n",
    "        self.parameters['general_delay'] = 2\n",
    "        self.variables = {}\n",
    "        self.paths = {}\n",
    "\n",
    "    #Clicks a HTML element with selenium by id\n",
    "    def click_element_id(self,n_id):\n",
    "        element = self.driver.find_element_by_id(n_id)\n",
    "        element.click()\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "        \n",
    "    def wait_for_id(self,n_id):\n",
    "        while True:\n",
    "            try:\n",
    "                element = self.driver.find_element_by_id(n_id)\n",
    "                break\n",
    "            except:\n",
    "                time.sleep(self.parameters['general_delay'])\n",
    "        \n",
    "\n",
    "    #Selects a choice from a HTML dropdown element with selenium by id\n",
    "    def select_dropdown_id(self,n_id, n_choice):\n",
    "        element = Select(self.driver.find_element_by_id(n_id))\n",
    "        element.select_by_visible_text(n_choice)\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "    def fatal_error_handler(self,message):\n",
    "        print(\"Error: \" + message)\n",
    "        print(\"Exiting now...\")\n",
    "        exit(0)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 0: GET BIGG MODEL TO SCRAPE AND SETUP DIRECTORIES AND PROGRESS FILE\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def start(self,): # find the BiGG model that will be scraped\n",
    "        while True:\n",
    "            bigg_model_path = input(\"Specify the BIGG Model JSON file path: \")\n",
    "\n",
    "            if os.path.exists(bigg_model_path) and bigg_model_path[-5:] == \".json\":\n",
    "                try:\n",
    "                    self.model = json.load(open(bigg_model_path))\n",
    "                    bigg_model_name = re.search(\"([\\w+\\.?\\s?]+)(?=\\.json)\", bigg_model_path).group()\n",
    "                    break\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "#         self.model = json.load(open('BiGG model of S. aureus.json'))\n",
    "#         bigg_model_path = 'BiGG model of S. aureus.json'\n",
    "#         bigg_model_name = re.search(\"([\\w+\\.?\\s?]+)(?=\\.json)\", bigg_model_path).group()\n",
    "\n",
    "        # define the paths\n",
    "        self.paths['cwd'] = os.path.dirname(os.path.realpath(bigg_model_path))\n",
    "        print(self.paths['cwd'])\n",
    "        self.paths['sub_directory_path'] = os.path.join(self.paths['cwd'],f\"scraping-{bigg_model_name}\")\n",
    "        if not os.path.isdir(self.paths['sub_directory_path']):        \n",
    "            os.mkdir(self.paths['sub_directory_path'])\n",
    "                    \n",
    "        self.variables['scraped_xls'] = {}\n",
    "        self.variables['scraped_entryids'] = {}\n",
    "        self.paths['scraped_model_json_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_model\") + \".json\"\n",
    "        self.paths['sel_xls_download_path'] = os.path.join(self.paths['sub_directory_path'],\"downloaded_xls\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.step_number = 1\n",
    "        \n",
    "        self.paths['progress_file_path'] = os.path.join(self.paths['sub_directory_path'], \"current_progress\") + '.txt'\n",
    "        if os.path.exists(self.paths['progress_file_path']):\n",
    "            f = open(self.paths['progress_file_path'], \"r\")\n",
    "            self.step_number = int(f.read(1))\n",
    "            if not re.search('[1-5]',str(self.step_number)):\n",
    "                self.fatal_error_handler(\"Progress file malformed. Please delete and restart\")\n",
    "        else:\n",
    "            self.step_number = 1\n",
    "            self.progress_update(self.step_number)\n",
    "\n",
    "        self.paths['xls_download_path'] = os.path.join(self.paths['sub_directory_path'], 'downloaded_xls') \n",
    "        if not os.path.isdir(self.paths['xls_download_path']):\n",
    "            os.mkdir(self.paths['xls_download_path'])\n",
    "\n",
    "        self.paths['scraped_xls_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_xls\") + \".json\"\n",
    "        if os.path.exists(self.paths['scraped_xls_file_path']):\n",
    "            f = open(self.paths['scraped_xls_file_path'], \"r\")\n",
    "            self.variables['scraped_xls'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "        self.paths['scraped_entryids_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_entryids\") + \".json\"\n",
    "        if os.path.exists(self.paths['scraped_entryids_file_path']):\n",
    "            f = open(self.paths['scraped_entryids_file_path'], \"r\")\n",
    "            self.variables['scraped_entryids'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "        self.paths['entryids_json_file_path'] = os.path.join(self.paths['sub_directory_path'], \"entryids_json\") + \".json\"\n",
    "        if os.path.exists(self.paths['entryids_json_file_path']):\n",
    "            f = open(self.paths['entryids_json_file_path'], \"r\")\n",
    "            self.variables['entry_id_json_out'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 1: SCRAPE SABIO WEBSITE BY DOWNLOAD XLS FOR GIVEN REACTIONS IN BIGG MODEL\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def scrape_xls(self,reaction_identifier, search_option):\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "        \n",
    "        self.wait_for_id(\"resetbtn\")\n",
    "        \n",
    "        self.click_element_id(\"resetbtn\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.click_element_id(\"option\")\n",
    "        self.select_dropdown_id(\"searchterms\", search_option)\n",
    "        text_area = self.driver.find_element_by_id(\"searchtermField\")\n",
    "        text_area.send_keys(reaction_identifier)  \n",
    "        time.sleep(self.parameters['general_delay'])  \n",
    "        self.click_element_id(\"addsearch\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        result_num = \"\"\n",
    "        try: \n",
    "            result_num_ele = self.driver.find_element_by_id(\"numberofKinLaw\")\n",
    "            for char in result_num_ele.text:\n",
    "                if re.search('[0-9]', char):\n",
    "                    result_num = result_num + char\n",
    "\n",
    "            result_num = int(result_num)\n",
    "        except:\n",
    "            #self.driver.close()\n",
    "            self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "            return False\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.select_dropdown_id(\"max\", \"100\")\n",
    "        element = Select(self.driver.find_element_by_id(\"max\"))\n",
    "        element.select_by_visible_text(\"100\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        if result_num > 0 and result_num <= 100:\n",
    "            self.click_element_id(\"allCheckbox\")\n",
    "            time.sleep(self.parameters['general_delay'])\n",
    "        elif result_num > 100:\n",
    "            self.click_element_id(\"allCheckbox\")\n",
    "            for i in range(int(result_num/100)):\n",
    "                element = self.driver.find_element_by_xpath(\"//*[@class = 'nextLink']\")\n",
    "                element.click()\n",
    "                time.sleep(self.parameters['general_delay'])\n",
    "                self.click_element_id(\"allCheckbox\")\n",
    "                time.sleep(self.parameters['general_delay'])\n",
    "        else:\n",
    "            #self.driver.close()\n",
    "            self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "            return False\n",
    "\n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/spreadsheetExport\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay']*7.5)\n",
    "\n",
    "        self.click_element_id(\"excelExport\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay']*2.5)\n",
    "\n",
    "        #self.driver.close()\n",
    "\n",
    "        return True\n",
    "    \n",
    "    def expand_shadow_element(self, element):\n",
    "        shadow_root = self.driver.execute_script('return arguments[0].shadowRoot', element)\n",
    "        return shadow_root\n",
    "\n",
    "\n",
    "    def scrape_bigg_xls(self,):\n",
    "        \"\"\"\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        prefs = {'download.default_directory' : self.paths['cwd'] + self.paths['sel_xls_download_path']}\n",
    "        chrome_options.add_experimental_option('prefs', prefs)\n",
    "        self.driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "        \n",
    "        self.driver.get(\"chrome://settings/security\")\n",
    "        \n",
    "\n",
    "        \n",
    "        root = self.driver.find_element_by_tag_name(\"settings-ui\")\n",
    "        shadow_root = self.expand_shadow_element(root)\n",
    "        \n",
    "        root1 = shadow_root.find_element_by_tag_name(\"settings-main\")\n",
    "        shadow_root1 = self.expand_shadow_element(root1)\n",
    "        \n",
    "        root2 = shadow_root1.find_element_by_tag_name(\"settings-basic-page\")\n",
    "        shadow_root2 = self.expand_shadow_element(root2)\n",
    "        \n",
    "        root3 = shadow_root2.find_element_by_tag_name(\"settings-privacy-page\")\n",
    "        shadow_root3 = self.expand_shadow_element(root3)\n",
    "        \n",
    "        root4 = shadow_root3.find_element_by_tag_name(\"settings-security-page\")\n",
    "        shadow_root4 = self.expand_shadow_element(root4)\n",
    "        \n",
    "        root5 = shadow_root4.find_element_by_css_selector(\"#safeBrowsingStandard\")\n",
    "        shadow_root5 = self.expand_shadow_element(root5)\n",
    "        \n",
    "        security_button = shadow_root5.find_element_by_css_selector(\"div.disc-border\")\n",
    "        \n",
    "        security_button.click()\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        fp = webdriver.FirefoxProfile(\"l2pnahxq.scraper\")\n",
    "        fp.set_preference(\"browser.download.folderList\",2)\n",
    "        fp.set_preference(\"browser.download.dir\", self.paths['cwd'] + self.paths['sel_xls_download_path'])\n",
    "        \"\"\"\n",
    "        #print(self.paths['cwd'] + self.paths['sel_xls_download_path'])\n",
    "        fp = webdriver.FirefoxProfile(\"l2pnahxq.scraper\")\n",
    "        fp.set_preference(\"browser.download.folderList\", 2)\n",
    "        fp.set_preference(\"browser.download.manager.showWhenStarting\", False)\n",
    "        #temp_path = f'⁦C:\\\\Users\\\\Ethan\\\\Documents\\\\Python\\\\Polar Bar Chart\\\\sabioawesome\\\\scraping-test_file\\\\downloaded_xls⁩'\n",
    "        #fp.set_preference(\"browser.download.dir\", self.paths[\"cwd\"] + self.paths[\"sel_xls_download_path\"])\n",
    "        print( self.paths[\"sel_xls_download_path\"])\n",
    "        fp.set_preference(\"browser.download.dir\", self.paths[\"sel_xls_download_path\"])\n",
    "        fp.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"application/octet-stream\")\n",
    "        self.driver = webdriver.Firefox(firefox_profile=fp, executable_path=\"geckodriver.exe\") \n",
    "        \n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "\n",
    "        \n",
    "        for reaction in self.model[\"reactions\"]:\n",
    "            if not reaction[\"name\"] in self.variables['scraped_xls']:\n",
    "                ids_to_try = reaction[\"annotation\"]\n",
    "\n",
    "                success_flag = False\n",
    "                annotation_search_pairs = {\"sabiork\":\"SabioReactionID\", \"metanetx.reaction\":\"MetaNetXReactionID\", \"ec-code\":\"ECNumber\", \"kegg.reaction\":\"KeggReactionID\", \"rhea\":\"RheaReactionID\"}\n",
    "                for annotation in annotation_search_pairs:\n",
    "                    if not success_flag:\n",
    "                        if annotation in ids_to_try:\n",
    "\n",
    "\n",
    "                            for id_to_try in ids_to_try[annotation]:\n",
    "#                                 success_flag = self.scrape_xls(id_to_try, annotation_search_pairs[annotation])\n",
    "                                try:\n",
    "                                    success_flag = self.scrape_xls(id_to_try, annotation_search_pairs[annotation])\n",
    "                                except:\n",
    "                                    success_flag = False\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                if not success_flag:\n",
    "                    try:\n",
    "                        success_flag = self.scrape_xls(reaction[\"name\"], \"Enzymename\")\n",
    "                    except:\n",
    "                        success_flag = False\n",
    "\n",
    "                json_dict_key = reaction[\"name\"].replace(\"\\\"\", \"\")\n",
    "                if success_flag:\n",
    "                    self.variables['scraped_xls'][json_dict_key] = \"yes\"\n",
    "                else:\n",
    "                    self.variables['scraped_xls'][json_dict_key] = \"no\"\n",
    "                    \n",
    "                self.count += 1\n",
    "                print(\"Reactions searched: \" + str(self.count) + \"/\" + str(len(self.model[\"reactions\"])), end='\\r')\n",
    "                    \n",
    "                    \n",
    "\n",
    "            with open(self.paths['scraped_xls_file_path'], 'w') as outfile:\n",
    "                json.dump(self.variables['scraped_xls'], outfile, indent = 4)   \n",
    "                outfile.close()\n",
    "                \n",
    "        self.step_number = 2\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 2: GLOB EXPORTED XLS FILES TOGETHER\n",
    "    --------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    def glob_xls_files(self,):\n",
    "#         scraped_sans_parentheses_enzymes = glob('./{}/*.xls'.format(self.paths['xls_download_path']))\n",
    "        scraped_sans_parentheses_enzymes = glob(os.path.join(self.paths['xls_download_path'], '*.xls'))\n",
    "        total_dataframes = []\n",
    "        for file in scraped_sans_parentheses_enzymes:\n",
    "            #file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "            dfn = pd.read_excel(file)\n",
    "            total_dataframes.append(dfn)\n",
    "\n",
    "        # combine the total set of dataframes\n",
    "        combined_df = pd.DataFrame()\n",
    "        combined_df = pd.concat(total_dataframes)\n",
    "        combined_df = combined_df.fillna(' ')\n",
    "        combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "        # export the dataframe\n",
    "        csv_path = os.path.join(self.paths['sub_directory_path'], \"proccessed-xls\") + \".csv\"\n",
    "        combined_df.to_csv(csv_path)\n",
    "        \n",
    "        # update the \n",
    "        self.step_number = 3\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 3: SCRAPE ADDITIONAL DATA BY ENTRYID\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def scrape_entry_id(self,entry_id):\n",
    "        global entry_id_json_out\n",
    "\n",
    "        entry_id = str(entry_id)\n",
    "\n",
    "        fp = webdriver.FirefoxProfile(\"l2pnahxq.scraper\")\n",
    "        fp.set_preference(\"browser.download.folderList\",2)\n",
    "        fp.set_preference(\"browser.download.dir\", self.paths['cwd'] + self.paths['sel_xls_download_path'])\n",
    "        self.driver = webdriver.Firefox(firefox_profile=fp, executable_path=\"geckodriver.exe\") \n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.click_element_id(\"option\")\n",
    "        self.select_dropdown_id(\"searchterms\", \"EntryID\")\n",
    "        text_area = self.driver.find_element_by_id(\"searchtermField\")\n",
    "        text_area.send_keys(entry_id)\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.click_element_id(\"addsearch\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.click_element_id(entry_id + \"img\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.driver.switch_to.frame(self.driver.find_element_by_xpath(\"//iframe[@name='iframe_\" + entry_id + \"']\"))\n",
    "        element = self.driver.find_element_by_xpath(\"//table\")\n",
    "        html_source = element.get_attribute('innerHTML')\n",
    "        \n",
    "        table_df = pd.read_html(html_source)\n",
    "        reaction_parameters_df = pd.DataFrame()\n",
    "        counter = 0\n",
    "        parameters_json = {}\n",
    "        for df in table_df:\n",
    "            try:\n",
    "                if df[0][0] == \"Parameter\":\n",
    "                    reaction_parameters_df = table_df[counter]\n",
    "            except:\n",
    "                self.driver.close()\n",
    "                return parameters_json\n",
    "            counter += 1\n",
    "            \n",
    "        parameter_name = \"\"\n",
    "        for i in range(len(reaction_parameters_df[0])-2):\n",
    "            parameter_name = reaction_parameters_df[0][i+2]\n",
    "            inner_parameters_json = {}\n",
    "            for j in range(len(reaction_parameters_df)-3):\n",
    "                inner_parameters_json[reaction_parameters_df[j+1][1]] = reaction_parameters_df[j+1][i+2]\n",
    "\n",
    "            parameters_json[parameter_name] = inner_parameters_json\n",
    "\n",
    "        self.driver.close()\n",
    "\n",
    "        return parameters_json\n",
    "\n",
    "\n",
    "    def scrape_entryids(self,):\n",
    "        global entry_id_json_out\n",
    "\n",
    "        sabio_xls_df = pd.read_csv(self.paths['xls_csv_file_path'])\n",
    "        entryids = sabio_xls_df[\"EntryID\"].unique().tolist()\n",
    "\n",
    "        for entryid in entryids:\n",
    "            if not entryid in self.variables['scraped_entryids']:\n",
    "                try:\n",
    "                    entry_id_json_out[str(entryid)] = self.scrape_entry_id(entryid)\n",
    "                    self.variables['scraped_entryids'][entryid] = \"yes\"\n",
    "                except:\n",
    "                    self.variables['scraped_entryids'][entryid] = \"no\"\n",
    "            with open(self.paths[\"(scraped_entryids_file_path\"], 'w') as outfile:\n",
    "                json.dump(self.variables['scraped_entryids'], outfile, indent = 4)   \n",
    "                outfile.close()\n",
    "            with open(self.paths[\"entryids_json_file_path\"], 'w') as f:\n",
    "                json.dump(entry_id_json_out, f, indent = 4)        \n",
    "                f.close()\n",
    "        \n",
    "        self.step_number = 4\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 4: COMBINE ENZYME AND ENTRYID DATA INTO JSON FILE\n",
    "    --------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    def combine_data(self,):\n",
    "\n",
    "        sabio_xls_df = pd.read_csv(self.paths['xls_csv_file_path'])\n",
    "\n",
    "        # Opening JSON file\n",
    "        with open(self.paths['entryids_json_file_path']) as json_file:\n",
    "            entry_id_data = json.load(json_file)\n",
    "\n",
    "        enzymenames = sabio_xls_df[\"Enzymename\"].unique().tolist()\n",
    "        enzyme_dict = {}\n",
    "        missing_entry_ids = []\n",
    "        parameters = {}\n",
    "\n",
    "        for enzyme in enzymenames:\n",
    "            sabio_grouped_enzyme_df = sabio_xls_df.loc[sabio_xls_df[\"Enzymename\"] == enzyme]\n",
    "            dict_to_append = {}\n",
    "            reactions = sabio_grouped_enzyme_df[\"Reaction\"].unique().tolist()\n",
    "            for reaction in reactions:\n",
    "                dict_reactions_to_append = {}\n",
    "                sabio_grouped_reactions_df = sabio_grouped_enzyme_df.loc[sabio_grouped_enzyme_df[\"Reaction\"] == reaction]\n",
    "                entryids = sabio_grouped_reactions_df[\"EntryID\"].unique().tolist()\n",
    "\n",
    "                for entryid in entryids:\n",
    "                    entry_ids_df = sabio_grouped_reactions_df.loc[sabio_grouped_reactions_df[\"EntryID\"] == entryid]\n",
    "                    dict_entryid_to_append = {}\n",
    "                    head_of_df = entry_ids_df.head(1).squeeze()\n",
    "                    entry_id_flag = True\n",
    "                    parameter_info = {}\n",
    "\n",
    "                    try:\n",
    "                        parameter_info = entry_id_data[str(entryid)]\n",
    "                        dict_entryid_to_append[\"Parameters\"] = parameter_info\n",
    "                    except:\n",
    "                        missing_entry_ids.append(str(entryid))\n",
    "                        entry_id_flag = False\n",
    "                        dict_entryid_to_append[\"Parameters\"] = \"NaN\"\n",
    "\n",
    "                    rate_law = head_of_df[\"Rate Equation\"]\n",
    "                    bad_rate_laws = [\"unknown\", \"\", \"-\"]\n",
    "\n",
    "                    if not rate_law in bad_rate_laws:                    \n",
    "                        dict_entryid_to_append[\"RateLaw\"] = rate_law\n",
    "                        dict_entryid_to_append[\"SubstitutedRateLaw\"] = rate_law\n",
    "                    else:\n",
    "                        dict_entryid_to_append[\"RateLaw\"] = \"NaN\"\n",
    "                        dict_entryid_to_append[\"SubstitutedRateLaw\"] = \"NaN\"\n",
    "\n",
    "                    if entry_id_flag:\n",
    "\n",
    "                        fields_to_copy = [\"Buffer\", \"Product\", \"PubMedID\", \"Publication\", \"pH\", \"Temperature\", \"Enzyme Variant\", \"UniProtKB_AC\", \"Organism\", \"KineticMechanismType\", \"SabioReactionID\"]\n",
    "                        for field in fields_to_copy:  \n",
    "                            dict_entryid_to_append[field] = head_of_df[field]\n",
    "                        dict_reactions_to_append[entryid] = dict_entryid_to_append\n",
    "                        dict_entryid_to_append[\"Substrates\"] = head_of_df[\"Substrate\"].split(\";\")\n",
    "                        out_rate_law = rate_law\n",
    "                        if not rate_law in bad_rate_laws:                    \n",
    "                            substrates = head_of_df[\"Substrate\"].split(\";\")\n",
    "\n",
    "                            stripped_string = re.sub('[0-9]', '', rate_law)\n",
    "\n",
    "                            variables = re.split(\"\\^|\\*|\\+|\\-|\\/|\\(|\\)| \", stripped_string)\n",
    "                            variables = ' '.join(variables).split()\n",
    "\n",
    "                            start_value_permutations = [\"start value\", \"start val.\"]\n",
    "                            substrates_key = {}\n",
    "                            for var in variables:\n",
    "                                if var in parameter_info:\n",
    "                                    for permutation in start_value_permutations:\n",
    "                                        try:\n",
    "                                            if var == \"A\" or var == \"B\":\n",
    "                                                substrates_key[var] = parameter_info[var][\"species\"]\n",
    "                                            else:\n",
    "                                                value = parameter_info[var][permutation]\n",
    "                                                if value != \"-\" and value != \"\" and value != \" \":\n",
    "                                                    out_rate_law = out_rate_law.replace(var, parameter_info[var][permutation])\n",
    "                                        except:\n",
    "                                            pass\n",
    "\n",
    "                            dict_entryid_to_append[\"RateLawSubstrates\"] = substrates_key\n",
    "                            dict_entryid_to_append[\"SubstitutedRateLaw\"] = out_rate_law\n",
    "\n",
    "                dict_to_append[reaction] = dict_reactions_to_append\n",
    "\n",
    "            enzyme_dict[enzyme] = dict_to_append\n",
    "\n",
    "        with open(self.paths[\"scraped_model_json_file_path\"], 'w') as f:\n",
    "            json.dump(enzyme_dict, f, indent=4)\n",
    "        \n",
    "        self.step_number = 5\n",
    "        self.progress_update(self.step_number)\n",
    "        \n",
    "    def progress_update(self, step):\n",
    "        if not re.search('[0-5]', str(step)):\n",
    "            print(f'--> ERROR: The {step} step is not acceptable.')\n",
    "        f = open(self.paths['progress_file_path'], \"w\")\n",
    "        f.write(str(step))\n",
    "        f.close()\n",
    "\n",
    "    def main(self,):\n",
    "        self.start()\n",
    "\n",
    "        while True:\n",
    "            if self.step_number == 1:\n",
    "                self.scrape_bigg_xls()\n",
    "            elif self.step_number == 2:\n",
    "                self.glob_xls_files()\n",
    "            elif self.step_number == 3:\n",
    "                self.scrape_entryids()\n",
    "            elif self.step_number == 4:\n",
    "                self.combine_data()\n",
    "            elif self.step_number == 5:\n",
    "                print(\"Execution complete. Scraper finished.\")\n",
    "                break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./BiGG_models/BiGG model of S. aureus.json\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@authors: Ethan Chan, Matthew Freiburger\n",
    "\"\"\"\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import time\n",
    "import json\n",
    "from itertools import islice\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "class SABIO_scraping():\n",
    "#     __slots__ = (str(x) for x in [progress_file_prefix, xls_download_prefix, scraped_xls_prefix, scraped_entryids_prefix, sel_xls_download_path, processed_xls, entry_json, scraped_model, bigg_model_name_suffix, sub_directory_path, progress_file_path, xls_download_path, scraped_xls_file_path, scraped_entryids_file_path, xls_csv_file_path, entryids_json_file_path, scraped_model_json_file_path, bigg_model, step_number, cwd])\n",
    "    \n",
    "    def __init__(self):           \n",
    "        self.count = 0\n",
    "            \n",
    "        self.parameters = {}\n",
    "        self.parameters['general_delay'] = 2\n",
    "        self.variables = {}\n",
    "        self.paths = {}\n",
    "\n",
    "    #Clicks a HTML element with selenium by id\n",
    "    def click_element_id(self,n_id):\n",
    "        element = self.driver.find_element_by_id(n_id)\n",
    "        element.click()\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "    #Selects a choice from a HTML dropdown element with selenium by id\n",
    "    def select_dropdown_id(self,n_id, n_choice):\n",
    "        element = Select(self.driver.find_element_by_id(n_id))\n",
    "        element.select_by_visible_text(n_choice)\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "    def fatal_error_handler(self,message):\n",
    "        print(\"Error: \" + message)\n",
    "        print(\"Exiting now...\")\n",
    "        exit(0)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 0: GET BIGG MODEL TO SCRAPE AND SETUP DIRECTORIES AND PROGRESS FILE\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def start(self,source='sabio',): # find the BiGG model that will be scraped\n",
    "        while True:\n",
    "            bigg_model_path = input(\"Specify the BIGG Model JSON file path: \")\n",
    "\n",
    "            if os.path.exists(bigg_model_path) and bigg_model_path[-5:] == \".json\":\n",
    "                try:\n",
    "                    self.model = json.load(open(bigg_model_path))\n",
    "                    bigg_model_name = re.search(\"([\\w+\\.?\\s?]+)(?=\\.json)\", bigg_model_path).group()\n",
    "                    break\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "#         self.model = json.load(open('BiGG model of S. aureus.json'))\n",
    "#         bigg_model_path = 'BiGG model of S. aureus.json'\n",
    "#         bigg_model_name = re.search(\"([\\w+\\.?\\s?]+)(?=\\.json)\", bigg_model_path).group()\n",
    "\n",
    "        # define the paths\n",
    "        self.paths['cwd'] = os.path.dirname(os.path.realpath(bigg_model_path))\n",
    "        print(self.paths['cwd'])\n",
    "        self.paths['sub_directory_path'] = os.path.join(self.paths['cwd'],f\"scraping-{bigg_model_name}\")\n",
    "        if not os.path.isdir(self.paths['sub_directory_path']):        \n",
    "            os.mkdir(self.paths['sub_directory_path'])\n",
    "                    \n",
    "        self.variables['scraped_xls'] = {}\n",
    "        self.variables['scraped_entryids'] = {}\n",
    "        self.paths['scraped_model_json_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_model\") + \".json\"\n",
    "        self.paths['sel_xls_download_path'] = os.path.join(self.paths['sub_directory_path'],\"xls_download\")\n",
    "        \n",
    "        self.step_number = 1\n",
    "        \n",
    "        self.paths['progress_file_path'] = os.path.join(self.paths['sub_directory_path'], \"current_progress\") + '.txt'\n",
    "        if os.path.exists(self.paths['progress_file_path']):\n",
    "            f = open(self.paths['progress_file_path'], \"r\")\n",
    "            self.step_number = int(f.read(1))\n",
    "            if not re.search('[1-5]',str(self.step_number)):\n",
    "                self.fatal_error_handler(\"Progress file malformed. Please delete and restart\")\n",
    "        else:\n",
    "            self.step_number = 1\n",
    "            self.progress_update(self.step_number)\n",
    "\n",
    "        self.paths['xls_download_path'] = os.path.join(self.paths['sub_directory_path'], 'downloaded_xls') \n",
    "        if not os.path.isdir(self.paths['xls_download_path']):\n",
    "            os.mkdir(self.paths['xls_download_path'])\n",
    "\n",
    "        self.paths['scraped_xls_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_xls\") + \".json\"\n",
    "        if os.path.exists(self.paths['scraped_xls_file_path']):\n",
    "            f = open(self.paths['scraped_xls_file_path'], \"r\")\n",
    "            self.variables['scraped_xls'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "        self.paths['scraped_entryids_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_entryids\") + \".json\"\n",
    "        if os.path.exists(self.paths['scraped_entryids_file_path']):\n",
    "            f = open(self.paths['scraped_entryids_file_path'], \"r\")\n",
    "            self.variables['scraped_entryids'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "        self.paths['entryids_json_file_path'] = os.path.join(self.paths['sub_directory_path'], \"entryids_json\") + \".json\"\n",
    "        if os.path.exists(self.paths['entryids_json_file_path']):\n",
    "            f = open(self.paths['entryids_json_file_path'], \"r\")\n",
    "            self.variables['entry_id_json_out'] = json.load(f)\n",
    "            f.close()\n",
    "            \n",
    "        fp = webdriver.FirefoxProfile(\"l2pnahxq.scraper\")\n",
    "        fp.set_preference(\"browser.download.folderList\",2)\n",
    "        self.driver = webdriver.Firefox(firefox_profile=fp, executable_path=\"geckodriver.exe\") \n",
    "        \n",
    "        if source == 'sabio':\n",
    "            self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "            self.source = 'sabio'\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 1: SCRAPE SABIO WEBSITE BY DOWNLOAD XLS FOR GIVEN REACTIONS IN BIGG MODEL\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def scrape_xls(self,reaction_identifier, search_option):\n",
    "        self.count += 1\n",
    "        print(self.count, end='\\r')\n",
    "        \n",
    "        \n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "        self.click_element_id(\"resetbtn\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.click_element_id(\"option\")\n",
    "        self.select_dropdown_id(\"searchterms\", search_option)\n",
    "        text_area = self.driver.find_element_by_id(\"searchtermField\")\n",
    "        text_area.send_keys(reaction_identifier)  \n",
    "        time.sleep(self.parameters['general_delay'])  \n",
    "        self.click_element_id(\"addsearch\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        result_num = \"\"\n",
    "        try: \n",
    "            result_num_ele = self.driver.find_element_by_id(\"numberofKinLaw\")\n",
    "            for char in result_num_ele.text:\n",
    "                if re.search('[0-9]', char):\n",
    "                    result_num = result_num + char\n",
    "\n",
    "            result_num = int(result_num)\n",
    "        except:\n",
    "            #self.driver.close()\n",
    "            self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "            return False\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.select_dropdown_id(\"max\", \"100\")\n",
    "        element = Select(self.driver.find_element_by_id(\"max\"))\n",
    "        element.select_by_visible_text(\"100\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        if result_num > 0 and result_num <= 100:\n",
    "            self.click_element_id(\"allCheckbox\")\n",
    "            time.sleep(self.parameters['general_delay'])\n",
    "        elif result_num > 100:\n",
    "            self.click_element_id(\"allCheckbox\")\n",
    "            for i in range(int(result_num/100)):\n",
    "                element = self.driver.find_element_by_xpath(\"//*[@class = 'nextLink']\")\n",
    "                element.click()\n",
    "                time.sleep(self.parameters['general_delay'])\n",
    "                self.click_element_id(\"allCheckbox\")\n",
    "                time.sleep(self.parameters['general_delay'])\n",
    "        else:\n",
    "            #self.driver.close()\n",
    "            self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "            return False\n",
    "\n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/spreadsheetExport\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay']*7.5)\n",
    "\n",
    "        self.click_element_id(\"excelExport\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay']*2.5)\n",
    "\n",
    "        #self.driver.close()\n",
    "\n",
    "        return True\n",
    "    \n",
    "    def expand_shadow_element(self, element):\n",
    "        shadow_root = self.driver.execute_script('return arguments[0].shadowRoot', element)\n",
    "        return shadow_root\n",
    "\n",
    "\n",
    "    def scrape_bigg_xls(self,):\n",
    "        \"\"\"\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        prefs = {'download.default_directory' : self.paths['cwd'] + self.paths['sel_xls_download_path']}\n",
    "        chrome_options.add_experimental_option('prefs', prefs)\n",
    "        self.driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "        \n",
    "        self.driver.get(\"chrome://settings/security\")\n",
    "        \n",
    "\n",
    "        \n",
    "        root = self.driver.find_element_by_tag_name(\"settings-ui\")\n",
    "        shadow_root = self.expand_shadow_element(root)\n",
    "        \n",
    "        root1 = shadow_root.find_element_by_tag_name(\"settings-main\")\n",
    "        shadow_root1 = self.expand_shadow_element(root1)\n",
    "        \n",
    "        root2 = shadow_root1.find_element_by_tag_name(\"settings-basic-page\")\n",
    "        shadow_root2 = self.expand_shadow_element(root2)\n",
    "        \n",
    "        root3 = shadow_root2.find_element_by_tag_name(\"settings-privacy-page\")\n",
    "        shadow_root3 = self.expand_shadow_element(root3)\n",
    "        \n",
    "        root4 = shadow_root3.find_element_by_tag_name(\"settings-security-page\")\n",
    "        shadow_root4 = self.expand_shadow_element(root4)\n",
    "        \n",
    "        root5 = shadow_root4.find_element_by_css_selector(\"#safeBrowsingStandard\")\n",
    "        shadow_root5 = self.expand_shadow_element(root5)\n",
    "        \n",
    "        security_button = shadow_root5.find_element_by_css_selector(\"div.disc-border\")\n",
    "        \n",
    "        security_button.click()\n",
    "        \"\"\"\n",
    "        \n",
    "        fp = webdriver.FirefoxProfile(\"l2pnahxq.scraper\")\n",
    "        fp.set_preference(\"browser.download.folderList\",2)\n",
    "        fp.set_preference(\"browser.download.dir\", self.paths['cwd'] + self.paths['sel_xls_download_path'])\n",
    "        driver = webdriver.Firefox(firefox_profile=fp, executable_path=\"geckodriver.exe\") \n",
    "        \n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "\n",
    "        \n",
    "        for reaction in self.model[\"reactions\"]:\n",
    "            if not reaction[\"name\"] in self.variables['scraped_xls']:\n",
    "                ids_to_try = reaction[\"annotation\"]\n",
    "\n",
    "                success_flag = False\n",
    "                annotation_search_pairs = {\"sabiork\":\"SabioReactionID\", \"metanetx.reaction\":\"MetaNetXReactionID\", \"ec-code\":\"ECNumber\", \"kegg.reaction\":\"KeggReactionID\", \"rhea\":\"RheaReactionID\"}\n",
    "                for annotation in annotation_search_pairs:\n",
    "                    if not success_flag:\n",
    "                        if annotation in ids_to_try:\n",
    "\n",
    "\n",
    "                            for id_to_try in ids_to_try[annotation]:\n",
    "#                                 success_flag = self.scrape_xls(id_to_try, annotation_search_pairs[annotation])\n",
    "                                try:\n",
    "                                    success_flag = self.scrape_xls(id_to_try, annotation_search_pairs[annotation])\n",
    "                                except:\n",
    "                                    success_flag = False\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                if not success_flag:\n",
    "                    try:\n",
    "                        success_flag = self.scrape_xls(reaction[\"name\"], \"Enzymename\")\n",
    "                    except:\n",
    "                        success_flag = False\n",
    "\n",
    "                json_dict_key = reaction[\"name\"].replace(\"\\\"\", \"\")\n",
    "                if success_flag:\n",
    "                    self.variables['scraped_xls'][json_dict_key] = \"yes\"\n",
    "                else:\n",
    "                    self.variables['scraped_xls'][json_dict_key] = \"no\"\n",
    "                    \n",
    "                    \n",
    "\n",
    "            with open(self.paths['scraped_xls_file_path'], 'w') as outfile:\n",
    "                json.dump(self.variables['scraped_xls'], outfile, indent = 4)   \n",
    "                outfile.close()\n",
    "                \n",
    "        self.step_number = 2\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 2: GLOB EXPORTED XLS FILES TOGETHER\n",
    "    --------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    def glob_xls_files(self,):\n",
    "        scraped_sans_parentheses_enzymes = glob.glob('./{}/*.xls'.format(self.paths['xls_download_path']))\n",
    "        total_dataframes = []\n",
    "        for file in scraped_sans_parentheses_enzymes:\n",
    "            #file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "            dfn = pd.read_excel(file)\n",
    "            total_dataframes.append(dfn)\n",
    "\n",
    "        # combine the total set of dataframes\n",
    "        combined_df = pd.DataFrame()\n",
    "        combined_df = pd.concat(total_dataframes)\n",
    "        combined_df = combined_df.fillna(' ')\n",
    "        combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "        # export the dataframe\n",
    "        csv_path = os.path.join(self.paths['sub_directory_path'], \"proccessed-xls\") + \".csv\"\n",
    "        combined_df.to_csv(csv_path)\n",
    "        \n",
    "        # update the \n",
    "        self.step_number = 3\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 3: SCRAPE ADDITIONAL DATA BY ENTRYID\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def scrape_entry_id(self,entry_id):\n",
    "        global entry_id_json_out\n",
    "\n",
    "        entry_id = str(entry_id)\n",
    "\n",
    "        self.driver = webdriver.Chrome(executable_path=r\".\\chromedriver.exe\")\n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(\"option\")\n",
    "        self.select_dropdown_id(\"searchterms\", \"EntryID\")\n",
    "        text_area = self.driver.find_element_by_id(\"searchtermField\")\n",
    "        text_area.send_keys(entry_id)\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(\"addsearch\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(entry_id + \"img\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.driver.switch_to.frame(self.driver.find_element_by_xpath(\"//iframe[@name='iframe_\" + entry_id + \"']\"))\n",
    "        element = self.driver.find_element_by_xpath(\"//table\")\n",
    "        html_source = element.get_attribute('innerHTML')\n",
    "        \n",
    "        table_df = pd.read_html(html_source)\n",
    "        reaction_parameters_df = pd.DataFrame()\n",
    "        counter = 0\n",
    "        parameters_json = {}\n",
    "        for df in table_df:\n",
    "            try:\n",
    "                if df[0][0] == \"Parameter\":\n",
    "                    reaction_parameters_df = table_df[counter]\n",
    "            except:\n",
    "                self.driver.close()\n",
    "                return parameters_json\n",
    "            counter += 1\n",
    "            \n",
    "        parameter_name = \"\"\n",
    "        for i in range(len(reaction_parameters_df[0])-2):\n",
    "            parameter_name = reaction_parameters_df[0][i+2]\n",
    "            inner_parameters_json = {}\n",
    "            for j in range(len(reaction_parameters_df)-3):\n",
    "                inner_parameters_json[reaction_parameters_df[j+1][1]] = reaction_parameters_df[j+1][i+2]\n",
    "\n",
    "            parameters_json[parameter_name] = inner_parameters_json\n",
    "\n",
    "        self.driver.close()\n",
    "\n",
    "        return parameters_json\n",
    "\n",
    "\n",
    "    def scrape_entryids(self,):\n",
    "        global entry_id_json_out\n",
    "\n",
    "        sabio_xls_df = pd.read_csv(xls_csv_file_path)\n",
    "        entryids = sabio_xls_df[\"EntryID\"].unique().tolist()\n",
    "\n",
    "        for entryid in entryids:\n",
    "            if not entryid in scraped_entryids:\n",
    "                try:\n",
    "                    entry_id_json_out[str(entryid)] = scrape_entry_id(entryid)\n",
    "                    self.variables['scraped_entryids'][entryid] = \"yes\"\n",
    "                except:\n",
    "                    self.variables['scraped_entryids'][entryid] = \"no\"\n",
    "            with open(scraped_entryids_file_path, 'w') as outfile:\n",
    "                json.dump(self.variables['scraped_entryids'], outfile, indent = 4)   \n",
    "                outfile.close()\n",
    "            with open(entryids_json_file_path, 'w') as f:\n",
    "                json.dump(entry_id_json_out, f, indent = 4)        \n",
    "                f.close()\n",
    "        \n",
    "        self.step_number = 4\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 4: COMBINE ENZYME AND ENTRYID DATA INTO JSON FILE\n",
    "    --------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    def combine_data(self,):\n",
    "\n",
    "        sabio_xls_df = pd.read_csv(self.paths['xls_csv_file_path'])\n",
    "\n",
    "        # Opening JSON file\n",
    "        with open(self.paths['entryids_json_file_path']) as json_file:\n",
    "            entry_id_data = json.load(json_file)\n",
    "\n",
    "        enzymenames = sabio_xls_df[\"Enzymename\"].unique().tolist()\n",
    "        enzyme_dict = {}\n",
    "        missing_entry_ids = []\n",
    "        parameters = {}\n",
    "\n",
    "        for enzyme in enzymenames:\n",
    "            sabio_grouped_enzyme_df = sabio_xls_df.loc[sabio_xls_df[\"Enzymename\"] == enzyme]\n",
    "            dict_to_append = {}\n",
    "            reactions = sabio_grouped_enzyme_df[\"Reaction\"].unique().tolist()\n",
    "            for reaction in reactions:\n",
    "                dict_reactions_to_append = {}\n",
    "                sabio_grouped_reactions_df = sabio_grouped_enzyme_df.loc[sabio_grouped_enzyme_df[\"Reaction\"] == reaction]\n",
    "                entryids = sabio_grouped_reactions_df[\"EntryID\"].unique().tolist()\n",
    "\n",
    "                for entryid in entryids:\n",
    "                    entry_ids_df = sabio_grouped_reactions_df.loc[sabio_grouped_reactions_df[\"EntryID\"] == entryid]\n",
    "                    dict_entryid_to_append = {}\n",
    "                    head_of_df = entry_ids_df.head(1).squeeze()\n",
    "                    entry_id_flag = True\n",
    "                    parameter_info = {}\n",
    "\n",
    "                    try:\n",
    "                        parameter_info = entry_id_data[str(entryid)]\n",
    "                        dict_entryid_to_append[\"Parameters\"] = parameter_info\n",
    "                    except:\n",
    "                        missing_entry_ids.append(str(entryid))\n",
    "                        entry_id_flag = False\n",
    "                        dict_entryid_to_append[\"Parameters\"] = \"NaN\"\n",
    "\n",
    "                    rate_law = head_of_df[\"Rate Equation\"]\n",
    "                    bad_rate_laws = [\"unknown\", \"\", \"-\"]\n",
    "\n",
    "                    if not rate_law in bad_rate_laws:                    \n",
    "                        dict_entryid_to_append[\"RateLaw\"] = rate_law\n",
    "                        dict_entryid_to_append[\"SubstitutedRateLaw\"] = rate_law\n",
    "                    else:\n",
    "                        dict_entryid_to_append[\"RateLaw\"] = \"NaN\"\n",
    "                        dict_entryid_to_append[\"SubstitutedRateLaw\"] = \"NaN\"\n",
    "\n",
    "                    if entry_id_flag:\n",
    "\n",
    "                        fields_to_copy = [\"Buffer\", \"Product\", \"PubMedID\", \"Publication\", \"pH\", \"Temperature\", \"Enzyme Variant\", \"UniProtKB_AC\", \"Organism\", \"KineticMechanismType\", \"SabioReactionID\"]\n",
    "                        for field in fields_to_copy:  \n",
    "                            dict_entryid_to_append[field] = head_of_df[field]\n",
    "                        dict_reactions_to_append[entryid] = dict_entryid_to_append\n",
    "                        dict_entryid_to_append[\"Substrates\"] = head_of_df[\"Substrate\"].split(\";\")\n",
    "                        out_rate_law = rate_law\n",
    "                        if not rate_law in bad_rate_laws:                    \n",
    "                            substrates = head_of_df[\"Substrate\"].split(\";\")\n",
    "\n",
    "                            stripped_string = re.sub('[0-9]', '', rate_law)\n",
    "\n",
    "                            variables = re.split(\"\\^|\\*|\\+|\\-|\\/|\\(|\\)| \", stripped_string)\n",
    "                            variables = ' '.join(variables).split()\n",
    "\n",
    "                            start_value_permutations = [\"start value\", \"start val.\"]\n",
    "                            substrates_key = {}\n",
    "                            for var in variables:\n",
    "                                if var in parameter_info:\n",
    "                                    for permutation in start_value_permutations:\n",
    "                                        try:\n",
    "                                            if var == \"A\" or var == \"B\":\n",
    "                                                substrates_key[var] = parameter_info[var][\"species\"]\n",
    "                                            else:\n",
    "                                                value = parameter_info[var][permutation]\n",
    "                                                if value != \"-\" and value != \"\" and value != \" \":\n",
    "                                                    out_rate_law = out_rate_law.replace(var, parameter_info[var][permutation])\n",
    "                                        except:\n",
    "                                            pass\n",
    "\n",
    "                            dict_entryid_to_append[\"RateLawSubstrates\"] = substrates_key\n",
    "                            dict_entryid_to_append[\"SubstitutedRateLaw\"] = out_rate_law\n",
    "\n",
    "                dict_to_append[reaction] = dict_reactions_to_append\n",
    "\n",
    "            enzyme_dict[enzyme] = dict_to_append\n",
    "\n",
    "        with open(scraped_model_json_file_path, 'w') as f:\n",
    "            json.dump(enzyme_dict, f, indent=4)\n",
    "        \n",
    "        self.step_number = 5\n",
    "        self.progress_update(self.step_number)\n",
    "        \n",
    "    def progress_update(self, step):\n",
    "        if not re.search('[0-5]', str(step)):\n",
    "            print(f'--> ERROR: The {step} step is not acceptable.')\n",
    "        f = open(self.paths['progress_file_path'], \"w\")\n",
    "        f.write(str(step))\n",
    "        f.close()\n",
    "\n",
    "    def main(self,):\n",
    "        self.start()\n",
    "\n",
    "        while True:\n",
    "            if self.step_number == 1:\n",
    "                self.scrape_bigg_xls()\n",
    "            elif self.step_number == 2:\n",
    "                self.glob_xls_files()\n",
    "            elif self.step_number == 3:\n",
    "                self.scrape_entryids()\n",
    "            elif self.step_number == 4:\n",
    "                self.combine_data()\n",
    "            elif self.step_number == 5:\n",
    "                print(\"Execution complete. Scraper finished.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import os\n",
    "fp = webdriver.FirefoxProfile(\"l2pnahxq.scraper\")\n",
    "fp.set_preference(\"browser.download.folderList\",2)\n",
    "fp.set_preference(\"browser.download.dir\", os.getcwd())\n",
    "driver = webdriver.Firefox(firefox_profile=fp, executable_path=\"geckodriver.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chrome driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specify the BIGG Model JSON file path: ./BiGG_models/BiGG model of S. aureus.json\n",
      "C:\\Users\\Andrew Freiburger\\Dropbox\\My PC (DESKTOP-M302P50)\\Documents\\UVic Civil Engineering\\dFBA\\dFBApy\\scraping\\SABIO\\BiGG_models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:200: DeprecationWarning: use options instead of chrome_options\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5613\r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e3274c8e69ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mscraping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSABIO_scraping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mscraping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-aec5b6801a26>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    492\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscrape_bigg_xls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_number\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob_xls_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_number\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscrape_entryids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-aec5b6801a26>\u001b[0m in \u001b[0;36mglob_xls_files\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[1;31m# combine the total set of dataframes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[0mcombined_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m         \u001b[0mcombined_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_dataframes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m         \u001b[0mcombined_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombined_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[0mcombined_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombined_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m     )\n\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No objects to concatenate\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# C:\\Users\\Andrew Freiburger\\Dropbox\\My PC (DESKTOP-M302P50)\\Documents\\UVic Civil Engineering\\dFBA\\BiGG_models\\BiGG model of S. aureus.json\n",
    "# ./BiGG_models/BiGG model of S. aureus.json\n",
    "\n",
    "scraping = SABIO_scraping()\n",
    "scraping.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@authors: Ethan Chan, Matthew Freiburger\n",
    "\"\"\"\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import time\n",
    "import json\n",
    "from itertools import islice\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "class SABIO_scraping():\n",
    "#     __slots__ = (str(x) for x in [progress_file_prefix, xls_download_prefix, scraped_xls_prefix, scraped_entryids_prefix, sel_xls_download_path, processed_xls, entry_json, scraped_model, bigg_model_name_suffix, sub_directory_path, progress_file_path, xls_download_path, scraped_xls_file_path, scraped_entryids_file_path, xls_csv_file_path, entryids_json_file_path, scraped_model_json_file_path, bigg_model, step_number, cwd])\n",
    "    \n",
    "    def __init__(self,source='sabio'):\n",
    "        driver = webdriver.Firefox(executable_path=r\".\\geckodriver.exe\")\n",
    "        if source == 'sabio':\n",
    "            driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "            self.source = 'sabio'\n",
    "            \n",
    "        self.count = 0\n",
    "            \n",
    "        self.parameters = {}\n",
    "        self.parameters['general_delay'] = 2\n",
    "        self.variables = {}\n",
    "        self.paths = {}\n",
    "\n",
    "    #Clicks a HTML element with selenium by id\n",
    "    def click_element_id(self,n_id):\n",
    "        element = self.driver.find_element_by_id(n_id)\n",
    "        element.click()\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "    #Selects a choice from a HTML dropdown element with selenium by id\n",
    "    def select_dropdown_id(self,n_id, n_choice):\n",
    "        element = Select(self.driver.find_element_by_id(n_id))\n",
    "        element.select_by_visible_text(n_choice)\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "    def fatal_error_handler(self,message):\n",
    "        print(\"Error: \" + message)\n",
    "        print(\"Exiting now...\")\n",
    "        exit(0)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 0: GET BIGG MODEL TO SCRAPE AND SETUP DIRECTORIES AND PROGRESS FILE\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def start(self,): # find the BiGG model that will be scraped\n",
    "        while True:\n",
    "            bigg_model_path = input(\"Specify the BIGG Model JSON file path: \")\n",
    "\n",
    "            if os.path.exists(bigg_model_path) and bigg_model_path[-5:] == \".json\":\n",
    "                try:\n",
    "                    self.model = json.load(open(bigg_model_path))\n",
    "                    bigg_model_name = re.search(\"([\\w+\\.?\\s?]+)(?=\\.json)\", bigg_model_path).group()\n",
    "                    break\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "#         self.model = json.load(open('BiGG model of S. aureus.json'))\n",
    "#         bigg_model_path = 'BiGG model of S. aureus.json'\n",
    "#         bigg_model_name = re.search(\"([\\w+\\.?\\s?]+)(?=\\.json)\", bigg_model_path).group()\n",
    "\n",
    "        # define the paths\n",
    "        self.paths['cwd'] = os.path.dirname(os.path.realpath(bigg_model_path))\n",
    "        print(self.paths['cwd'])\n",
    "        self.paths['sub_directory_path'] = os.path.join(self.paths['cwd'],f\"scraping-{bigg_model_name}\")\n",
    "        if not os.path.isdir(self.paths['sub_directory_path']):        \n",
    "            os.mkdir(self.paths['sub_directory_path'])\n",
    "                    \n",
    "        self.variables['scraped_xls'] = {}\n",
    "        self.variables['scraped_entryids'] = {}\n",
    "        self.paths['scraped_model_json_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_model\") + \".json\"\n",
    "        self.paths['sel_xls_download_path'] = os.path.join(self.paths['sub_directory_path'],\"xls_download\")\n",
    "        \n",
    "        self.step_number = 1\n",
    "        \n",
    "        self.paths['progress_file_path'] = os.path.join(self.paths['sub_directory_path'], \"current_progress\") + '.txt'\n",
    "        if os.path.exists(self.paths['progress_file_path']):\n",
    "            f = open(self.paths['progress_file_path'], \"r\")\n",
    "            self.step_number = int(f.read(1))\n",
    "            if not re.search('[1-5]',str(self.step_number)):\n",
    "                self.fatal_error_handler(\"Progress file malformed. Please delete and restart\")\n",
    "        else:\n",
    "            self.step_number = 1\n",
    "            self.progress_update(self.step_number)\n",
    "\n",
    "        self.paths['xls_download_path'] = os.path.join(self.paths['sub_directory_path'], 'downloaded_xls') \n",
    "        if not os.path.isdir(self.paths['xls_download_path']):\n",
    "            os.mkdir(self.paths['xls_download_path'])\n",
    "\n",
    "        self.paths['scraped_xls_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_xls\") + \".json\"\n",
    "        if os.path.exists(self.paths['scraped_xls_file_path']):\n",
    "            f = open(self.paths['scraped_xls_file_path'], \"r\")\n",
    "            self.variables['scraped_xls'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "        self.paths['scraped_entryids_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_entryids\") + \".json\"\n",
    "        if os.path.exists(self.paths['scraped_entryids_file_path']):\n",
    "            f = open(self.paths['scraped_entryids_file_path'], \"r\")\n",
    "            self.variables['scraped_entryids'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "        self.paths['entryids_json_file_path'] = os.path.join(self.paths['sub_directory_path'], \"entryids_json\") + \".json\"\n",
    "        if os.path.exists(self.paths['entryids_json_file_path']):\n",
    "            f = open(self.paths['entryids_json_file_path'], \"r\")\n",
    "            self.variables['entry_id_json_out'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 1: SCRAPE SABIO WEBSITE BY DOWNLOAD XLS FOR GIVEN REACTIONS IN BIGG MODEL\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def scrape_xls(self,reaction_identifier, search_option):\n",
    "        self.count += 1\n",
    "        print(self.count, end='\\r')\n",
    "        \n",
    "        \n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "        self.click_element_id(\"resetbtn\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.click_element_id(\"option\")\n",
    "        self.select_dropdown_id(\"searchterms\", search_option)\n",
    "        text_area = self.driver.find_element_by_id(\"searchtermField\")\n",
    "        text_area.send_keys(reaction_identifier)  \n",
    "        time.sleep(self.parameters['general_delay'])  \n",
    "        self.click_element_id(\"addsearch\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        result_num = \"\"\n",
    "        try: \n",
    "            result_num_ele = self.driver.find_element_by_id(\"numberofKinLaw\")\n",
    "            for char in result_num_ele.text:\n",
    "                if re.search('[0-9]', char):\n",
    "                    result_num = result_num + char\n",
    "\n",
    "            result_num = int(result_num)\n",
    "        except:\n",
    "            self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "            return False\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.select_dropdown_id(\"max\", \"100\")\n",
    "        element = Select(self.driver.find_element_by_id(\"max\"))\n",
    "        element.select_by_visible_text(\"100\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        if result_num > 0 and result_num <= 100:\n",
    "            self.click_element_id(\"allCheckbox\")\n",
    "            time.sleep(self.parameters['general_delay'])\n",
    "        elif result_num > 100:\n",
    "            self.click_element_id(\"allCheckbox\")\n",
    "            for i in range(int(result_num/100)):\n",
    "                element = self.driver.find_element_by_xpath(\"//*[@class = 'nextLink']\")\n",
    "                element.click()\n",
    "                time.sleep(self.parameters['general_delay'])\n",
    "                self.click_element_id(\"allCheckbox\")\n",
    "                time.sleep(self.parameters['general_delay'])\n",
    "        else:\n",
    "            self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "            return False\n",
    "\n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/spreadsheetExport\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay']*7.5)\n",
    "\n",
    "        self.click_element_id(\"excelExport\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay']*2.5)\n",
    "\n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "\n",
    "        return True\n",
    "    \n",
    "    def expand_shadow_element(self, element):\n",
    "        shadow_root = self.driver.execute_script('return arguments[0].shadowRoot', element)\n",
    "        return shadow_root\n",
    "\n",
    "\n",
    "    def scrape_bigg_xls(self,):\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        prefs = {'download.default_directory' : self.paths['cwd'] + self.paths['sel_xls_download_path']}\n",
    "        chrome_options.add_experimental_option('prefs', prefs)\n",
    "        self.driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "        \n",
    "        self.driver.get(\"chrome://settings/security\")\n",
    "\n",
    "        \n",
    "        root = self.driver.find_element_by_tag_name(\"settings-ui\")\n",
    "        shadow_root = self.expand_shadow_element(root)\n",
    "        \n",
    "        root1 = shadow_root.find_element_by_tag_name(\"settings-main\")\n",
    "        shadow_root1 = self.expand_shadow_element(root1)\n",
    "        \n",
    "        root2 = shadow_root1.find_element_by_tag_name(\"settings-basic-page\")\n",
    "        shadow_root2 = self.expand_shadow_element(root2)\n",
    "        \n",
    "        root3 = shadow_root2.find_element_by_tag_name(\"settings-privacy-page\")\n",
    "        shadow_root3 = self.expand_shadow_element(root3)\n",
    "        \n",
    "        root4 = shadow_root3.find_element_by_tag_name(\"settings-security-page\")\n",
    "        shadow_root4 = self.expand_shadow_element(root4)\n",
    "        \n",
    "        root5 = shadow_root4.find_element_by_css_selector(\"#safeBrowsingStandard\")\n",
    "        shadow_root5 = self.expand_shadow_element(root5)\n",
    "        \n",
    "        security_button = shadow_root5.find_element_by_css_selector(\"div.disc-border\")\n",
    "        \n",
    "        security_button.click()\n",
    "\n",
    "\n",
    "        \n",
    "        for reaction in self.model[\"reactions\"]:\n",
    "            if not reaction[\"name\"] in self.variables['scraped_xls']:\n",
    "                ids_to_try = reaction[\"annotation\"]\n",
    "\n",
    "                success_flag = False\n",
    "                annotation_search_pairs = {\"sabiork\":\"SabioReactionID\", \"metanetx.reaction\":\"MetaNetXReactionID\", \"ec-code\":\"ECNumber\", \"kegg.reaction\":\"KeggReactionID\", \"rhea\":\"RheaReactionID\"}\n",
    "                for annotation in annotation_search_pairs:\n",
    "                    if not success_flag:\n",
    "                        if annotation in ids_to_try:\n",
    "\n",
    "\n",
    "                            for id_to_try in ids_to_try[annotation]:\n",
    "#                                 success_flag = self.scrape_xls(id_to_try, annotation_search_pairs[annotation])\n",
    "                                try:\n",
    "                                    success_flag = self.scrape_xls(id_to_try, annotation_search_pairs[annotation])\n",
    "                                except:\n",
    "                                    success_flag = False\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                if not success_flag:\n",
    "                    try:\n",
    "                        success_flag = self.scrape_xls(reaction[\"name\"], \"Enzymename\")\n",
    "                    except:\n",
    "                        success_flag = False\n",
    "\n",
    "                json_dict_key = reaction[\"name\"].replace(\"\\\"\", \"\")\n",
    "                if success_flag:\n",
    "                    self.variables['scraped_xls'][json_dict_key] = \"yes\"\n",
    "                else:\n",
    "                    self.variables['scraped_xls'][json_dict_key] = \"no\"\n",
    "                    \n",
    "                    \n",
    "\n",
    "            with open(self.paths['scraped_xls_file_path'], 'w') as outfile:\n",
    "                json.dump(self.variables['scraped_xls'], outfile, indent = 4)   \n",
    "                outfile.close()\n",
    "                \n",
    "        self.step_number = 2\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 2: GLOB EXPORTED XLS FILES TOGETHER\n",
    "    --------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    def glob_xls_files(self,):\n",
    "        scraped_sans_parentheses_enzymes = glob.glob('./{}/*.xls'.format(self.paths['xls_download_path']))\n",
    "        total_dataframes = []\n",
    "        for file in scraped_sans_parentheses_enzymes:\n",
    "            #file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "            dfn = pd.read_excel(file)\n",
    "            total_dataframes.append(dfn)\n",
    "\n",
    "        # combine the total set of dataframes\n",
    "        combined_df = pd.DataFrame()\n",
    "        combined_df = pd.concat(total_dataframes)\n",
    "        combined_df = combined_df.fillna(' ')\n",
    "        combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "        # export the dataframe\n",
    "        csv_path = os.path.join(self.paths['sub_directory_path'], \"proccessed-xls\") + \".csv\"\n",
    "        combined_df.to_csv(csv_path)\n",
    "        \n",
    "        # update the \n",
    "        self.step_number = 3\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 3: SCRAPE ADDITIONAL DATA BY ENTRYID\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def scrape_entry_id(self,entry_id):\n",
    "        global entry_id_json_out\n",
    "\n",
    "        entry_id = str(entry_id)\n",
    "\n",
    "        self.driver = webdriver.Chrome(executable_path=r\".\\chromedriver.exe\")\n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(\"option\")\n",
    "        self.select_dropdown_id(\"searchterms\", \"EntryID\")\n",
    "        text_area = self.driver.find_element_by_id(\"searchtermField\")\n",
    "        text_area.send_keys(entry_id)\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(\"addsearch\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(entry_id + \"img\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.driver.switch_to.frame(self.driver.find_element_by_xpath(\"//iframe[@name='iframe_\" + entry_id + \"']\"))\n",
    "        element = self.driver.find_element_by_xpath(\"//table\")\n",
    "        html_source = element.get_attribute('innerHTML')\n",
    "        \n",
    "        table_df = pd.read_html(html_source)\n",
    "        reaction_parameters_df = pd.DataFrame()\n",
    "        counter = 0\n",
    "        parameters_json = {}\n",
    "        for df in table_df:\n",
    "            try:\n",
    "                if df[0][0] == \"Parameter\":\n",
    "                    reaction_parameters_df = table_df[counter]\n",
    "            except:\n",
    "                self.driver.close()\n",
    "                return parameters_json\n",
    "            counter += 1\n",
    "            \n",
    "        parameter_name = \"\"\n",
    "        for i in range(len(reaction_parameters_df[0])-2):\n",
    "            parameter_name = reaction_parameters_df[0][i+2]\n",
    "            inner_parameters_json = {}\n",
    "            for j in range(len(reaction_parameters_df)-3):\n",
    "                inner_parameters_json[reaction_parameters_df[j+1][1]] = reaction_parameters_df[j+1][i+2]\n",
    "\n",
    "            parameters_json[parameter_name] = inner_parameters_json\n",
    "\n",
    "        self.driver.close()\n",
    "\n",
    "        return parameters_json\n",
    "\n",
    "\n",
    "    def scrape_entryids(self,):\n",
    "        global entry_id_json_out\n",
    "\n",
    "        sabio_xls_df = pd.read_csv(xls_csv_file_path)\n",
    "        entryids = sabio_xls_df[\"EntryID\"].unique().tolist()\n",
    "\n",
    "        for entryid in entryids:\n",
    "            if not entryid in scraped_entryids:\n",
    "                try:\n",
    "                    entry_id_json_out[str(entryid)] = scrape_entry_id(entryid)\n",
    "                    self.variables['scraped_entryids'][entryid] = \"yes\"\n",
    "                except:\n",
    "                    self.variables['scraped_entryids'][entryid] = \"no\"\n",
    "            with open(scraped_entryids_file_path, 'w') as outfile:\n",
    "                json.dump(self.variables['scraped_entryids'], outfile, indent = 4)   \n",
    "                outfile.close()\n",
    "            with open(entryids_json_file_path, 'w') as f:\n",
    "                json.dump(entry_id_json_out, f, indent = 4)        \n",
    "                f.close()\n",
    "        \n",
    "        self.step_number = 4\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 4: COMBINE ENZYME AND ENTRYID DATA INTO JSON FILE\n",
    "    --------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    def combine_data(self,):\n",
    "\n",
    "        sabio_xls_df = pd.read_csv(self.paths['xls_csv_file_path'])\n",
    "\n",
    "        # Opening JSON file\n",
    "        with open(self.paths['entryids_json_file_path']) as json_file:\n",
    "            entry_id_data = json.load(json_file)\n",
    "\n",
    "        enzymenames = sabio_xls_df[\"Enzymename\"].unique().tolist()\n",
    "        enzyme_dict = {}\n",
    "        missing_entry_ids = []\n",
    "        parameters = {}\n",
    "\n",
    "        for enzyme in enzymenames:\n",
    "            sabio_grouped_enzyme_df = sabio_xls_df.loc[sabio_xls_df[\"Enzymename\"] == enzyme]\n",
    "            dict_to_append = {}\n",
    "            reactions = sabio_grouped_enzyme_df[\"Reaction\"].unique().tolist()\n",
    "            for reaction in reactions:\n",
    "                dict_reactions_to_append = {}\n",
    "                sabio_grouped_reactions_df = sabio_grouped_enzyme_df.loc[sabio_grouped_enzyme_df[\"Reaction\"] == reaction]\n",
    "                entryids = sabio_grouped_reactions_df[\"EntryID\"].unique().tolist()\n",
    "\n",
    "                for entryid in entryids:\n",
    "                    entry_ids_df = sabio_grouped_reactions_df.loc[sabio_grouped_reactions_df[\"EntryID\"] == entryid]\n",
    "                    dict_entryid_to_append = {}\n",
    "                    head_of_df = entry_ids_df.head(1).squeeze()\n",
    "                    entry_id_flag = True\n",
    "                    parameter_info = {}\n",
    "\n",
    "                    try:\n",
    "                        parameter_info = entry_id_data[str(entryid)]\n",
    "                        dict_entryid_to_append[\"Parameters\"] = parameter_info\n",
    "                    except:\n",
    "                        missing_entry_ids.append(str(entryid))\n",
    "                        entry_id_flag = False\n",
    "                        dict_entryid_to_append[\"Parameters\"] = \"NaN\"\n",
    "\n",
    "                    rate_law = head_of_df[\"Rate Equation\"]\n",
    "                    bad_rate_laws = [\"unknown\", \"\", \"-\"]\n",
    "\n",
    "                    if not rate_law in bad_rate_laws:                    \n",
    "                        dict_entryid_to_append[\"RateLaw\"] = rate_law\n",
    "                        dict_entryid_to_append[\"SubstitutedRateLaw\"] = rate_law\n",
    "                    else:\n",
    "                        dict_entryid_to_append[\"RateLaw\"] = \"NaN\"\n",
    "                        dict_entryid_to_append[\"SubstitutedRateLaw\"] = \"NaN\"\n",
    "\n",
    "                    if entry_id_flag:\n",
    "\n",
    "                        fields_to_copy = [\"Buffer\", \"Product\", \"PubMedID\", \"Publication\", \"pH\", \"Temperature\", \"Enzyme Variant\", \"UniProtKB_AC\", \"Organism\", \"KineticMechanismType\", \"SabioReactionID\"]\n",
    "                        for field in fields_to_copy:  \n",
    "                            dict_entryid_to_append[field] = head_of_df[field]\n",
    "                        dict_reactions_to_append[entryid] = dict_entryid_to_append\n",
    "                        dict_entryid_to_append[\"Substrates\"] = head_of_df[\"Substrate\"].split(\";\")\n",
    "                        out_rate_law = rate_law\n",
    "                        if not rate_law in bad_rate_laws:                    \n",
    "                            substrates = head_of_df[\"Substrate\"].split(\";\")\n",
    "\n",
    "                            stripped_string = re.sub('[0-9]', '', rate_law)\n",
    "\n",
    "                            variables = re.split(\"\\^|\\*|\\+|\\-|\\/|\\(|\\)| \", stripped_string)\n",
    "                            variables = ' '.join(variables).split()\n",
    "\n",
    "                            start_value_permutations = [\"start value\", \"start val.\"]\n",
    "                            substrates_key = {}\n",
    "                            for var in variables:\n",
    "                                if var in parameter_info:\n",
    "                                    for permutation in start_value_permutations:\n",
    "                                        try:\n",
    "                                            if var == \"A\" or var == \"B\":\n",
    "                                                substrates_key[var] = parameter_info[var][\"species\"]\n",
    "                                            else:\n",
    "                                                value = parameter_info[var][permutation]\n",
    "                                                if value != \"-\" and value != \"\" and value != \" \":\n",
    "                                                    out_rate_law = out_rate_law.replace(var, parameter_info[var][permutation])\n",
    "                                        except:\n",
    "                                            pass\n",
    "\n",
    "                            dict_entryid_to_append[\"RateLawSubstrates\"] = substrates_key\n",
    "                            dict_entryid_to_append[\"SubstitutedRateLaw\"] = out_rate_law\n",
    "\n",
    "                dict_to_append[reaction] = dict_reactions_to_append\n",
    "\n",
    "            enzyme_dict[enzyme] = dict_to_append\n",
    "\n",
    "        with open(scraped_model_json_file_path, 'w') as f:\n",
    "            json.dump(enzyme_dict, f, indent=4)\n",
    "        \n",
    "        self.step_number = 5\n",
    "        self.progress_update(self.step_number)\n",
    "        \n",
    "    def progress_update(self, step):\n",
    "        if not re.search('[0-5]', str(step)):\n",
    "            print(f'--> ERROR: The {step} step is not acceptable.')\n",
    "        f = open(self.paths['progress_file_path'], \"w\")\n",
    "        f.write(str(step))\n",
    "        f.close()\n",
    "\n",
    "    def main(self,):\n",
    "        self.start()\n",
    "\n",
    "        while True:\n",
    "            if self.step_number == 1:\n",
    "                self.scrape_bigg_xls()\n",
    "            elif self.step_number == 2:\n",
    "                self.glob_xls_files()\n",
    "            elif self.step_number == 3:\n",
    "                self.scrape_entryids()\n",
    "            elif self.step_number == 4:\n",
    "                self.combine_data()\n",
    "            elif self.step_number == 5:\n",
    "                print(\"Execution complete. Scraper finished.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@authors: Ethan Chan, Matthew Freiburger\n",
    "\"\"\"\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import time\n",
    "import json\n",
    "from itertools import islice\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "class SABIO_scraping():\n",
    "#     __slots__ = (str(x) for x in [progress_file_prefix, xls_download_prefix, scraped_xls_prefix, scraped_entryids_prefix, sel_xls_download_path, processed_xls, entry_json, scraped_model, bigg_model_name_suffix, sub_directory_path, progress_file_path, xls_download_path, scraped_xls_file_path, scraped_entryids_file_path, xls_csv_file_path, entryids_json_file_path, scraped_model_json_file_path, bigg_model, step_number, cwd])\n",
    "    \n",
    "    def __init__(self,source='sabio'):\n",
    "        driver = webdriver.Chrome(executable_path=r\".\\chromedriver.exe\")\n",
    "        if source == 'sabio':\n",
    "            driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "            self.source = 'sabio'\n",
    "            \n",
    "        self.count = 0\n",
    "            \n",
    "        self.parameters = {}\n",
    "        self.parameters['general_delay'] = 2\n",
    "        self.variables = {}\n",
    "        self.paths = {}\n",
    "\n",
    "    #Clicks a HTML element with selenium by id\n",
    "    def click_element_id(self,n_id):\n",
    "        element = self.driver.find_element_by_id(n_id)\n",
    "        element.click()\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "    #Selects a choice from a HTML dropdown element with selenium by id\n",
    "    def select_dropdown_id(self,n_id, n_choice):\n",
    "        element = Select(self.driver.find_element_by_id(n_id))\n",
    "        element.select_by_visible_text(n_choice)\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "    def fatal_error_handler(self,message):\n",
    "        print(\"Error: \" + message)\n",
    "        print(\"Exiting now...\")\n",
    "        exit(0)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 0: GET BIGG MODEL TO SCRAPE AND SETUP DIRECTORIES AND PROGRESS FILE\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def start(self,): # find the BiGG model that will be scraped\n",
    "        while True:\n",
    "            bigg_model_path = input(\"Specify the BIGG Model JSON file path: \")\n",
    "\n",
    "            if os.path.exists(bigg_model_path) and bigg_model_path[-5:] == \".json\":\n",
    "                try:\n",
    "                    self.model = json.load(open(bigg_model_path))\n",
    "                    bigg_model_name = re.search(\"([\\w+\\.?\\s?]+)(?=\\.json)\", bigg_model_path).group()\n",
    "                    break\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "#         self.model = json.load(open('BiGG model of S. aureus.json'))\n",
    "#         bigg_model_path = 'BiGG model of S. aureus.json'\n",
    "#         bigg_model_name = re.search(\"([\\w+\\.?\\s?]+)(?=\\.json)\", bigg_model_path).group()\n",
    "\n",
    "        # define the paths\n",
    "        self.paths['cwd'] = os.path.dirname(os.path.realpath(bigg_model_path))\n",
    "        print(self.paths['cwd'])\n",
    "        self.paths['sub_directory_path'] = os.path.join(self.paths['cwd'],f\"scraping-{bigg_model_name}\")\n",
    "        if not os.path.isdir(self.paths['sub_directory_path']):        \n",
    "            os.mkdir(self.paths['sub_directory_path'])\n",
    "                    \n",
    "        self.variables['scraped_xls'] = {}\n",
    "        self.variables['scraped_entryids'] = {}\n",
    "        self.paths['scraped_model_json_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_model\") + \".json\"\n",
    "        self.paths['sel_xls_download_path'] = os.path.join(self.paths['sub_directory_path'],\"xls_download\")\n",
    "        \n",
    "        self.step_number = 1\n",
    "        \n",
    "        self.paths['progress_file_path'] = os.path.join(self.paths['sub_directory_path'], \"current_progress\") + '.txt'\n",
    "        if os.path.exists(self.paths['progress_file_path']):\n",
    "            f = open(self.paths['progress_file_path'], \"r\")\n",
    "            self.step_number = int(f.read(1))\n",
    "            if not re.search('[1-5]',str(self.step_number)):\n",
    "                self.fatal_error_handler(\"Progress file malformed. Please delete and restart\")\n",
    "        else:\n",
    "            self.step_number = 1\n",
    "            self.progress_update(self.step_number)\n",
    "\n",
    "        self.paths['xls_download_path'] = os.path.join(self.paths['sub_directory_path'], 'downloaded_xls') \n",
    "        if not os.path.isdir(self.paths['xls_download_path']):\n",
    "            os.mkdir(self.paths['xls_download_path'])\n",
    "\n",
    "        self.paths['scraped_xls_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_xls\") + \".json\"\n",
    "        if os.path.exists(self.paths['scraped_xls_file_path']):\n",
    "            f = open(self.paths['scraped_xls_file_path'], \"r\")\n",
    "            self.variables['scraped_xls'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "        self.paths['scraped_entryids_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_entryids\") + \".json\"\n",
    "        if os.path.exists(self.paths['scraped_entryids_file_path']):\n",
    "            f = open(self.paths['scraped_entryids_file_path'], \"r\")\n",
    "            self.variables['scraped_entryids'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "        self.paths['entryids_json_file_path'] = os.path.join(self.paths['sub_directory_path'], \"entryids_json\") + \".json\"\n",
    "        if os.path.exists(self.paths['entryids_json_file_path']):\n",
    "            f = open(self.paths['entryids_json_file_path'], \"r\")\n",
    "            self.variables['entry_id_json_out'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 1: SCRAPE SABIO WEBSITE BY DOWNLOAD XLS FOR GIVEN REACTIONS IN BIGG MODEL\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def scrape_xls(self,reaction_identifier, search_option):\n",
    "        self.count += 1\n",
    "        print(self.count, end='\\r')\n",
    "        \n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        prefs = {'download.default_directory' : self.paths['cwd'] + self.paths['sel_xls_download_path'], \"download.prompt_for_download\": False, \"download.directory_upgrade\": True, \"safebrowsing.enabled\": True}\n",
    "\n",
    "#         prefs = {'download.default_directory' : self.paths['cwd'] + self.paths['sel_xls_download_path']}\n",
    "        chrome_options.add_experimental_option('prefs', prefs)\n",
    "        self.driver = webdriver.Chrome(chrome_options=chrome_options)    \n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.click_element_id(\"option\")\n",
    "        self.select_dropdown_id(\"searchterms\", search_option)\n",
    "        text_area = self.driver.find_element_by_id(\"searchtermField\")\n",
    "        text_area.send_keys(reaction_identifier)  \n",
    "        time.sleep(self.parameters['general_delay'])  \n",
    "        self.click_element_id(\"addsearch\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        result_num = \"\"\n",
    "        try: \n",
    "            result_num_ele = self.driver.find_element_by_id(\"numberofKinLaw\")\n",
    "            for char in result_num_ele.text:\n",
    "                if re.search('[0-9]', char):\n",
    "                    result_num = result_num + char\n",
    "\n",
    "            result_num = int(result_num)\n",
    "        except:\n",
    "            self.driver.close()\n",
    "            return False\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.select_dropdown_id(\"max\", \"100\")\n",
    "        element = Select(self.driver.find_element_by_id(\"max\"))\n",
    "        element.select_by_visible_text(\"100\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        if result_num > 0 and result_num <= 100:\n",
    "            self.click_element_id(\"allCheckbox\")\n",
    "            time.sleep(self.parameters['general_delay'])\n",
    "        elif result_num > 100:\n",
    "            self.click_element_id(\"allCheckbox\")\n",
    "            for i in range(int(result_num/100)):\n",
    "                element = self.driver.find_element_by_xpath(\"//*[@class = 'nextLink']\")\n",
    "                element.click()\n",
    "                time.sleep(self.parameters['general_delay'])\n",
    "                self.click_element_id(\"allCheckbox\")\n",
    "                time.sleep(self.parameters['general_delay'])\n",
    "        else:\n",
    "            self.driver.close()\n",
    "            return False\n",
    "\n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/spreadsheetExport\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay']*7.5)\n",
    "\n",
    "        self.click_element_id(\"excelExport\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay']*2.5)\n",
    "\n",
    "        self.driver.close()\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "    def scrape_bigg_xls(self,):\n",
    "        for reaction in self.model[\"reactions\"]:\n",
    "            if not reaction[\"name\"] in self.variables['scraped_xls']:\n",
    "                ids_to_try = reaction[\"annotation\"]\n",
    "\n",
    "                success_flag = False\n",
    "                annotation_search_pairs = {\"sabiork\":\"SabioReactionID\", \"metanetx.reaction\":\"MetaNetXReactionID\", \"ec-code\":\"ECNumber\", \"kegg.reaction\":\"KeggReactionID\", \"rhea\":\"RheaReactionID\"}\n",
    "                for annotation in annotation_search_pairs:\n",
    "                    if not success_flag:\n",
    "                        if annotation in ids_to_try:\n",
    "                            for id_to_try in ids_to_try[annotation]:\n",
    "#                                 success_flag = self.scrape_xls(id_to_try, annotation_search_pairs[annotation])\n",
    "                                try:\n",
    "                                    success_flag = self.scrape_xls(id_to_try, annotation_search_pairs[annotation])\n",
    "                                except:\n",
    "                                    success_flag = False\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                if not success_flag:\n",
    "#                     success_flag = self.scrape_xls(reaction[\"name\"], \"Enzymename\")\n",
    "                    try:\n",
    "                        success_flag = self.scrape_xls(reaction[\"name\"], \"Enzymename\")\n",
    "                    except:\n",
    "                        success_flag = False\n",
    "\n",
    "                json_dict_key = reaction[\"name\"].replace(\"\\\"\", \"\")\n",
    "                if success_flag:\n",
    "                    self.variables['scraped_xls'][json_dict_key] = \"yes\"\n",
    "                else:\n",
    "                    self.variables['scraped_xls'][json_dict_key] = \"no\"\n",
    "\n",
    "            with open(self.paths['scraped_xls_file_path'], 'w') as outfile:\n",
    "                json.dump(self.variables['scraped_xls'], outfile, indent = 4)   \n",
    "                outfile.close()\n",
    "                \n",
    "        self.step_number = 2\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 2: GLOB EXPORTED XLS FILES TOGETHER\n",
    "    --------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    def glob_xls_files(self,):\n",
    "        scraped_sans_parentheses_enzymes = glob.glob('./{}/*.xls'.format(self.paths['xls_download_path']))\n",
    "        total_dataframes = []\n",
    "        for file in scraped_sans_parentheses_enzymes:\n",
    "            #file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "            dfn = pd.read_excel(file)\n",
    "            total_dataframes.append(dfn)\n",
    "\n",
    "        # combine the total set of dataframes\n",
    "        combined_df = pd.DataFrame()\n",
    "        combined_df = pd.concat(total_dataframes)\n",
    "        combined_df = combined_df.fillna(' ')\n",
    "        combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "        # export the dataframe\n",
    "        csv_path = os.path.join(self.paths['sub_directory_path'], \"proccessed-xls\") + \".csv\"\n",
    "        combined_df.to_csv(csv_path)\n",
    "        \n",
    "        # update the \n",
    "        self.step_number = 3\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 3: SCRAPE ADDITIONAL DATA BY ENTRYID\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def scrape_entry_id(self,entry_id):\n",
    "        global entry_id_json_out\n",
    "\n",
    "        entry_id = str(entry_id)\n",
    "\n",
    "        self.driver = webdriver.Chrome(executable_path=r\".\\chromedriver.exe\")\n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(\"option\")\n",
    "        self.select_dropdown_id(\"searchterms\", \"EntryID\")\n",
    "        text_area = self.driver.find_element_by_id(\"searchtermField\")\n",
    "        text_area.send_keys(entry_id)\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(\"addsearch\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(entry_id + \"img\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.driver.switch_to.frame(self.driver.find_element_by_xpath(\"//iframe[@name='iframe_\" + entry_id + \"']\"))\n",
    "        element = self.driver.find_element_by_xpath(\"//table\")\n",
    "        html_source = element.get_attribute('innerHTML')\n",
    "        \n",
    "        table_df = pd.read_html(html_source)\n",
    "        reaction_parameters_df = pd.DataFrame()\n",
    "        counter = 0\n",
    "        parameters_json = {}\n",
    "        for df in table_df:\n",
    "            try:\n",
    "                if df[0][0] == \"Parameter\":\n",
    "                    reaction_parameters_df = table_df[counter]\n",
    "            except:\n",
    "                self.driver.close()\n",
    "                return parameters_json\n",
    "            counter += 1\n",
    "            \n",
    "        parameter_name = \"\"\n",
    "        for i in range(len(reaction_parameters_df[0])-2):\n",
    "            parameter_name = reaction_parameters_df[0][i+2]\n",
    "            inner_parameters_json = {}\n",
    "            for j in range(len(reaction_parameters_df)-3):\n",
    "                inner_parameters_json[reaction_parameters_df[j+1][1]] = reaction_parameters_df[j+1][i+2]\n",
    "\n",
    "            parameters_json[parameter_name] = inner_parameters_json\n",
    "\n",
    "        self.driver.close()\n",
    "\n",
    "        return parameters_json\n",
    "\n",
    "\n",
    "    def scrape_entryids(self,):\n",
    "        global entry_id_json_out\n",
    "\n",
    "        sabio_xls_df = pd.read_csv(xls_csv_file_path)\n",
    "        entryids = sabio_xls_df[\"EntryID\"].unique().tolist()\n",
    "\n",
    "        for entryid in entryids:\n",
    "            if not entryid in scraped_entryids:\n",
    "                try:\n",
    "                    entry_id_json_out[str(entryid)] = scrape_entry_id(entryid)\n",
    "                    self.variables['scraped_entryids'][entryid] = \"yes\"\n",
    "                except:\n",
    "                    self.variables['scraped_entryids'][entryid] = \"no\"\n",
    "            with open(scraped_entryids_file_path, 'w') as outfile:\n",
    "                json.dump(self.variables['scraped_entryids'], outfile, indent = 4)   \n",
    "                outfile.close()\n",
    "            with open(entryids_json_file_path, 'w') as f:\n",
    "                json.dump(entry_id_json_out, f, indent = 4)        \n",
    "                f.close()\n",
    "        \n",
    "        self.step_number = 4\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 4: COMBINE ENZYME AND ENTRYID DATA INTO JSON FILE\n",
    "    --------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    def combine_data(self,):\n",
    "\n",
    "        sabio_xls_df = pd.read_csv(self.paths['xls_csv_file_path'])\n",
    "\n",
    "        # Opening JSON file\n",
    "        with open(self.paths['entryids_json_file_path']) as json_file:\n",
    "            entry_id_data = json.load(json_file)\n",
    "\n",
    "        enzymenames = sabio_xls_df[\"Enzymename\"].unique().tolist()\n",
    "        enzyme_dict = {}\n",
    "        missing_entry_ids = []\n",
    "        parameters = {}\n",
    "\n",
    "        for enzyme in enzymenames:\n",
    "            sabio_grouped_enzyme_df = sabio_xls_df.loc[sabio_xls_df[\"Enzymename\"] == enzyme]\n",
    "            dict_to_append = {}\n",
    "            reactions = sabio_grouped_enzyme_df[\"Reaction\"].unique().tolist()\n",
    "            for reaction in reactions:\n",
    "                dict_reactions_to_append = {}\n",
    "                sabio_grouped_reactions_df = sabio_grouped_enzyme_df.loc[sabio_grouped_enzyme_df[\"Reaction\"] == reaction]\n",
    "                entryids = sabio_grouped_reactions_df[\"EntryID\"].unique().tolist()\n",
    "\n",
    "                for entryid in entryids:\n",
    "                    entry_ids_df = sabio_grouped_reactions_df.loc[sabio_grouped_reactions_df[\"EntryID\"] == entryid]\n",
    "                    dict_entryid_to_append = {}\n",
    "                    head_of_df = entry_ids_df.head(1).squeeze()\n",
    "                    entry_id_flag = True\n",
    "                    parameter_info = {}\n",
    "\n",
    "                    try:\n",
    "                        parameter_info = entry_id_data[str(entryid)]\n",
    "                        dict_entryid_to_append[\"Parameters\"] = parameter_info\n",
    "                    except:\n",
    "                        missing_entry_ids.append(str(entryid))\n",
    "                        entry_id_flag = False\n",
    "                        dict_entryid_to_append[\"Parameters\"] = \"NaN\"\n",
    "\n",
    "                    rate_law = head_of_df[\"Rate Equation\"]\n",
    "                    bad_rate_laws = [\"unknown\", \"\", \"-\"]\n",
    "\n",
    "                    if not rate_law in bad_rate_laws:                    \n",
    "                        dict_entryid_to_append[\"RateLaw\"] = rate_law\n",
    "                        dict_entryid_to_append[\"SubstitutedRateLaw\"] = rate_law\n",
    "                    else:\n",
    "                        dict_entryid_to_append[\"RateLaw\"] = \"NaN\"\n",
    "                        dict_entryid_to_append[\"SubstitutedRateLaw\"] = \"NaN\"\n",
    "\n",
    "                    if entry_id_flag:\n",
    "\n",
    "                        fields_to_copy = [\"Buffer\", \"Product\", \"PubMedID\", \"Publication\", \"pH\", \"Temperature\", \"Enzyme Variant\", \"UniProtKB_AC\", \"Organism\", \"KineticMechanismType\", \"SabioReactionID\"]\n",
    "                        for field in fields_to_copy:  \n",
    "                            dict_entryid_to_append[field] = head_of_df[field]\n",
    "                        dict_reactions_to_append[entryid] = dict_entryid_to_append\n",
    "                        dict_entryid_to_append[\"Substrates\"] = head_of_df[\"Substrate\"].split(\";\")\n",
    "                        out_rate_law = rate_law\n",
    "                        if not rate_law in bad_rate_laws:                    \n",
    "                            substrates = head_of_df[\"Substrate\"].split(\";\")\n",
    "\n",
    "                            stripped_string = re.sub('[0-9]', '', rate_law)\n",
    "\n",
    "                            variables = re.split(\"\\^|\\*|\\+|\\-|\\/|\\(|\\)| \", stripped_string)\n",
    "                            variables = ' '.join(variables).split()\n",
    "\n",
    "                            start_value_permutations = [\"start value\", \"start val.\"]\n",
    "                            substrates_key = {}\n",
    "                            for var in variables:\n",
    "                                if var in parameter_info:\n",
    "                                    for permutation in start_value_permutations:\n",
    "                                        try:\n",
    "                                            if var == \"A\" or var == \"B\":\n",
    "                                                substrates_key[var] = parameter_info[var][\"species\"]\n",
    "                                            else:\n",
    "                                                value = parameter_info[var][permutation]\n",
    "                                                if value != \"-\" and value != \"\" and value != \" \":\n",
    "                                                    out_rate_law = out_rate_law.replace(var, parameter_info[var][permutation])\n",
    "                                        except:\n",
    "                                            pass\n",
    "\n",
    "                            dict_entryid_to_append[\"RateLawSubstrates\"] = substrates_key\n",
    "                            dict_entryid_to_append[\"SubstitutedRateLaw\"] = out_rate_law\n",
    "\n",
    "                dict_to_append[reaction] = dict_reactions_to_append\n",
    "\n",
    "            enzyme_dict[enzyme] = dict_to_append\n",
    "\n",
    "        with open(scraped_model_json_file_path, 'w') as f:\n",
    "            json.dump(enzyme_dict, f, indent=4)\n",
    "        \n",
    "        self.step_number = 5\n",
    "        self.progress_update(self.step_number)\n",
    "        \n",
    "    def progress_update(self, step):\n",
    "        if not re.search('[0-5]', str(step)):\n",
    "            print(f'--> ERROR: The {step} step is not acceptable.')\n",
    "        f = open(self.paths['progress_file_path'], \"w\")\n",
    "        f.write(str(step))\n",
    "        f.close()\n",
    "\n",
    "    def main(self,):\n",
    "        self.start()\n",
    "\n",
    "        while True:\n",
    "            if self.step_number == 1:\n",
    "                self.scrape_bigg_xls()\n",
    "            elif self.step_number == 2:\n",
    "                self.glob_xls_files()\n",
    "            elif self.step_number == 3:\n",
    "                self.scrape_entryids()\n",
    "            elif self.step_number == 4:\n",
    "                self.combine_data()\n",
    "            elif self.step_number == 5:\n",
    "                print(\"Execution complete. Scraper finished.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@authors: Ethan Chan, Matthew Freiburger\n",
    "\"\"\"\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import time\n",
    "import json\n",
    "from itertools import islice\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "class SABIO_scraping():\n",
    "#     __slots__ = (str(x) for x in [progress_file_prefix, xls_download_prefix, scraped_xls_prefix, scraped_entryids_prefix, sel_xls_download_path, processed_xls, entry_json, scraped_model, bigg_model_name_suffix, sub_directory_path, progress_file_path, xls_download_path, scraped_xls_file_path, scraped_entryids_file_path, xls_csv_file_path, entryids_json_file_path, scraped_model_json_file_path, bigg_model, step_number, cwd])\n",
    "    \n",
    "    def __init__(self,source='sabio'):\n",
    "        driver = webdriver.Chrome(executable_path=r\".\\chromedriver.exe\")\n",
    "        if source == 'sabio':\n",
    "            driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "            self.source = 'sabio'\n",
    "            \n",
    "        self.count = 0\n",
    "            \n",
    "        self.parameters = {}\n",
    "        self.parameters['general_delay'] = 2\n",
    "        self.variables = {}\n",
    "        self.paths = {}\n",
    "\n",
    "    #Clicks a HTML element with selenium by id\n",
    "    def click_element_id(self,n_id):\n",
    "        element = self.driver.find_element_by_id(n_id)\n",
    "        element.click()\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "    #Selects a choice from a HTML dropdown element with selenium by id\n",
    "    def select_dropdown_id(self,n_id, n_choice):\n",
    "        element = Select(self.driver.find_element_by_id(n_id))\n",
    "        element.select_by_visible_text(n_choice)\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "    def fatal_error_handler(self,message):\n",
    "        print(\"Error: \" + message)\n",
    "        print(\"Exiting now...\")\n",
    "        exit(0)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 0: GET BIGG MODEL TO SCRAPE AND SETUP DIRECTORIES AND PROGRESS FILE\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def start(self,): # find the BiGG model that will be scraped\n",
    "        while True:\n",
    "            bigg_model_path = input(\"Specify the BIGG Model JSON file path: \")\n",
    "\n",
    "            if os.path.exists(bigg_model_path) and bigg_model_path[-5:] == \".json\":\n",
    "                try:\n",
    "                    self.model = json.load(open(bigg_model_path))\n",
    "                    bigg_model_name = re.search(\"([\\w+\\.?\\s?]+)(?=\\.json)\", bigg_model_path).group()\n",
    "                    break\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "#         self.model = json.load(open('BiGG model of S. aureus.json'))\n",
    "#         bigg_model_path = 'BiGG model of S. aureus.json'\n",
    "#         bigg_model_name = re.search(\"([\\w+\\.?\\s?]+)(?=\\.json)\", bigg_model_path).group()\n",
    "\n",
    "        # define the paths\n",
    "        self.paths['cwd'] = os.path.dirname(os.path.realpath(bigg_model_path))\n",
    "        print(self.paths['cwd'])\n",
    "        self.paths['sub_directory_path'] = os.path.join(self.paths['cwd'],f\"scraping-{bigg_model_name}\")\n",
    "        if not os.path.isdir(self.paths['sub_directory_path']):        \n",
    "            os.mkdir(self.paths['sub_directory_path'])\n",
    "                    \n",
    "        self.variables['scraped_xls'] = {}\n",
    "        self.variables['scraped_entryids'] = {}\n",
    "        self.paths['scraped_model_json_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_model\") + \".json\"\n",
    "        self.paths['sel_xls_download_path'] = os.path.join(self.paths['sub_directory_path'],\"xls_download\")\n",
    "        \n",
    "        self.step_number = 1\n",
    "        \n",
    "        self.paths['progress_file_path'] = os.path.join(self.paths['sub_directory_path'], \"current_progress\") + '.txt'\n",
    "        if os.path.exists(self.paths['progress_file_path']):\n",
    "            f = open(self.paths['progress_file_path'], \"r\")\n",
    "            self.step_number = int(f.read(1))\n",
    "            if not re.search('[1-5]',str(self.step_number)):\n",
    "                self.fatal_error_handler(\"Progress file malformed. Please delete and restart\")\n",
    "        else:\n",
    "            self.step_number = 1\n",
    "            self.progress_update(self.step_number)\n",
    "\n",
    "        self.paths['xls_download_path'] = os.path.join(self.paths['sub_directory_path'], 'downloaded_xls') \n",
    "        if not os.path.isdir(self.paths['xls_download_path']):\n",
    "            os.mkdir(self.paths['xls_download_path'])\n",
    "\n",
    "        self.paths['scraped_xls_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_xls\") + \".json\"\n",
    "        if os.path.exists(self.paths['scraped_xls_file_path']):\n",
    "            f = open(self.paths['scraped_xls_file_path'], \"r\")\n",
    "            self.variables['scraped_xls'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "        self.paths['scraped_entryids_file_path'] = os.path.join(self.paths['sub_directory_path'], \"scraped_entryids\") + \".json\"\n",
    "        if os.path.exists(self.paths['scraped_entryids_file_path']):\n",
    "            f = open(self.paths['scraped_entryids_file_path'], \"r\")\n",
    "            self.variables['scraped_entryids'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "        self.paths['entryids_json_file_path'] = os.path.join(self.paths['sub_directory_path'], \"entryids_json\") + \".json\"\n",
    "        if os.path.exists(self.paths['entryids_json_file_path']):\n",
    "            f = open(self.paths['entryids_json_file_path'], \"r\")\n",
    "            self.variables['entry_id_json_out'] = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 1: SCRAPE SABIO WEBSITE BY DOWNLOAD XLS FOR GIVEN REACTIONS IN BIGG MODEL\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def scrape_xls(self,reaction_identifier, search_option):\n",
    "        self.count += 1\n",
    "        print(self.count, end='\\r')\n",
    "        \n",
    "        \n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "        self.click_element_id(\"resetbtn\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.click_element_id(\"option\")\n",
    "        self.select_dropdown_id(\"searchterms\", search_option)\n",
    "        text_area = self.driver.find_element_by_id(\"searchtermField\")\n",
    "        text_area.send_keys(reaction_identifier)  \n",
    "        time.sleep(self.parameters['general_delay'])  \n",
    "        self.click_element_id(\"addsearch\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        result_num = \"\"\n",
    "        try: \n",
    "            result_num_ele = self.driver.find_element_by_id(\"numberofKinLaw\")\n",
    "            for char in result_num_ele.text:\n",
    "                if re.search('[0-9]', char):\n",
    "                    result_num = result_num + char\n",
    "\n",
    "            result_num = int(result_num)\n",
    "        except:\n",
    "            self.driver.close()\n",
    "            return False\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        self.select_dropdown_id(\"max\", \"100\")\n",
    "        element = Select(self.driver.find_element_by_id(\"max\"))\n",
    "        element.select_by_visible_text(\"100\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay'])\n",
    "\n",
    "        if result_num > 0 and result_num <= 100:\n",
    "            self.click_element_id(\"allCheckbox\")\n",
    "            time.sleep(self.parameters['general_delay'])\n",
    "        elif result_num > 100:\n",
    "            self.click_element_id(\"allCheckbox\")\n",
    "            for i in range(int(result_num/100)):\n",
    "                element = self.driver.find_element_by_xpath(\"//*[@class = 'nextLink']\")\n",
    "                element.click()\n",
    "                time.sleep(self.parameters['general_delay'])\n",
    "                self.click_element_id(\"allCheckbox\")\n",
    "                time.sleep(self.parameters['general_delay'])\n",
    "        else:\n",
    "            #self.driver.close()\n",
    "            return False\n",
    "\n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/spreadsheetExport\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay']*7.5)\n",
    "\n",
    "        self.click_element_id(\"excelExport\")\n",
    "\n",
    "        time.sleep(self.parameters['general_delay']*2.5)\n",
    "\n",
    "        #self.driver.close()\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "    def scrape_bigg_xls(self,):\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        prefs = {'download.default_directory' : self.paths['cwd'] + self.paths['sel_xls_download_path']}\n",
    "        chrome_options.add_experimental_option('prefs', prefs)\n",
    "        self.driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "        \n",
    "        self.driver.get(\"chrome://settings/security?search=downloads\")\n",
    "        element = self.driver.find_element_by_xpath(\"//div[contains(@class, 'disc-border')]\")\n",
    "        element.click()\n",
    "        \n",
    "        for reaction in self.model[\"reactions\"]:\n",
    "            if not reaction[\"name\"] in self.variables['scraped_xls']:\n",
    "                ids_to_try = reaction[\"annotation\"]\n",
    "\n",
    "                success_flag = False\n",
    "                annotation_search_pairs = {\"sabiork\":\"SabioReactionID\", \"metanetx.reaction\":\"MetaNetXReactionID\", \"ec-code\":\"ECNumber\", \"kegg.reaction\":\"KeggReactionID\", \"rhea\":\"RheaReactionID\"}\n",
    "                for annotation in annotation_search_pairs:\n",
    "                    if not success_flag:\n",
    "                        if annotation in ids_to_try:\n",
    "                            for id_to_try in ids_to_try[annotation]:\n",
    "#                                 success_flag = self.scrape_xls(id_to_try, annotation_search_pairs[annotation])\n",
    "                                try:\n",
    "                                    success_flag = self.scrape_xls(id_to_try, annotation_search_pairs[annotation])\n",
    "                                except:\n",
    "                                    success_flag = False\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                if not success_flag:\n",
    "#                     success_flag = self.scrape_xls(reaction[\"name\"], \"Enzymename\")\n",
    "                    try:\n",
    "                        success_flag = self.scrape_xls(reaction[\"name\"], \"Enzymename\")\n",
    "                    except:\n",
    "                        success_flag = False\n",
    "\n",
    "                json_dict_key = reaction[\"name\"].replace(\"\\\"\", \"\")\n",
    "                if success_flag:\n",
    "                    self.variables['scraped_xls'][json_dict_key] = \"yes\"\n",
    "                else:\n",
    "                    self.variables['scraped_xls'][json_dict_key] = \"no\"\n",
    "\n",
    "            with open(self.paths['scraped_xls_file_path'], 'w') as outfile:\n",
    "                json.dump(self.variables['scraped_xls'], outfile, indent = 4)   \n",
    "                outfile.close()\n",
    "                \n",
    "        self.step_number = 2\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 2: GLOB EXPORTED XLS FILES TOGETHER\n",
    "    --------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    def glob_xls_files(self,):\n",
    "        scraped_sans_parentheses_enzymes = glob.glob('./{}/*.xls'.format(self.paths['xls_download_path']))\n",
    "        total_dataframes = []\n",
    "        for file in scraped_sans_parentheses_enzymes:\n",
    "            #file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "            dfn = pd.read_excel(file)\n",
    "            total_dataframes.append(dfn)\n",
    "\n",
    "        # combine the total set of dataframes\n",
    "        combined_df = pd.DataFrame()\n",
    "        combined_df = pd.concat(total_dataframes)\n",
    "        combined_df = combined_df.fillna(' ')\n",
    "        combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "        # export the dataframe\n",
    "        csv_path = os.path.join(self.paths['sub_directory_path'], \"proccessed-xls\") + \".csv\"\n",
    "        combined_df.to_csv(csv_path)\n",
    "        \n",
    "        # update the \n",
    "        self.step_number = 3\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 3: SCRAPE ADDITIONAL DATA BY ENTRYID\n",
    "    --------------------------------------------------------------------    \n",
    "    \"\"\"\n",
    "\n",
    "    def scrape_entry_id(self,entry_id):\n",
    "        global entry_id_json_out\n",
    "\n",
    "        entry_id = str(entry_id)\n",
    "\n",
    "        self.driver = webdriver.Chrome(executable_path=r\".\\chromedriver.exe\")\n",
    "        self.driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(\"option\")\n",
    "        self.select_dropdown_id(\"searchterms\", \"EntryID\")\n",
    "        text_area = self.driver.find_element_by_id(\"searchtermField\")\n",
    "        text_area.send_keys(entry_id)\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(\"addsearch\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.click_element_id(entry_id + \"img\")\n",
    "\n",
    "        time.sleep(general_delay)\n",
    "\n",
    "        self.driver.switch_to.frame(self.driver.find_element_by_xpath(\"//iframe[@name='iframe_\" + entry_id + \"']\"))\n",
    "        element = self.driver.find_element_by_xpath(\"//table\")\n",
    "        html_source = element.get_attribute('innerHTML')\n",
    "        \n",
    "        table_df = pd.read_html(html_source)\n",
    "        reaction_parameters_df = pd.DataFrame()\n",
    "        counter = 0\n",
    "        parameters_json = {}\n",
    "        for df in table_df:\n",
    "            try:\n",
    "                if df[0][0] == \"Parameter\":\n",
    "                    reaction_parameters_df = table_df[counter]\n",
    "            except:\n",
    "                self.driver.close()\n",
    "                return parameters_json\n",
    "            counter += 1\n",
    "            \n",
    "        parameter_name = \"\"\n",
    "        for i in range(len(reaction_parameters_df[0])-2):\n",
    "            parameter_name = reaction_parameters_df[0][i+2]\n",
    "            inner_parameters_json = {}\n",
    "            for j in range(len(reaction_parameters_df)-3):\n",
    "                inner_parameters_json[reaction_parameters_df[j+1][1]] = reaction_parameters_df[j+1][i+2]\n",
    "\n",
    "            parameters_json[parameter_name] = inner_parameters_json\n",
    "\n",
    "        self.driver.close()\n",
    "\n",
    "        return parameters_json\n",
    "\n",
    "\n",
    "    def scrape_entryids(self,):\n",
    "        global entry_id_json_out\n",
    "\n",
    "        sabio_xls_df = pd.read_csv(xls_csv_file_path)\n",
    "        entryids = sabio_xls_df[\"EntryID\"].unique().tolist()\n",
    "\n",
    "        for entryid in entryids:\n",
    "            if not entryid in scraped_entryids:\n",
    "                try:\n",
    "                    entry_id_json_out[str(entryid)] = scrape_entry_id(entryid)\n",
    "                    self.variables['scraped_entryids'][entryid] = \"yes\"\n",
    "                except:\n",
    "                    self.variables['scraped_entryids'][entryid] = \"no\"\n",
    "            with open(scraped_entryids_file_path, 'w') as outfile:\n",
    "                json.dump(self.variables['scraped_entryids'], outfile, indent = 4)   \n",
    "                outfile.close()\n",
    "            with open(entryids_json_file_path, 'w') as f:\n",
    "                json.dump(entry_id_json_out, f, indent = 4)        \n",
    "                f.close()\n",
    "        \n",
    "        self.step_number = 4\n",
    "        self.progress_update(self.step_number)\n",
    "\n",
    "    \"\"\"\n",
    "    --------------------------------------------------------------------\n",
    "        STEP 4: COMBINE ENZYME AND ENTRYID DATA INTO JSON FILE\n",
    "    --------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    def combine_data(self,):\n",
    "\n",
    "        sabio_xls_df = pd.read_csv(self.paths['xls_csv_file_path'])\n",
    "\n",
    "        # Opening JSON file\n",
    "        with open(self.paths['entryids_json_file_path']) as json_file:\n",
    "            entry_id_data = json.load(json_file)\n",
    "\n",
    "        enzymenames = sabio_xls_df[\"Enzymename\"].unique().tolist()\n",
    "        enzyme_dict = {}\n",
    "        missing_entry_ids = []\n",
    "        parameters = {}\n",
    "\n",
    "        for enzyme in enzymenames:\n",
    "            sabio_grouped_enzyme_df = sabio_xls_df.loc[sabio_xls_df[\"Enzymename\"] == enzyme]\n",
    "            dict_to_append = {}\n",
    "            reactions = sabio_grouped_enzyme_df[\"Reaction\"].unique().tolist()\n",
    "            for reaction in reactions:\n",
    "                dict_reactions_to_append = {}\n",
    "                sabio_grouped_reactions_df = sabio_grouped_enzyme_df.loc[sabio_grouped_enzyme_df[\"Reaction\"] == reaction]\n",
    "                entryids = sabio_grouped_reactions_df[\"EntryID\"].unique().tolist()\n",
    "\n",
    "                for entryid in entryids:\n",
    "                    entry_ids_df = sabio_grouped_reactions_df.loc[sabio_grouped_reactions_df[\"EntryID\"] == entryid]\n",
    "                    dict_entryid_to_append = {}\n",
    "                    head_of_df = entry_ids_df.head(1).squeeze()\n",
    "                    entry_id_flag = True\n",
    "                    parameter_info = {}\n",
    "\n",
    "                    try:\n",
    "                        parameter_info = entry_id_data[str(entryid)]\n",
    "                        dict_entryid_to_append[\"Parameters\"] = parameter_info\n",
    "                    except:\n",
    "                        missing_entry_ids.append(str(entryid))\n",
    "                        entry_id_flag = False\n",
    "                        dict_entryid_to_append[\"Parameters\"] = \"NaN\"\n",
    "\n",
    "                    rate_law = head_of_df[\"Rate Equation\"]\n",
    "                    bad_rate_laws = [\"unknown\", \"\", \"-\"]\n",
    "\n",
    "                    if not rate_law in bad_rate_laws:                    \n",
    "                        dict_entryid_to_append[\"RateLaw\"] = rate_law\n",
    "                        dict_entryid_to_append[\"SubstitutedRateLaw\"] = rate_law\n",
    "                    else:\n",
    "                        dict_entryid_to_append[\"RateLaw\"] = \"NaN\"\n",
    "                        dict_entryid_to_append[\"SubstitutedRateLaw\"] = \"NaN\"\n",
    "\n",
    "                    if entry_id_flag:\n",
    "\n",
    "                        fields_to_copy = [\"Buffer\", \"Product\", \"PubMedID\", \"Publication\", \"pH\", \"Temperature\", \"Enzyme Variant\", \"UniProtKB_AC\", \"Organism\", \"KineticMechanismType\", \"SabioReactionID\"]\n",
    "                        for field in fields_to_copy:  \n",
    "                            dict_entryid_to_append[field] = head_of_df[field]\n",
    "                        dict_reactions_to_append[entryid] = dict_entryid_to_append\n",
    "                        dict_entryid_to_append[\"Substrates\"] = head_of_df[\"Substrate\"].split(\";\")\n",
    "                        out_rate_law = rate_law\n",
    "                        if not rate_law in bad_rate_laws:                    \n",
    "                            substrates = head_of_df[\"Substrate\"].split(\";\")\n",
    "\n",
    "                            stripped_string = re.sub('[0-9]', '', rate_law)\n",
    "\n",
    "                            variables = re.split(\"\\^|\\*|\\+|\\-|\\/|\\(|\\)| \", stripped_string)\n",
    "                            variables = ' '.join(variables).split()\n",
    "\n",
    "                            start_value_permutations = [\"start value\", \"start val.\"]\n",
    "                            substrates_key = {}\n",
    "                            for var in variables:\n",
    "                                if var in parameter_info:\n",
    "                                    for permutation in start_value_permutations:\n",
    "                                        try:\n",
    "                                            if var == \"A\" or var == \"B\":\n",
    "                                                substrates_key[var] = parameter_info[var][\"species\"]\n",
    "                                            else:\n",
    "                                                value = parameter_info[var][permutation]\n",
    "                                                if value != \"-\" and value != \"\" and value != \" \":\n",
    "                                                    out_rate_law = out_rate_law.replace(var, parameter_info[var][permutation])\n",
    "                                        except:\n",
    "                                            pass\n",
    "\n",
    "                            dict_entryid_to_append[\"RateLawSubstrates\"] = substrates_key\n",
    "                            dict_entryid_to_append[\"SubstitutedRateLaw\"] = out_rate_law\n",
    "\n",
    "                dict_to_append[reaction] = dict_reactions_to_append\n",
    "\n",
    "            enzyme_dict[enzyme] = dict_to_append\n",
    "\n",
    "        with open(scraped_model_json_file_path, 'w') as f:\n",
    "            json.dump(enzyme_dict, f, indent=4)\n",
    "        \n",
    "        self.step_number = 5\n",
    "        self.progress_update(self.step_number)\n",
    "        \n",
    "    def progress_update(self, step):\n",
    "        if not re.search('[0-5]', str(step)):\n",
    "            print(f'--> ERROR: The {step} step is not acceptable.')\n",
    "        f = open(self.paths['progress_file_path'], \"w\")\n",
    "        f.write(str(step))\n",
    "        f.close()\n",
    "\n",
    "    def main(self,):\n",
    "        self.start()\n",
    "\n",
    "        while True:\n",
    "            if self.step_number == 1:\n",
    "                self.scrape_bigg_xls()\n",
    "            elif self.step_number == 2:\n",
    "                self.glob_xls_files()\n",
    "            elif self.step_number == 3:\n",
    "                self.scrape_entryids()\n",
    "            elif self.step_number == 4:\n",
    "                self.combine_data()\n",
    "            elif self.step_number == 5:\n",
    "                print(\"Execution complete. Scraper finished.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specify the BIGG Model JSON file path: ./BiGG_models/BiGG model of S. aureus.json\n",
      "C:\\Users\\Andrew Freiburger\\Dropbox\\My PC (DESKTOP-M302P50)\\Documents\\UVic Civil Engineering\\dFBA\\dFBApy\\scraping\\SABIO\\BiGG_models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:200: DeprecationWarning: use options instead of chrome_options\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\r"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=r\".\\chromedriver.exe\")\n",
    "driver.get(\"http://sabiork.h-its.org/newSearch/index\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
